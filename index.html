<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-03T00:00:00Z">2025-02-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile
  Manipulation Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05313v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05313v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Jaafar, Shreyas Sundara Raman, Yichen Wei, Sudarshan Harithas, Sofia Juliani, Anneke Wernerfelt, Benedict Quartey, Ifrah Idrees, Jason Xinyu Liu, Stefanie Tellex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently learning and executing long-horizon mobile manipulation (MoMa)
tasks is crucial for advancing robotics in household and workplace settings.
However, current MoMa models are data-inefficient, underscoring the need for
improved models that require realistic-sized benchmarks to evaluate their
efficiency, which do not exist. To address this, we introduce the LAMBDA
({\lambda}) benchmark (Long-horizon Actions for Mobile-manipulation
Benchmarking of Directed Activities), which evaluates the data efficiency of
models on language-conditioned, long-horizon, multi-room, multi-floor,
pick-and-place tasks using a dataset of manageable size, more feasible for
collection. The benchmark includes 571 human-collected demonstrations that
provide realism and diversity in simulated and real-world settings. Unlike
planner-generated data, these trajectories offer natural variability and
replay-verifiability, ensuring robust learning and evaluation. We benchmark
several models, including learning-based models and a neuro-symbolic modular
approach combining foundation models with task and motion planning.
Learning-based models show suboptimal success rates, even when leveraging
pretrained weights, underscoring significant data inefficiencies. However, the
neuro-symbolic approach performs significantly better while being more data
efficient. Findings highlight the need for more data-efficient learning-based
MoMa approaches. {\lambda} addresses this gap by serving as a key benchmark for
evaluating the data efficiency of those future models in handling household
robotics tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Framework for Nonlinear Kalman Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05717v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05717v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shida Jiang, Junzhe Shi, Scott Moura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Kalman filter (KF) is a state estimation algorithm that optimally
combines system knowledge and measurements to minimize the mean squared error
of the estimated states. While KF was initially designed for linear systems,
numerous extensions of it, such as extended Kalman filter (EKF), unscented
Kalman filter (UKF), cubature Kalman filter (CKF), etc., have been proposed for
nonlinear systems. Although different types of nonlinear KFs have different
pros and cons, they all use the same framework of linear KF. Yet, according to
what we found in this paper, the framework tends to give overconfident and less
accurate state estimations when the measurement functions are nonlinear.
Therefore, in this study, we designed a new framework that can be combined with
any existing type of nonlinear KFs and showed theoretically and empirically
that the new framework estimates the states and covariance more accurately than
the old one. The new framework was tested on four different nonlinear KFs and
five different tasks, showcasing its ability to reduce estimation errors by
several orders of magnitude in low-measurement-noise conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the MATLAB code for UKF and CKF, we replaced "sqrt(Variance)" with
  "chol(Variance).'", making the algorithm faster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advances in Multimodal Adaptation and Generalization: From Traditional
  Approaches to Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/donghao51/Awesome-Multimodal-Adaptation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exoskeleton-Assisted Balance and Task Evaluation During Quiet Stance and
  Kneeling in Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayatri Sreenivasan, Chunchu Zhu, Jingang Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Construction workers exert intense physical effort and experience serious
safety and health risks in hazardous working environments. Quiet stance and
kneeling are among the most common postures performed by construction workers
during their daily work. This paper analyzes lower-limb joint influence on
neural balance control strategies using the frequency behavior of the
intersection point of ground reaction forces. To evaluate the impact of
elevation and wearable knee exoskeletons on postural balance and welding task
performance, we design and integrate virtual- and mixed-reality (VR/MR) to
simulate elevated environments and welding tasks. A linear quadratic
regulator-controlled triple- and double-link inverted pendulum model is used
for balance strategy quantification in quiet stance and kneeling, respectively.
Extensive multi-subject experiments are conducted to evaluate the usability of
occupational exoskeletons in destabilizing construction environments. The
quantified balance strategies capture the significance of knee joint during
balance control of quiet stance and kneeling gaits. Results show that center of
pressure sway area reduced up to 62% in quiet stance and 39% in kneeling for
subjects tested in high-elevation VR/MR worksites when provided knee
exoskeleton assistance. The comprehensive balance and multitask evaluation
methodology developed aims to reveal exoskeleton design considerations to
mitigate the fall risk in construction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures, accepted by IEEE Transactions on Automation
  Science and Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DecTrain: Deciding When to Train a Monocular Depth DNN Online 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zih-Sing Fu, Soumya Sudhakar, Sertac Karaman, Vivienne Sze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) can deteriorate in accuracy when deployment data
differs from training data. While performing online training at all timesteps
can improve accuracy, it is computationally expensive. We propose DecTrain, a
new algorithm that decides when to train a monocular depth DNN online using
self-supervision with low overhead. To make the decision at each timestep,
DecTrain compares the cost of training with the predicted accuracy gain. We
evaluate DecTrain on out-of-distribution data, and find DecTrain maintains
accuracy compared to online training at all timesteps, while training only 44%
of the time on average. We also compare the recovery of a low inference cost
DNN using DecTrain and a more generalizable high inference cost DNN on various
sequences. DecTrain recovers the majority (97%) of the accuracy gain of online
training at all timesteps while reducing computation compared to the high
inference cost DNN which recovers only 66%. With an even smaller DNN, we
achieve 89% recovery while reducing computation by 56%. DecTrain enables
low-cost online training for a smaller DNN to have competitive accuracy with a
larger, more generalizable DNN at a lower overall computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-free Lifelong Robot Learning with Retrieval-based Weighted Local
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02995v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02995v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhi Yang, Xinyu Wang, Ruipeng Zhang, Cong Wang, Frans A. Oliehoek, Jens Kober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental objective in intelligent robotics is to move towards lifelong
learning robot that can learn and adapt to unseen scenarios over time. However,
continually learning new tasks would introduce catastrophic forgetting problems
due to data distribution shifts. To mitigate this, we store a subset of data
from previous tasks and utilize it in two manners: leveraging experience replay
to retain learned skills and applying a novel Retrieval-based Local Adaptation
technique to restore relevant knowledge. Since a lifelong learning robot must
operate in task-free scenarios, where task IDs and even boundaries are not
available, our method performs effectively without relying on such information.
We also incorporate a selective weighting mechanism to focus on the most
"forgotten" skill segment, ensuring effective knowledge restoration.
Experimental results across diverse manipulation tasks demonstrate that our
framework provides a scalable paradigm for lifelong learning, enhancing robot
performance in open-ended, task-free scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Atomic Skill Library Construction Method for Data-Efficient Embodied
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjiang Li, Bo Peng, Chang Li, Ning Qiao, Qi Zheng, Lei Sun, Yusen Qin, Bangguo Li, Yifeng Luan, Yibing Zhan, Mingang Sun, Tong Xu, Lusong Li, Hui Shen, Xiaodong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied manipulation is a fundamental ability in the realm of embodied
artificial intelligence. Although current embodied manipulation models show
certain generalizations in specific settings, they struggle in new environments
and tasks due to the complexity and diversity of real-world scenarios. The
traditional end-to-end data collection and training manner leads to significant
data demands. Decomposing end-to-end tasks into reusable atomic skills helps
reduce data requirements and improve task execution success rate. However,
existing methods are limited by predefined skill sets that cannot be
dynamically updated. To address the issue, we introduce a three-wheeled
data-driven method to build an atomic skill library, which contains general
skills enabling the execution of complex tasks. We divide tasks into subtasks
using the Vision-Language Planning (VLP). Then, atomic skill definitions are
formed by abstracting the subtasks. Finally, an atomic skill library is
constructed via data collection and Vision-Language-Action (VLA) fine-tuning.
As the atomic skill library expands dynamically with the three-wheel update
strategy, the range of tasks it can cover grows naturally. In this way, our
method shifts focus from end-to-end tasks to atomic skills, significantly
reducing data costs while maintaining high performance and enabling efficient
adaptation to new tasks. Extensive experiments in real-world settings
demonstrate the effectiveness and efficiency of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Degeneracy Detection for Point-to-Plane Error Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Hatleskog, Kostas Alexis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Degeneracies arising from uninformative geometry are known to deteriorate
LiDAR-based localization and mapping. This work introduces a new probabilistic
method to detect and mitigate the effect of degeneracies in point-to-plane
error minimization. The noise on the Hessian of the point-to-plane optimization
problem is characterized by the noise on points and surface normals used in its
construction. We exploit this characterization to quantify the probability of a
direction being degenerate. The degeneracy-detection procedure is used in a new
real-time degeneracy-aware iterative closest point algorithm for LiDAR
registration, in which we smoothly attenuate updates in degenerate directions.
The method's parameters are selected based on the noise characteristics
provided in the LiDAR's datasheet. We validate the approach in four real-world
experiments, demonstrating that it outperforms state-of-the-art methods at
detecting and mitigating the adverse effects of degeneracies. For the benefit
of the community, we release the code for the method at:
github.com/ntnu-arl/drpm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, accepted by IEEE Robotics and Automation Letters
  (IEEE RAL). Supplementary video: https://www.youtube.com/watch?v=bKnHs_wwnXs.
  Code: https://github.com/ntnu-arl/drpm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multiple Initial Solutions to Optimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elad Sharony, Heng Yang, Tong Che, Marco Pavone, Shie Mannor, Peter Karkus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequentially solving similar optimization problems under strict runtime
constraints is essential for many applications, such as robot control,
autonomous driving, and portfolio management. The performance of local
optimization methods in these settings is sensitive to the initial solution:
poor initialization can lead to slow convergence or suboptimal solutions. To
address this challenge, we propose learning to predict \emph{multiple} diverse
initial solutions given parameters that define the problem instance. We
introduce two strategies for utilizing multiple initial solutions: (i) a
single-optimizer approach, where the most promising initial solution is chosen
using a selection function, and (ii) a multiple-optimizers approach, where
several optimizers, potentially run in parallel, are each initialized with a
different solution, with the best solution chosen afterward. Notably, by
including a default initialization among predicted ones, the cost of the final
output is guaranteed to be equal or lower than with the default initialization.
We validate our method on three optimal control benchmark tasks: cart-pole,
reacher, and autonomous driving, using different optimizers: DDP, MPPI, and
iLQR. We find significant and consistent improvement with our method across all
evaluation settings and demonstrate that it efficiently scales with the number
of initial solutions required. The code is available at MISO
(https://github.com/EladSharony/miso).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-robot connective collaboration toward collective obstacle field
  traversal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodi Hu, Xingjue Liao, Wuhao Du, Feifei Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environments with large terrain height variations present great challenges
for legged robot locomotion. Drawing inspiration from fire ants' collective
assembly behavior, we study strategies that can enable two ``connectable''
robots to collectively navigate over bumpy terrains with height variations
larger than robot leg length. Each robot was designed to be extremely simple,
with a cubical body and one rotary motor actuating four vertical peg legs that
move in pairs. Two or more robots could physically connect to one another to
enhance collective mobility. We performed locomotion experiments with a
two-robot group, across an obstacle field filled with uniformly-distributed
semi-spherical ``boulders''. Experimentally-measured robot speed suggested that
the connection length between the robots has a significant effect on collective
mobility: connection length C in [0.86, 0.9] robot unit body length (UBL) were
able to produce sustainable movements across the obstacle field, whereas
connection length C in [0.63, 0.84] and [0.92, 1.1] UBL resulted in low
traversability. An energy landscape based model revealed the underlying
mechanism of how connection length modulated collective mobility through the
system's potential energy landscape, and informed adaptation strategies for the
two-robot system to adapt their connection length for traversing obstacle
fields with varying spatial frequencies. Our results demonstrated that by
varying the connection configuration between the robots, the two-robot system
could leverage mechanical intelligence to better utilize obstacle interaction
forces and produce improved locomotion. Going forward, we envision that
generalized principles of robot-environment coupling can inform design and
control strategies for a large group of small robots to achieve ant-like
collective environment negotiation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GUIDEd Agents: Enhancing Navigation Policies through Task-Specific
  Uncertainty Abstraction in Localization-Limited Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15178v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15178v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Puthumanaillam, Paulo Padrao, Jose Fuentes, Leonardo Bobadilla, Melkior Ornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles performing navigation tasks in complex environments face
significant challenges due to uncertainty in state estimation. In many
scenarios, such as stealth operations or resource-constrained settings,
accessing high-precision localization comes at a significant cost, forcing
robots to rely primarily on less precise state estimates. Our key observation
is that different tasks require varying levels of precision in different
regions: a robot navigating a crowded space might need precise localization
near obstacles but can operate effectively with less precision elsewhere. In
this paper, we present a planning method for integrating task-specific
uncertainty requirements directly into navigation policies. We introduce
Task-Specific Uncertainty Maps (TSUMs), which abstract the acceptable levels of
state estimation uncertainty across different regions. TSUMs align task
requirements and environmental features using a shared representation space,
generated via a domain-adapted encoder. Using TSUMs, we propose Generalized
Uncertainty Integration for Decision-Making and Execution (GUIDE), a policy
conditioning framework that incorporates these uncertainty requirements into
robot decision-making. We find that TSUMs provide an effective way to abstract
task-specific uncertainty requirements, and conditioning policies on TSUMs
enables the robot to reason about the context-dependent value of certainty and
adapt its behavior accordingly. We show how integrating GUIDE into
reinforcement learning frameworks allows the agent to learn navigation policies
that effectively balance task completion and uncertainty management without
explicit reward engineering. We evaluate GUIDE on various real-world robotic
navigation tasks and find that it demonstrates significant improvement in task
completion rates compared to baseline methods that do not explicitly consider
task-specific uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiRT: Enhancing Robotic Control with Hierarchical Robot <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05273v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05273v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, Jianyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language-Action (VLA) models, leveraging powerful pre trained
Vision-Language Models (VLMs) backends, have shown promise in robotic control
due to their impressive generalization ability. However, the success comes at a
cost. Their reliance on VLM backends with billions of parameters leads to high
computational costs and inference latency, limiting the testing scenarios to
mainly quasi-static tasks and hindering performance in dynamic tasks requiring
rapid interactions. To address these limitations, this paper proposes HiRT, a
Hierarchical Robot Transformer framework that enables flexible frequency and
performance trade-off. HiRT keeps VLMs running at low frequencies to capture
temporarily invariant features while enabling real-time interaction through a
high-frequency vision-based policy guided by the slowly updated features.
Experiment results in both simulation and real-world settings demonstrate
significant improvements over baseline methods. Empirically, in static tasks,
we double the control frequency and achieve comparable success rates.
Additionally, on novel real-world dynamic ma nipulation tasks which are
challenging for previous VLA models, HiRT improves the success rate from 48% to
75%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CORL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M3PT: A <span class="highlight-title">Transformer</span> for Multimodal, Multi-Party Social Signal Prediction
  with Person-aware Blockwise Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Tang, Abrar Anwar, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding social signals in multi-party conversations is important for
human-robot interaction and artificial social intelligence. Social signals
include body pose, head pose, speech, and context-specific activities like
acquiring and taking bites of food when dining. Past work in multi-party
interaction tends to build task-specific models for predicting social signals.
In this work, we address the challenge of predicting multimodal social signals
in multi-party settings in a single model. We introduce M3PT, a causal
transformer architecture with modality and temporal blockwise attention masking
to simultaneously process multiple social cues across multiple participants and
their temporal interactions. We train and evaluate M3PT on the Human-Human
Commensality Dataset (HHCD), and demonstrate that using multiple modalities
improves bite timing and speaking status prediction. Source code:
https://github.com/AbrarAnwar/masked-social-signals/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation
  with Language Models <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10027v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10027v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Kim, Keonwoo Kim, Mintaek Oh, Hanbi Baek, Jiyang Lee, Donghwi Jung, Soojin Woo, Younkyung Woo, John Tucker, Roya Firoozi, Seung-Woo Seo, Mac Schwager, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown significant potential in guiding
embodied agents to execute language instructions across a range of tasks,
including robotic manipulation and navigation. However, existing methods are
primarily designed for static environments and do not leverage the agent's own
experiences to refine its initial plans. Given that real-world environments are
inherently stochastic, initial plans based solely on LLMs' general knowledge
may fail to achieve their objectives, unlike in static scenarios. To address
this limitation, this study introduces the Experience-and-Emotion Map (E2Map),
which integrates not only LLM knowledge but also the agent's real-world
experiences, drawing inspiration from human emotional responses. The proposed
methodology enables one-shot behavior adjustments by updating the E2Map based
on the agent's experiences. Our evaluation in stochastic navigation
environments, including both simulations and real-world scenarios, demonstrates
that the proposed method significantly enhances performance in stochastic
environments compared to existing LLM-based approaches. Code and supplementary
materials are available at https://e2map.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 28 figures. Project page: https://e2map.github.io. Accepted
  to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-02T00:00:00Z">2025-02-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V2I-Calib++: A Multi-terminal Spatial Calibration Approach in Urban
  Intersections for Collaborative Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianxin Qu, Yijin Xiong, Xinyu Zhang, Chen Xia, Qian Peng, Ziqiang Song, Kang Liu, Xin Wu, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban intersections, dense with pedestrian and vehicular traffic and
compounded by GPS signal obstructions from high-rise buildings, are among the
most challenging areas in urban traffic systems. Traditional single-vehicle
intelligence systems often perform poorly in such environments due to a lack of
global traffic flow information and the ability to respond to unexpected
events. Vehicle-to-Everything (V2X) technology, through real-time communication
between vehicles (V2V) and vehicles to infrastructure (V2I), offers a robust
solution. However, practical applications still face numerous challenges.
Calibration among heterogeneous vehicle and infrastructure endpoints in
multi-end LiDAR systems is crucial for ensuring the accuracy and consistency of
perception system data. Most existing multi-end calibration methods rely on
initial calibration values provided by positioning systems, but the instability
of GPS signals due to high buildings in urban canyons poses severe challenges
to these methods. To address this issue, this paper proposes a novel multi-end
LiDAR system calibration method that does not require positioning priors to
determine initial external parameters and meets real-time requirements. Our
method introduces an innovative multi-end perception object association
technique, utilizing a new Overall Distance metric (oDist) to measure the
spatial association between perception objects, and effectively combines global
consistency search algorithms with optimal transport theory. By this means, we
can extract co-observed targets from object association results for further
external parameter computation and optimization. Extensive comparative and
ablation experiments conducted on the simulated dataset V2X-Sim and the real
dataset DAIR-V2X confirm the effectiveness and efficiency of our method. The
code for this method can be accessed at:
https://github.com/MassimoQu/v2i-calib.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Cross-hand Policies for High-DOF Reaching and Grasping <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09150v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09150v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijin She, Shishun Zhang, Yunfan Ye, Ruizhen Hu, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reaching-and-grasping is a fundamental skill for robotic manipulation, but
existing methods usually train models on a specific gripper and cannot be
reused on another gripper. In this paper, we propose a novel method that can
learn a unified policy model that can be easily transferred to different
dexterous grippers. Our method consists of two stages: a gripper-agnostic
policy model that predicts the displacements of pre-defined key points on the
gripper, and a gripper-specific adaptation model that translates these
displacements into adjustments for controlling the grippers' joints. The
gripper state and interactions with objects are captured at the finger level
using robust geometric representations, integrated with a transformer-based
network to address variations in gripper morphology and geometry. In the
experiments, we evaluate our method on several dexterous grippers and diverse
objects, and the result shows that our method significantly outperforms the
baseline methods. Pioneering the transfer of grasp policies across dexterous
grippers, our method effectively demonstrates its potential for learning
generalizable and transferable manipulation skills for various robotic hands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable
  Manipulation with Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02787v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02787v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang You, Bokui Shen, Congyue Deng, Haoran Geng, Songlin Wei, He Wang, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable object manipulation stands as one of the most captivating yet
formidable challenges in robotics. While previous techniques have predominantly
relied on learning latent dynamics through demonstrations, typically
represented as either particles or images, there exists a pertinent limitation:
acquiring suitable demonstrations, especially for long-horizon tasks, can be
elusive. Moreover, basing learning entirely on demonstrations can hamper the
model's ability to generalize beyond the demonstrated tasks. In this work, we
introduce a demonstration-free hierarchical planning approach capable of
tackling intricate long-horizon tasks without necessitating any training. We
employ large language models (LLMs) to articulate a high-level, stage-by-stage
plan corresponding to a specified task. For every individual stage, the LLM
provides both the tool's name and the Python code to craft intermediate subgoal
point clouds. With the tool and subgoal for a particular stage at our disposal,
we present a granular closed-loop model predictive control strategy. This
leverages Differentiable Physics with Point-to-Point correspondence
(DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied
iteratively. Experimental findings affirm that our technique surpasses multiple
benchmarks in dough manipulation, spanning both short and long horizons.
Remarkably, our model demonstrates robust generalization capabilities to novel
and previously unencountered complex tasks without any preliminary
demonstrations. We further substantiate our approach with experimental trials
on real-world robotic platforms. Our project page:
https://qq456cvb.github.io/projects/donut.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. IEEE Robotics and Automation Letters (RA-L). Preprint
  Version. Accepted January, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tightly-Coupled LiDAR-IMU-Wheel Odometry with an Online Neural Kinematic
  Model Learning via Factor Graph Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08907v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08907v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Okawara, Kenji Koide, Shuji Oishi, Masashi Yokozuka, Atsuhiko Banno, Kentaro Uno, Kazuya Yoshida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environments lacking geometric features (e.g., tunnels and long straight
corridors) are challenging for LiDAR-based odometry algorithms because LiDAR
point clouds degenerate in such environments. For wheeled robots, a wheel
kinematic model (i.e., wheel odometry) can improve the reliability of the
odometry estimation. However, the kinematic model suffers from complex motions
(e.g., wheel slippage, lateral movement) in the case of skid-steering robots
particularly because this robot model rotates by skidding its wheels.
Furthermore, these errors change nonlinearly when the wheel slippage is large
(e.g., drifting) and are subject to terrain-dependent parameters. To
simultaneously tackle point cloud degeneration and the kinematic model errors,
we developed a LiDAR-IMU-wheel odometry algorithm incorporating online training
of a neural network that learns the kinematic model of wheeled robots with
nonlinearity. We propose to train the neural network online on a factor graph
along with robot states, allowing the learning-based kinematic model to adapt
to the current terrain condition. The proposed method jointly solves online
training of the neural network and LiDAR-IMU-wheel odometry on a unified factor
graph to retain the consistency of all those constraints. Through experiments,
we first verified that the proposed network adapted to a changing environment,
resulting in an accurate odometry estimation across different environments. We
then confirmed that the proposed odometry estimation algorithm was robust
against point cloud degeneration and nonlinearity (e.g., large wheel slippage
by drifting) of the kinematic model. The summary video is available here:
https://www.youtube.com/watch?v=CvRVhdda7Cw
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the journal, Robotics and Autonomous Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-01T00:00:00Z">2025-02-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point-LN: A Lightweight Framework for Efficient Point Cloud
  Classification Using Non-Parametric Positional Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marzieh Mohammadi, Amir Salarpour, Pedram MohajerAnsari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Point-LN, a novel lightweight framework engineered for efficient
3D point cloud classification. Point-LN integrates essential non-parametric
components-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),
and non-learnable positional encoding-with a streamlined learnable classifier
that significantly enhances classification accuracy while maintaining a minimal
parameter footprint. This hybrid architecture ensures low computational costs
and rapid inference speeds, making Point-LN ideal for real-time and
resource-constrained applications. Comprehensive evaluations on benchmark
datasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN
achieves competitive performance compared to state-of-the-art methods, all
while offering exceptional efficiency. These results establish Point-LN as a
robust and scalable solution for diverse point cloud classification tasks,
highlighting its potential for widespread adoption in various computer vision
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at the 29th
  International Computer Conference, Computer Society of Iran (CSICC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Coordination using Active Epistemic Inference for Multi-Robot
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauren Bramblett, Jonathan Reasoner, Nicola Bezzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Multi-robot system (MRS) provides significant advantages for intricate
tasks such as environmental monitoring, underwater inspections, and space
missions. However, addressing potential communication failures or the lack of
communication infrastructure in these fields remains a challenge. A significant
portion of MRS research presumes that the system can maintain communication
with proximity constraints, but this approach does not solve situations where
communication is either non-existent, unreliable, or poses a security risk.
Some approaches tackle this issue using predictions about other robots while
not communicating, but these methods generally only permit agents to utilize
first-order reasoning, which involves reasoning based purely on their own
observations. In contrast, to deal with this problem, our proposed framework
utilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a
robot's perspective to reason about a belief of others observations. Our
approach has two main phases: i) an efficient runtime plan adaptation using
active inference to signal intentions and reason about a robot's own belief and
the beliefs of others in the system, and ii) a hierarchical epistemic planning
framework to iteratively reason about the current MRS mission state. The
proposed framework outperforms greedy and first-order reasoning approaches and
is validated using simulations and experiments with heterogeneous robotic
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Learning of Physical Object Properties Through Robot
  Manipulation and Database of Object Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrej Kruzliak, Jiri Hartvich, Shubhan P. Patni, Lukas Rustler, Jan Kristof Behrens, Fares J. Abu-Dakka, Krystian Mikolajczyk, Ville Kyrki, Matej Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a framework for automatically extracting physical object
properties, such as material composition, mass, volume, and stiffness, through
robot manipulation and a database of object measurements. The framework
involves exploratory action selection to maximize learning about objects on a
table. A Bayesian network models conditional dependencies between object
properties, incorporating prior probability distributions and uncertainty
associated with measurement actions. The algorithm selects optimal exploratory
actions based on expected information gain and updates object properties
through Bayesian inference. Experimental evaluation demonstrates effective
action selection compared to a baseline and correct termination of the
experiments if there is nothing more to be learned. The algorithm proved to
behave intelligently when presented with trick objects with material properties
in conflict with their appearance. The robot pipeline integrates with a logging
module and an online database of objects, containing over 24,000 measurements
of 63 objects with different grippers. All code and data are publicly
available, facilitating automatic digitization of objects and their physical
properties through exploratory manipulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Xiang Xu, Jiawei Ren, Wenwei Zhang, Liang Pan, Kai Chen, Wei Tsang Ooi, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient data utilization is crucial for advancing 3D scene understanding in
autonomous driving, where reliance on heavily human-annotated LiDAR point
clouds challenges fully supervised methods. Addressing this, our study extends
into semi-supervised learning for LiDAR semantic segmentation, leveraging the
intrinsic spatial priors of driving scenes and multi-sensor complements to
augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved
framework that integrates laser beam manipulations from disparate LiDAR scans
and incorporates LiDAR-camera correspondences to further assist data-efficient
learning. Our framework is tailored to enhance 3D scene consistency
regularization by incorporating multi-modality, including 1) multi-modal
LaserMix operation for fine-grained cross-sensor interactions; 2)
camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and
3) language-driven knowledge guidance generating auxiliary supervisions using
open-vocabulary models. The versatility of LaserMix++ enables applications
across LiDAR representations, establishing it as a universally applicable
solution. Our framework is rigorously validated through theoretical analysis
and extensive experiments on popular driving perception datasets. Results
demonstrate that LaserMix++ markedly outperforms fully supervised alternatives,
achieving comparable accuracy with five times fewer annotations and
significantly improving the supervised-only baselines. This substantial
advancement underscores the potential of semi-supervised approaches in reducing
the reliance on extensive labeled data in LiDAR-based 3D scene understanding
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TPAMI 2025; 18 pages, 6 figures, 9 tables; Code at
  https://github.com/ldkong1205/LaserMix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking and Improving Bird's Eye View Perception Robustness in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in bird's eye view (BEV) representations have shown
remarkable promise for in-vehicle 3D perception. However, while these methods
have achieved impressive results on standard benchmarks, their robustness in
varied conditions remains insufficiently assessed. In this study, we present
RoboBEV, an extensive benchmark suite designed to evaluate the resilience of
BEV algorithms. This suite incorporates a diverse set of camera corruption
types, each examined over three severity levels. Our benchmarks also consider
the impact of complete sensor failures that occur when using multi-modal
models. Through RoboBEV, we assess 33 state-of-the-art BEV-based perception
models spanning tasks like detection, map segmentation, depth estimation, and
occupancy prediction. Our analyses reveal a noticeable correlation between the
model's performance on in-distribution datasets and its resilience to
out-of-distribution challenges. Our experimental results also underline the
efficacy of strategies like pre-training and depth-free BEV transformations in
enhancing robustness against out-of-distribution data. Furthermore, we observe
that leveraging extensive temporal information significantly improves the
model's robustness. Based on our observations, we design an effective
robustness enhancement strategy based on the CLIP model. The insights from this
study pave the way for the development of future BEV models that seamlessly
combine accuracy with real-world robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TPAMI 2025; 17 pages, 13 figures, 11 tables; Code at this https URL:
  https://github.com/Daniel-xsy/RoboBEV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Affordance-based Robot Manipulation with Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01083v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01083v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhang, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for assistive robot manipulation, which focuses on two
fundamental challenges: first, efficiently adapting large-scale models to
downstream scene affordance understanding tasks, especially in daily living
scenarios where gathering multi-task data involving humans requires strenuous
effort; second, effectively learning robot action trajectories by grounding the
visual affordance model. We tackle the first challenge by employing a
parameter-efficient prompt tuning method that prepends learnable text prompts
to the frozen vision model to predict manipulation affordances in multi-task
scenarios. Then we propose to learn robot action trajectories guided by
affordances in a supervised flow matching method. Flow matching represents a
robot visuomotor policy as a conditional process of flowing random waypoints to
desired robot action trajectories. Finally, we introduce a real-world dataset
with 10 tasks across Activities of Daily Living to test our framework. Our
extensive evaluation highlights that the proposed prompt tuning method for
learning manipulation affordance achieves competitive performance and even
outperforms some other finetuning protocols across data scales, while
satisfying parameter efficiency. Learning multi-task robot action trajectories
with flow matching leads to consistently favorable results in several robot
manipulation benchmarks than some alternative behavior cloning methods. This
includes more stable training and evaluation, and noticeably faster inference,
while maintaining comparable generalization performance to diffusion policy,
where flow matching performs marginally better in most cases. Our framework
seamlessly unifies affordance learning and action generation with flow matching
for robot manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RH20T-P: A Primitive-Level Robotic <span class="highlight-title">Dataset</span> Towards Composable
  Generalization Agents <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeren Chen, Zhelun Shi, Xiaoya Lu, Lehan He, Sucheng Qian, Zhenfei Yin, Wanli Ouyang, Jing Shao, Yu Qiao, Cewu Lu, Lu Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving generalizability in solving out-of-distribution tasks is one of the
ultimate goals of learning robotic manipulation. Recent progress of
Vision-Language Models (VLMs) has shown that VLM-based task planners can
alleviate the difficulty of solving novel tasks, by decomposing the compounded
tasks as a plan of sequentially executing primitive-level skills that have been
already mastered. It is also promising for robotic manipulation to adapt such
composable generalization ability, in the form of composable generalization
agents (CGAs). However, the community lacks of reliable design of primitive
skills and a sufficient amount of primitive-level data annotations. Therefore,
we propose RH20T-P, a primitive-level robotic manipulation dataset, which
contains about 38k video clips covering 67 diverse manipulation tasks in
real-world scenarios. Each clip is manually annotated according to a set of
meticulously designed primitive skills that are common in robotic manipulation.
Furthermore, we standardize a plan-execute CGA paradigm and implement an
exemplar baseline called RA-P on our RH20T-P, whose positive performance on
solving unseen tasks validates that the proposed dataset can offer composable
generalization ability to robotic manipulation agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures, 7 tables. Accepted by NeurIPS 2024 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoTip: A Finger-Shaped Tactile Sensor with Active Rotation Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Zhang, Jiaqi Jiang, Shan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, advancements in optical tactile sensor technology have
primarily centred on enhancing sensing precision and expanding the range of
sensing modalities. To meet the requirements for more skilful manipulation,
there should be a movement towards making tactile sensors more dynamic. In this
paper, we introduce RoTip, a novel vision-based tactile sensor that is uniquely
designed with an independently controlled joint and the capability to sense
contact over its entire surface. The rotational capability of the sensor is
particularly crucial for manipulating everyday objects, especially thin and
flexible ones, as it enables the sensor to mobilize while in contact with the
object's surface. The manipulation experiments demonstrate the ability of our
proposed RoTip to manipulate rigid and flexible objects, and the full-finger
tactile feedback and active rotation capabilities have the potential to explore
more complex and precise manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenLiDARMap: Zero-Drift Point Cloud Mapping using Map Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Kulmer, Maximilian Leitenstern, Marcel Weinmann, Markus Lienkamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate localization is a critical component of mobile autonomous systems,
especially in Global Navigation Satellite Systems (GNSS)-denied environments
where traditional methods fail. In such scenarios, environmental sensing is
essential for reliable operation. However, approaches such as LiDAR odometry
and Simultaneous Localization and Mapping (SLAM) suffer from drift over long
distances, especially in the absence of loop closures. Map-based localization
offers a robust alternative, but the challenge lies in creating and
georeferencing maps without GNSS support. To address this issue, we propose a
method for creating georeferenced maps without GNSS by using publicly available
data, such as building footprints and surface models derived from sparse aerial
scans. Our approach integrates these data with onboard LiDAR scans to produce
dense, accurate, georeferenced 3D point cloud maps. By combining an Iterative
Closest Point (ICP) scan-to-scan and scan-to-map matching strategy, we achieve
high local consistency without suffering from long-term drift. Thus, we
eliminate the reliance on GNSS for the creation of georeferenced maps. The
results demonstrate that LiDAR-only mapping can produce accurate georeferenced
point cloud maps when augmented with existing map priors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xTED: Cross-Domain Adaptation via Diffusion-Based Trajectory Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Niu, Qimao Chen, Tenglong Liu, Jianxiong Li, Guyue Zhou, Yi Zhang, Jianming Hu, Xianyuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reusing pre-collected data from different domains is an appealing solution
for decision-making tasks, especially when data in the target domain are
limited. Existing cross-domain policy transfer methods mostly aim at learning
domain correspondences or corrections to facilitate policy learning, such as
learning task/domain-specific discriminators, representations, or policies.
This design philosophy often results in heavy model architectures or
task/domain-specific modeling, lacking flexibility. This reality makes us
wonder: can we directly bridge the domain gaps universally at the data level,
instead of relying on complex downstream cross-domain policy transfer
procedures? In this study, we propose the Cross-Domain Trajectory EDiting
(xTED) framework that employs a specially designed diffusion model for
cross-domain trajectory adaptation. Our proposed model architecture effectively
captures the intricate dependencies among states, actions, and rewards, as well
as the dynamics patterns within target data. Edited by adding noises and
denoising with the pre-trained diffusion model, source domain trajectories can
be transformed to align with target domain properties while preserving original
semantic information. This process effectively corrects underlying domain gaps,
enhancing state realism and dynamics reliability in source data, and allowing
flexible integration with various single-domain and cross-domain downstream
policy learning methods. Despite its simplicity, xTED demonstrates superior
performance in extensive simulation and real-robot experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>xTED offers a novel, generic, flexible, simple and effective paradigm
  that casts cross-domain policy adaptation as a data pre-processing problem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DextrAH-RGB: Visuomotor Policies to Grasp Anything with Dexterous Hands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritvik Singh, Arthur Allshire, Ankur Handa, Nathan Ratliff, Karl Van Wyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most important, yet challenging, skills for a dexterous robot is
grasping a diverse range of objects. Much of the prior work has been limited by
speed, generality, or reliance on depth maps and object poses. In this paper,
we introduce DextrAH-RGB, a system that can perform dexterous arm-hand grasping
end-to-end from RGB image input. We train a privileged fabric-guided policy
(FGP) in simulation through reinforcement learning that acts on a geometric
fabric controller to dexterously grasp a wide variety of objects. We then
distill this privileged FGP into a RGB-based FGP strictly in simulation using
photorealistic tiled rendering. To our knowledge, this is the first work that
is able to demonstrate robust sim2real transfer of an end2end RGB-based policy
for complex, dynamic, contact-rich tasks such as dexterous grasping.
DextrAH-RGB is competitive with depth-based dexterous grasping policies, and
generalizes to novel objects with unseen geometry, texture, and lighting
conditions in the real world. Videos of our system grasping a diverse range of
unseen objects are available at \url{https://dextrah-rgb.github.io/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Splatting Visual MPC for Granular Media Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Cheng Tseng, Ellina Zhang, Krishna Murthy Jatavallabhula, Florian Shkurti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in learned 3D representations have enabled significant
progress in solving complex robotic manipulation tasks, particularly for
rigid-body objects. However, manipulating granular materials such as beans,
nuts, and rice, remains challenging due to the intricate physics of particle
interactions, high-dimensional and partially observable state, inability to
visually track individual particles in a pile, and the computational demands of
accurate dynamics prediction. Current deep latent dynamics models often
struggle to generalize in granular material manipulation due to a lack of
inductive biases. In this work, we propose a novel approach that learns a
visual dynamics model over Gaussian splatting representations of scenes and
leverages this model for manipulating granular media via Model-Predictive
Control. Our method enables efficient optimization for complex manipulation
tasks on piles of granular media. We evaluate our approach in both simulated
and real-world settings, demonstrating its ability to solve unseen planning
tasks and generalize to new environments in a zero-shot transfer. We also show
significant prediction and manipulation performance improvements compared to
existing granular media manipulation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project website https://weichengtseng.github.io/gs-granular-mani/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaze-based Task Decomposition for Robot Manipulation in Imitation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Takizawa, Yoshiyuki Ohmura, Yasuo Kuniyoshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In imitation learning for robotic manipulation, decomposing object
manipulation tasks into multiple sub-tasks is essential. This decomposition
enables the reuse of learned skills in varying contexts and the combination of
acquired skills to perform novel tasks, rather than merely replicating
demonstrated motions. Gaze plays a critical role in human object manipulation,
where it is strongly correlated with hand movements. We hypothesize that an
imitating agent's gaze control, fixating on specific landmarks and
transitioning between them, simultaneously segments demonstrated manipulations
into sub-tasks. In this study, we propose a simple yet robust task
decomposition method based on gaze transitions. The method leverages
teleoperation, a common modality in robotic manipulation for collecting
demonstrations, in which a human operator's gaze is measured and used for task
decomposition as a substitute for an imitating agent's gaze. Notably, our
method achieves consistent task decomposition across all demonstrations for
each task, which is desirable in contexts such as machine learning. We applied
this method to demonstrations of various tasks and evaluated the
characteristics and consistency of the resulting sub-tasks. Furthermore,
through extensive testing across a wide range of hyperparameter variations, we
demonstrated that the proposed method possesses the robustness necessary for
application to different robotic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FastUMI: A Scalable and Hardware-Independent Universal Manipulation
  Interface with <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Zhaxizhuoma, Kehui Liu, Chuyue Guan, Zhongjie Jia, Ziniu Wu, Xin Liu, Tianyu Wang, Shuai Liang, Pengan Chen, Pingrui Zhang, Haoming Song, Delin Qu, Dong Wang, Zhigang Wang, Nieqing Cao, Yan Ding, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world manipulation data involving robotic arms is crucial for developing
generalist action policies, yet such data remains scarce since existing data
collection methods are hindered by high costs, hardware dependencies, and
complex setup requirements. In this work, we introduce FastUMI, a substantial
redesign of the Universal Manipulation Interface (UMI) system that addresses
these challenges by enabling rapid deployment, simplifying hardware-software
integration, and delivering robust performance in real-world data acquisition.
Compared with UMI, FastUMI has several advantages: 1) It adopts a decoupled
hardware design and incorporates extensive mechanical modifications, removing
dependencies on specialized robotic components while preserving consistent
observation perspectives. 2) It also refines the algorithmic pipeline by
replacing complex Visual-Inertial Odometry (VIO) implementations with an
off-the-shelf tracking module, significantly reducing deployment complexity
while maintaining accuracy. 3) FastUMI includes an ecosystem for data
collection, verification, and integration with both established and newly
developed imitation learning algorithms, accelerating policy learning
advancement. Additionally, we have open-sourced a high-quality dataset of over
10,000 real-world demonstration trajectories spanning 22 everyday tasks,
forming one of the most diverse UMI-like datasets to date. Experimental results
confirm that FastUMI facilitates rapid deployment, reduces operational costs
and labor demands, and maintains robust performance across diverse manipulation
scenarios, thereby advancing scalable data-driven robotic learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeePerSea: Multi-modal Perception <span class="highlight-title">Dataset</span> of In-water Objects for
  Autonomous Surface Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingi Jeong, Arihant Chadda, Ziang Ren, Luyang Zhao, Haowen Liu, Monika Roznere, Aiwei Zhang, Yitao Jiang, Sabriel Achong, Samuel Lensgraf, Alberto Quattrini Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the first publicly accessible labeled multi-modal
perception dataset for autonomous maritime navigation, focusing on in-water
obstacles within the aquatic environment to enhance situational awareness for
Autonomous Surface Vehicles (ASVs). This dataset, collected over 4 years and
consisting of diverse objects encountered under varying environmental
conditions, aims to bridge the research gap in autonomous surface vehicles by
providing a multi-modal, annotated, and ego-centric perception dataset, for
object detection and classification. We also show the applicability of the
proposed dataset by training deep learning-based open-source perception
algorithms that have shown success. We expect that our dataset will contribute
to development of the marine autonomy pipelines and marine (field) robotics.
This dataset is opensource and can be found at https://seepersea.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12155v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12155v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younggyo Seo, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning (RL), we train a value function to understand the
long-term consequence of executing a single action. However, the value of
taking each action can be ambiguous in robotics as robot movements are
typically the aggregate result of executing multiple small actions. Moreover,
robotic training data often consists of noisy trajectories, in which each
action is noisy but executing a series of actions results in a meaningful robot
movement. This further makes it difficult for the value function to understand
the effect of individual actions. To address this, we introduce Coarse-to-fine
Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that
learns a critic network that outputs Q-values over a sequence of actions, i.e.,
explicitly training the value function to learn the consequence of executing
action sequences. We study our algorithm on 53 robotic tasks with sparse and
dense rewards, as well as with and without demonstrations, from BiGym,
HumanoidBench, and RLBench. We find that CQN-AS outperforms various baselines,
in particular on humanoid control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages. Website: https://younggyo.me/cqn-as/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DINO-WM: World Models on <span class="highlight-title">Pre-train</span>ed Visual Features enable Zero-shot
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaoyue Zhou, Hengkai Pan, <span class="highlight-author">Yann LeCun</span>, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to predict future outcomes given control actions is fundamental
for physical reasoning. However, such predictive models, often called world
models, remains challenging to learn and are typically developed for
task-specific solutions with online policy learning. To unlock world models'
true potential, we argue that they should 1) be trainable on offline,
pre-collected trajectories, 2) support test-time behavior optimization, and 3)
facilitate task-agnostic reasoning. To this end, we present DINO World Model
(DINO-WM), a new method to model visual dynamics without reconstructing the
visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2,
enabling it to learn from offline behavioral trajectories by predicting future
patch features. This allows DINO-WM to achieve observational goals through
action sequence optimization, facilitating task-agnostic planning by treating
goal features as prediction targets. We demonstrate that DINO-WM achieves
zero-shot behavioral solutions at test time on six environments without expert
demonstrations, reward modeling, or pre-learned inverse models, outperforming
prior state-of-the-art work across diverse task families such as arbitrarily
configured mazes, push manipulation with varied object shapes, and
multi-particle scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-31T00:00:00Z">2025-01-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">30</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vintix: Action Model via In-Context Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Polubarov, Nikita Lyubaykin, Alexander Derevyagin, Ilya Zisman, Denis Tarasov, Alexander Nikulin, Vladislav Kurenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Reinforcement Learning (ICRL) represents a promising paradigm for
developing generalist agents that learn at inference time through
trial-and-error interactions, analogous to how large language models adapt
contextually, but with a focus on reward maximization. However, the scalability
of ICRL beyond toy tasks and single-domain settings remains an open challenge.
In this work, we present the first steps toward scaling ICRL by introducing a
fixed, cross-domain model capable of learning behaviors through in-context
reinforcement learning. Our results demonstrate that Algorithm Distillation, a
framework designed to facilitate ICRL, offers a compelling and competitive
alternative to expert distillation to construct versatile action models. These
findings highlight the potential of ICRL as a scalable approach for generalist
decision-making systems. Code to be released at
https://github.com/dunnolab/vintix
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. In review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precision Harvesting in Cluttered Environments: Integrating End Effector
  Design with Dual Camera Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kendall Koe, Poojan Kalpeshbhai Shah, Benjamin Walt, Jordan Westphal, Samhita Marri, Shivani Kamtikar, James Seungbum Nam, Naveen Kumar Uppalapati, Girish Krishnan, Girish Chowdhary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to labor shortages in specialty crop industries, a need for robotic
automation to increase agricultural efficiency and productivity has arisen.
Previous manipulation systems perform well in harvesting in uncluttered and
structured environments. High tunnel environments are more compact and
cluttered in nature, requiring a rethinking of the large form factor systems
and grippers. We propose a novel codesigned framework incorporating a global
detection camera and a local eye-in-hand camera that demonstrates precise
localization of small fruits via closed-loop visual feedback and reliable error
handling. Field experiments in high tunnels show our system can reach an
average of 85.0\% of cherry tomato fruit in 10.98s on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptive Mixed-Integer Footstep Control for Underactuated Bipedal
  Walking on Rough Terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Acosta, Michael Posa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traversing rough terrain requires dynamic bipeds to stabilize themselves
through foot placement without stepping in unsafe areas. Planning these
footsteps online is challenging given non-convexity of the safe terrain, and
imperfect perception and state estimation. This paper addresses these
challenges with a full-stack perception and control system for achieving
underactuated walking on discontinuous terrain. First, we develop
model-predictive footstep control (MPFC), a single mixed-integer quadratic
program which assumes a convex polygon terrain decomposition to optimize over
discrete foothold choice, footstep position, ankle torque, template dynamics,
and footstep timing at over 100 Hz. We then propose a novel approach for
generating convex polygon terrain decompositions online. Our perception stack
decouples safe-terrain classification from fitting planar polygons, generating
a temporally consistent terrain segmentation in real time using a single CPU
thread. We demonstrate the performance of our perception and control stack
through outdoor experiments with the underactuated biped Cassie, achieving
state of the art perceptive bipedal walking on discontinuous terrain.
Supplemental Video: https://youtu.be/eCOD1bMi638
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liudi Yang, Ruben Mascaro, Ignacio Alzugaray, Sai Manoj Prakhya, Marco Karrer, Ziyuan Liu, Margarita Chli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel loop closure detection algorithm that uses
graph attention neural networks to encode semantic graphs to perform place
recognition and then use semantic registration to estimate the 6 DoF relative
pose constraint. Our place recognition algorithm has two key modules, namely, a
semantic graph encoder module and a graph comparison module. The semantic graph
encoder employs graph attention networks to efficiently encode spatial,
semantic and geometric information from the semantic graph of the input point
cloud. We then use self-attention mechanism in both node-embedding and
graph-embedding steps to create distinctive graph vectors. The graph vectors of
the current scan and a keyframe scan are then compared in the graph comparison
module to identify a possible loop closure. Specifically, employing the
difference of the two graph vectors showed a significant improvement in
performance, as shown in ablation studies. Lastly, we implemented a semantic
registration algorithm that takes in loop closure candidate scans and estimates
the relative 6 DoF pose constraint for the LiDAR SLAM system. Extensive
evaluation on public datasets shows that our model is more accurate and robust,
achieving 13% improvement in maximum F1 score on the SemanticKITTI dataset,
when compared to the baseline semantic graph algorithm. For the benefit of the
community, we open-source the complete implementation of our proposed algorithm
and custom implementation of semantic registration at
https://github.com/crepuscularlight/SemanticLoopClosure
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven
  Surface Normal-aware Tracking and Mapping <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Huang, Beilei Cui, Long Bai, Zhen Chen, Jinlin Wu, Zhen Li, Hongbin Liu, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) is essential for precise
surgical interventions and robotic tasks in minimally invasive procedures.
While recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM
with high-quality novel view synthesis and fast rendering, these systems
struggle with accurate depth and surface reconstruction due to multi-view
inconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between
the reconstructed frames. In this work, we present Endo-2DTAM, a real-time
endoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these
challenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which
consists of tracking, mapping, and bundle adjustment modules for geometrically
accurate reconstruction. Our robust tracking module combines point-to-point and
point-to-plane distance metrics, while the mapping module utilizes normal
consistency and depth distortion to enhance surface reconstruction quality. We
also introduce a pose-consistent strategy for efficient and geometrically
coherent keyframe sampling. Extensive experiments on public endoscopic datasets
demonstrate that Endo-2DTAM achieves an RMSE of $1.87\pm 0.63$ mm for depth
reconstruction of surgical scenes while maintaining computationally efficient
tracking, high-quality visual appearance, and real-time rendering. Our code
will be released at github.com/lastbasket/Endo-2DTAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GO: The Great Outdoors Multimodal <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Jiang, Kasi Viswanath, Akhil Nagariya, George Chustz, Maggie Wigness, Philip Osteen, Timothy Overbye, Christian Ellis, Long Quang, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Great Outdoors (GO) dataset is a multi-modal annotated data resource
aimed at advancing ground robotics research in unstructured environments. This
dataset provides the most comprehensive set of data modalities and annotations
compared to existing off-road datasets. In total, the GO dataset includes six
unique sensor types with high-quality semantic annotations and GPS traces to
support tasks such as semantic segmentation, object detection, and SLAM. The
diverse environmental conditions represented in the dataset present significant
real-world challenges that provide opportunities to develop more robust
solutions to support the continued advancement of field robotics, autonomous
exploration, and perception systems in natural environments. The dataset can be
downloaded at: https://www.unmannedlab.org/the-great-outdoors-dataset/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for
  Autonomous Drone FlighT at the Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amogh Joshi, Sourav Sanyal, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of human-intuitive interactions into autonomous systems has
been limited. Traditional Natural Language Processing (NLP) systems struggle
with context and intent understanding, severely restricting human-robot
interaction. Recent advancements in Large Language Models (LLMs) have
transformed this dynamic, allowing for intuitive and high-level communication
through speech and text, and bridging the gap between human commands and
robotic actions. Additionally, autonomous navigation has emerged as a central
focus in robotics research, with artificial intelligence (AI) increasingly
being leveraged to enhance these systems. However, existing AI-based navigation
algorithms face significant challenges in latency-critical tasks where rapid
decision-making is critical. Traditional frame-based vision systems, while
effective for high-level decision-making, suffer from high energy consumption
and latency, limiting their applicability in real-time scenarios. Neuromorphic
vision systems, combining event-based cameras and spiking neural networks
(SNNs), offer a promising alternative by enabling energy-efficient, low-latency
navigation. Despite their potential, real-world implementations of these
systems, particularly on physical platforms such as drones, remain scarce. In
this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework
implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural
language processing, Neuro-LIFT translates human speech into high-level
planning commands which are then autonomously executed using event-based
neuromorphic vision and physics-driven planning. Our framework demonstrates its
capabilities in navigating in a dynamic environment, avoiding obstacles, and
adapting to human instructions in real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Objective Metrics for Human-Subjects Evaluation in Explainable
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Balint Gyevnar, Mark Towers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanation is a fundamentally human process. Understanding the goal and
audience of the explanation is vital, yet existing work on explainable
reinforcement learning (XRL) routinely does not consult humans in their
evaluations. Even when they do, they routinely resort to subjective metrics,
such as confidence or understanding, that can only inform researchers of users'
opinions, not their practical effectiveness for a given problem. This paper
calls on researchers to use objective human metrics for explanation evaluations
based on observable and actionable behaviour to build more reproducible,
comparable, and epistemically grounded research. To this end, we curate,
describe, and compare several objective evaluation methodologies for applying
explanations to debugging agent behaviour and supporting human-agent teaming,
illustrating our proposed methods using a novel grid-based environment. We
discuss how subjective and objective metrics complement each other to provide
holistic validation and how future work needs to utilise standardised
benchmarks for testing to enable greater comparisons between research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikingSoft: A Spiking Neuron Controller for Bio-inspired Locomotion
  with Soft Snake Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Zhang, Cong Wang, Wei Pan, Cosimo Della Santina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the dynamic coupling of moto-neurons and physical elasticity in
animals, this work explores the possibility of generating locomotion gaits by
utilizing physical oscillations in a soft snake by means of a low-level spiking
neural mechanism. To achieve this goal, we introduce the Double Threshold
Spiking neuron model with adjustable thresholds to generate varied output
patterns. This neuron model can excite the natural dynamics of soft robotic
snakes, and it enables distinct movements, such as turning or moving forward,
by simply altering the neural thresholds. Finally, we demonstrate that our
approach, termed SpikingSoft, naturally pairs and integrates with reinforcement
learning. The high-level agent only needs to adjust the two thresholds to
generate complex movement patterns, thus strongly simplifying the learning of
reactive locomotion. Simulation results demonstrate that the proposed
architecture significantly enhances the performance of the soft snake robot,
enabling it to achieve target objectives with a 21.6% increase in success rate,
a 29% reduction in time to reach the target, and smoother movements compared to
the vanilla reinforcement learning controllers or Central Pattern Generator
controller acting in torque space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8th IEEE-RAS International Conference on Soft Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gravity Compensation of the dVRK-Si Patient Side Manipulator based on
  Dynamic Model Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoying Zhou, Hao Yang, Anton Deguet, Loris Fichera, Jie Ying Wu, Peter Kazanzides
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The da Vinci Research Kit (dVRK, also known as dVRK Classic) is an
open-source teleoperated surgical robotic system whose hardware is obtained
from the first generation da Vinci Surgical System (Intuitive, Sunnyvale, CA,
USA). The dVRK has greatly facilitated research in robot-assisted surgery over
the past decade and helped researchers address multiple major challenges in
this domain. Recently, the dVRK-Si system, a new version of the dVRK which uses
mechanical components from the da Vinci Si Surgical System, became available to
the community. The major difference between the first generation da Vinci and
the da Vinci Si is in the structural upgrade of the Patient Side Manipulator
(PSM). Because of this upgrade, the gravity of the dVRK-Si PSM can no longer be
ignored as in the dVRK Classic. The high gravity offset may lead to relatively
low control accuracy and longer response time. In addition, although
substantial progress has been made in addressing the dynamic model
identification problem for the dVRK Classic, further research is required on
model-based control for the dVRK-Si, due to differences in mechanical
components and the demand for enhanced control performance. To address these
problems, in this work, we present (1) a novel full kinematic model of the
dVRK-Si PSM, and (2) a gravity compensation approach based on the dynamic model
identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Optimization Under Stochastic Dynamics Leveraging Maximum
  Mean Discrepancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Basant Sharma, Arun Kumar Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses sampling-based trajectory optimization for risk-aware
navigation under stochastic dynamics. Typically such approaches operate by
computing $\tilde{N}$ perturbed rollouts around the nominal dynamics to
estimate the collision risk associated with a sequence of control commands. We
consider a setting where it is expensive to estimate risk using perturbed
rollouts, for example, due to expensive collision-checks. We put forward two
key contributions. First, we develop an algorithm that distills the statistical
information from a larger set of rollouts to a reduced-set with sample size
$N<<\tilde{N}$. Consequently, we estimate collision risk using just $N$
rollouts instead of $\tilde{N}$. Second, we formulate a novel surrogate for the
collision risk that can leverage the distilled statistical information
contained in the reduced-set. We formalize both algorithmic contributions using
distribution embedding in Reproducing Kernel Hilbert Space (RKHS) and Maximum
Mean Discrepancy (MMD). We perform extensive benchmarking to demonstrate that
our MMD-based approach leads to safer trajectories at low sample regime than
existing baselines using Conditional Value-at Risk (CVaR) based collision risk
estimate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Basant1861/MPC-MMD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swarm-Gen: Fast Generation of Diverse Feasible Swarm Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Idoko, B. Bhanu Teja, K. Madhava Krishna, Arun Kumar Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordination behavior in robot swarms is inherently multi-modal in nature.
That is, there are numerous ways in which a swarm of robots can avoid
inter-agent collisions and reach their respective goals. However, the problem
of generating diverse and feasible swarm behaviors in a scalable manner remains
largely unaddressed. In this paper, we fill this gap by combining generative
models with a safety-filter (SF). Specifically, we sample diverse trajectories
from a learned generative model which is subsequently projected onto the
feasible set using the SF. We experiment with two choices for generative
models, namely: Conditional Variational Autoencoder (CVAE) and Vector-Quantized
Variational Autoencoder (VQ-VAE). We highlight the trade-offs these two models
provide in terms of computation time and trajectory diversity. We develop a
custom solver for our SF and equip it with a neural network that predicts
context-specific initialization. Thecinitialization network is trained in a
self-supervised manner, taking advantage of the differentiability of the SF
solver. We provide two sets of empirical results. First, we demonstrate that we
can generate a large set of multi-modal, feasible trajectories, simulating
diverse swarm behaviors, within a few tens of milliseconds. Second, we show
that our initialization network provides faster convergence of our SF solver
vis-a-vis other alternative heuristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to RAL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Simulation of Soft Robots with Frictional Contacts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etienne Ménager, Louis Montaut, Quentin Le Lidec, Justin Carpentier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, soft robotics simulators have evolved to offer various
functionalities, including the simulation of different material types (e.g.,
elastic, hyper-elastic) and actuation methods (e.g., pneumatic, cable-driven,
servomotor). These simulators also provide tools for various tasks, such as
calibration, design, and control. However, efficiently and accurately computing
derivatives within these simulators remains a challenge, particularly in the
presence of physical contact interactions. Incorporating these derivatives can,
for instance, significantly improve the convergence speed of control methods
like reinforcement learning and trajectory optimization, enable gradient-based
techniques for design, or facilitate end-to-end machine-learning approaches for
model reduction. This paper addresses these challenges by introducing a unified
method for computing the derivatives of mechanical equations within the finite
element method framework, including contact interactions modeled as a nonlinear
complementarity problem. The proposed approach handles both collision and
friction phases, accounts for their nonsmooth dynamics, and leverages the
sparsity introduced by mesh-based models. Its effectiveness is demonstrated
through several examples of controlling and calibrating soft systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning based Quasi-consciousness Training for Robot Intelligent
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchun Li, Fang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores a deep learning based robot intelligent model that
renders robots learn and reason for complex tasks. First, by constructing a
network of environmental factor matrix to stimulate the learning process of the
robot intelligent model, the model parameters must be subjected to coarse &
fine tuning to optimize the loss function for minimizing the loss score,
meanwhile robot intelligent model can fuse all previously known concepts
together to represent things never experienced before, which need robot
intelligent model can be generalized extensively. Secondly, in order to
progressively develop a robot intelligent model with primary consciousness,
every robot must be subjected to at least 1~3 years of special school for
training anthropomorphic behaviour patterns to understand and process complex
environmental information and make rational decisions. This work explores and
delivers the potential application of deep learning-based quasi-consciousness
training in the field of robot intelligent model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HeLiOS: Heterogeneous LiDAR Place Recognition via Overlap-based Learning
  and Local Spherical <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minwoo Jung, Sangwoo Jung, Hyeonjae Gil, Ayoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR place recognition is a crucial module in localization that matches the
current location with previously observed environments. Most existing
approaches in LiDAR place recognition dominantly focus on the spinning type
LiDAR to exploit its large FOV for matching. However, with the recent emergence
of various LiDAR types, the importance of matching data across different LiDAR
types has grown significantly-a challenge that has been largely overlooked for
many years. To address these challenges, we introduce HeLiOS, a deep network
tailored for heterogeneous LiDAR place recognition, which utilizes small local
windows with spherical transformers and optimal transport-based cluster
assignment for robust global descriptors. Our overlap-based data mining and
guided-triplet loss overcome the limitations of traditional distance-based
mining and discrete class constraints. HeLiOS is validated on public datasets,
demonstrating performance in heterogeneous LiDAR place recognition while
including an evaluation for long-term recognition, showcasing its ability to
handle unseen LiDAR types. We release the HeLiOS code as an open source for the
robotics community at https://github.com/minwoo0611/HeLiOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 5 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Source Autonomous Driving Software Platforms: Comparison of
  Autoware and Apollo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee-Yang Jung, Dong-Hee Paek, Seung-Hyun Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full-stack autonomous driving system spans diverse technological
domains-including perception, planning, and control-that each require in-depth
research. Moreover, validating such technologies of the system necessitates
extensive supporting infrastructure, from simulators and sensors to
high-definition maps. These complexities with barrier to entry pose substantial
limitations for individual developers and research groups. Recently,
open-source autonomous driving software platforms have emerged to address this
challenge by providing autonomous driving technologies and practical supporting
infrastructure for implementing and evaluating autonomous driving
functionalities. Among the prominent open-source platforms, Autoware and Apollo
are frequently adopted in both academia and industry. While previous studies
have assessed each platform independently, few have offered a quantitative and
detailed head-to-head comparison of their capabilities. In this paper, we
systematically examine the core modules of Autoware and Apollo and evaluate
their middleware performance to highlight key differences. These insights serve
as a practical reference for researchers and engineers, guiding them in
selecting the most suitable platform for their specific development
environments and advancing the field of full-stack autonomous driving system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arxiv preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimum Time Strategies for a Differential Drive Robot Escaping from a
  Circular Detection Region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ubaldo Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Differential Drive Robot (DDR) located inside a circular detection region
in the plane wants to escape from it in minimum time. Various robotics
applications can be modeled like the previous problem, such as a DDR escaping
as soon as possible from a forbidden/dangerous region in the plane or running
out from the sensor footprint of an unmanned vehicle flying at a constant
altitude. In this paper, we find the motion strategies to accomplish its goal
under two scenarios. In one, the detection region moves slower than the DDR and
seeks to prevent escape; in another, its position is fixed. We formulate the
problem as a zero-sum pursuit-evasion game, and using differential games
theory, we compute the players' time-optimal motion strategies. Given the DDR's
speed advantage, it can always escape by translating away from the center of
the detection region at maximum speed. In this work, we show that the previous
strategy could be optimal in some cases; however, other motion strategies
emerge based on the player's speed ratio and the players' initial
configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning of Flexible Policies for Symbolic Instructions
  with Adjustable Mapping Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wataru Hatanaka, Ryota Yamashina, Takamitsu Matsubara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic task representation is a powerful tool for encoding human
instructions and domain knowledge. Such instructions guide robots to accomplish
diverse objectives and meet constraints through reinforcement learning (RL).
Most existing methods are based on fixed mappings from environmental states to
symbols. However, in inspection tasks, where equipment conditions must be
evaluated from multiple perspectives to avoid errors of oversight, robots must
fulfill the same symbol from different states. To help robots respond to
flexible symbol mapping, we propose representing symbols and their mapping
specifications separately within an RL policy. This approach imposes on RL
policy to learn combinations of symbolic instructions and mapping
specifications, requiring an efficient learning framework. To cope with this
issue, we introduce an approach for learning flexible policies called Symbolic
Instructions with Adjustable Mapping Specifications (SIAMS). This paper
represents symbolic instructions using linear temporal logic (LTL), a formal
language that can be easily integrated into RL. Our method addresses the
diversified completion patterns of instructions by (1) a specification-aware
state modulation, which embeds differences in mapping specifications in state
features, and (2) a symbol-number-based task curriculum, which gradually
provides tasks according to the learning's progress. Evaluations in 3D
simulations with discrete and continuous action spaces demonstrate that our
method outperforms context-aware multitask RL comparisons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, Accepted by IEEE Robotics and Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hamiltonian Dynamics with Bayesian Data Assimilation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehyeun Kim, Tae-Geun Kim, Anouck Girard, Ilya Kolmanovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a neural network-based approach for time-series
prediction in unknown Hamiltonian dynamical systems. Our approach leverages a
surrogate model and learns the system dynamics using generalized coordinates
(positions) and their conjugate momenta while preserving a constant
Hamiltonian. To further enhance long-term prediction accuracy, we introduce an
Autoregressive Hamiltonian Neural Network, which incorporates autoregressive
prediction errors into the training objective. Additionally, we employ Bayesian
data assimilation to refine predictions in real-time using online measurement
data. Numerical experiments on a spring-mass system and highly elliptic orbits
under gravitational perturbations demonstrate the effectiveness of the proposed
method, highlighting its potential for accurate and robust long-term
predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication- and Computation-Efficient Distributed Submodular
  Optimization in Robot Mesh Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Xu, Sandilya Sai Garimella, Vasileios Tzoumas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a communication- and computation-efficient method for distributed
submodular optimization in robot mesh networks. Submodularity is a property of
diminishing returns that arises in active information gathering such as
mapping, surveillance, and target tracking. Our method, Resource-Aware
distributed Greedy (RAG), introduces a new distributed optimization paradigm
that enables scalable and near-optimal action coordination. To this end, RAG
requires each robot to make decisions based only on information received from
and about their neighbors. In contrast, the current paradigms allow the relay
of information about all robots across the network. As a result, RAG's
decision-time scales linearly with the network size, while state-of-the-art
near-optimal submodular optimization algorithms scale cubically. We also
characterize how the designed mesh-network topology affects RAG's approximation
performance. Our analysis implies that sparser networks favor scalability
without proportionally compromising approximation performance: while RAG's
decision time scales linearly with network size, the gain in approximation
performance scales sublinearly. We demonstrate RAG's performance in simulated
scenarios of area detection with up to 45 robots, simulating realistic
robot-to-robot (r2r) communication speeds such as the 0.25 Mbps speed of the
Digi XBee 3 Zigbee 3.0. In the simulations, RAG enables real-time planning, up
to three orders of magnitude faster than competitive near-optimal algorithms,
while also achieving superior mean coverage performance. To enable the
simulations, we extend the high-fidelity and photo-realistic simulator AirSim
by integrating a scalable collaborative autonomy pipeline to tens of robots and
simulating r2r communication delays. Our code is available at
https://github.com/UM-iRaL/Resource-Aware-Coordination-AirSim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and
  Parameter Efficient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based reinforcement learning (RL) offers a solution to the data
inefficiency that plagues most model-free RL algorithms. However, learning a
robust world model often demands complex and deep architectures, which are
expensive to compute and train. Within the world model, dynamics models are
particularly crucial for accurate predictions, and various dynamics-model
architectures have been explored, each with its own set of challenges.
Currently, recurrent neural network (RNN) based world models face issues such
as vanishing gradients and difficulty in capturing long-term dependencies
effectively. In contrast, use of transformers suffers from the well-known
issues of self-attention mechanisms, where both memory and computational
complexity scale as $O(n^2)$, with $n$ representing the sequence length.
  To address these challenges we propose a state space model (SSM) based world
model, specifically based on Mamba, that achieves $O(n)$ memory and
computational complexity while effectively capturing long-term dependencies and
facilitating the use of longer training sequences efficiently. We also
introduce a novel sampling method to mitigate the suboptimality caused by an
incorrect world model in the early stages of training, combining it with the
aforementioned technique to achieve a normalised score comparable to other
state-of-the-art model-based RL algorithms using only a 7 million trainable
parameter world model. This model is accessible and can be trained on an
off-the-shelf laptop. Our code is available at
https://github.com/realwenlongwang/Drama.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOP-ERL: <span class="highlight-title">Transformer</span>-based Off-Policy Episodic Reinforcement Learning <span class="chip">ICLR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Li, Dong Tian, Hongyi Zhou, Xinkai Jiang, Rudolf Lioutikov, Gerhard Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces Transformer-based Off-Policy Episodic Reinforcement
Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the
ERL framework. In ERL, policies predict entire action trajectories over
multiple time steps instead of single actions at every time step. These
trajectories are typically parameterized by trajectory generators such as
Movement Primitives (MP), allowing for smooth and efficient exploration over
long horizons while capturing high-level temporal correlations. However, ERL
methods are often constrained to on-policy frameworks due to the difficulty of
evaluating state-action values for entire action sequences, limiting their
sample efficiency and preventing the use of more efficient off-policy
architectures. TOP-ERL addresses this shortcoming by segmenting long action
sequences and estimating the state-action values for each segment using a
transformer-based critic architecture alongside an n-step return estimation.
These contributions result in efficient and stable training that is reflected
in the empirical results conducted on sophisticated robot learning
environments. TOP-ERL significantly outperforms state-of-the-art RL methods.
Thorough ablation studies additionally show the impact of key design choices on
the model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codebase: https://github.com/BruceGeLi/TOP_ERL_ICLR25. arXiv admin
  note: text overlap with arXiv:2401.11437</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Attention Based Cognitive Human-Robot Collaboration for Pedicle
  Screw Placement in Robot-Assisted Orthopedic Surgery <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Qikai Zou, Yuhang Song, Mingrui Yu, Senqiang Zhu, Shiji Song, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current orthopedic robotic systems largely focus on navigation, aiding
surgeons in positioning a guiding tube but still requiring manual drilling and
screw placement. The automation of this task not only demands high precision
and safety due to the intricate physical interactions between the surgical tool
and bone but also poses significant risks when executed without adequate human
oversight. As it involves continuous physical interaction, the robot should
collaborate with the surgeon, understand the human intent, and always include
the surgeon in the loop. To achieve this, this paper proposes a new cognitive
human-robot collaboration framework, including the intuitive AR-haptic
human-robot interface, the visual-attention-based surgeon model, and the shared
interaction control scheme for the robot. User studies on a robotic platform
for orthopedic surgery are presented to illustrate the performance of the
proposed method. The results demonstrate that the proposed human-robot
collaboration framework outperforms full robot and full human control in terms
of safety and ergonomics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures, in IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tabletop Object Rearrangement: Structure, Complexity, and Efficient
  Combinatorial Search-Based Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis provides an in-depth structural analysis and efficient
algorithmic solutions for tabletop object rearrangement with overhand grasps
(TORO), a foundational task in advancing intelligent robotic manipulation.
Rearranging multiple objects in a confined workspace presents two primary
challenges: sequencing actions to minimize pick-and-place operations - an
NP-hard problem in TORO - and determining temporary object placements ("buffer
poses") within a cluttered environment, which is essential yet highly complex.
For TORO with available external free space, this work investigates the minimum
buffer space, or "running buffer size," required for temporary relocations,
presenting both theoretical insights and exact algorithms. For TORO without
external free space, the concept of lazy buffer verification is introduced,
with its efficiency evaluated across various manipulator configurations,
including single-arm, dual-arm, and mobile manipulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis of Kai Gao, written under the direction of Prof. Jingjin
  Yu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LNS2+RL: Combining Multi-Agent Reinforcement Learning with Large
  Neighborhood Search in Multi-Agent Path Finding <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17794v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17794v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Wang, Tanishq Duhan, Jiaoyang Li, Guillaume Sartoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Path Finding (MAPF) is a critical component of logistics and
warehouse management, which focuses on planning collision-free paths for a team
of robots in a known environment. Recent work introduced a novel MAPF approach,
LNS2, which proposed to repair a quickly obtained set of infeasible paths via
iterative replanning, by relying on a fast, yet lower-quality, prioritized
planning (PP) algorithm. At the same time, there has been a recent push for
Multi-Agent Reinforcement Learning (MARL) based MAPF algorithms, which exhibit
improved cooperation over such PP algorithms, although inevitably remaining
slower. In this paper, we introduce a new MAPF algorithm, LNS2+RL, which
combines the distinct yet complementary characteristics of LNS2 and MARL to
effectively balance their individual limitations and get the best from both
worlds. During early iterations, LNS2+RL relies on MARL for low-level
replanning, which we show eliminates collisions much more than a PP algorithm.
There, our MARL-based planner allows agents to reason about past and future
information to gradually learn cooperative decision-making through a finely
designed curriculum learning. At later stages of planning, LNS2+RL adaptively
switches to PP algorithm to quickly resolve the remaining collisions, naturally
trading off solution quality (number of collisions in the solution) and
computational efficiency. Our comprehensive experiments on high-agent-density
tasks across various team sizes, world sizes, and map structures consistently
demonstrate the superior performance of LNS2+RL compared to many MAPF
algorithms, including LNS2, LaCAM, EECBS, and SCRIMP. In maps with complex
structures, the advantages of LNS2+RL are particularly pronounced, with LNS2+RL
achieving a success rate of over 50% in nearly half of the tested tasks, while
that of LaCAM, EECBS and SCRIMP falls to 0%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal Explainability Approach for Human-Aware Robots in
  Multi-Party Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iveta Bečková, Štefan Pócoš, Giulia Belgiovine, Marco Matarese, Omar Eldardeer, Alessandra Sciutti, Carlo Mazzola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The addressee estimation (understanding to whom somebody is talking) is a
fundamental task for human activity recognition in multi-party conversation
scenarios. Specifically, in the field of human-robot interaction, it becomes
even more crucial to enable social robots to participate in such interactive
contexts. However, it is usually implemented as a binary classification task,
restricting the robot's capability to estimate whether it was addressed
\review{or not, which} limits its interactive skills. For a social robot to
gain the trust of humans, it is also important to manifest a certain level of
transparency and explainability. Explainable artificial intelligence thus plays
a significant role in the current machine learning applications and models, to
provide explanations for their decisions besides excellent performance. In our
work, we a) present an addressee estimation model with improved performance in
comparison with the previous state-of-the-art; b) further modify this model to
include inherently explainable attention-based segments; c) implement the
explainable addressee estimation as part of a modular cognitive architecture
for multi-party conversation in an iCub robot; d) validate the real-time
performance of the explainable model in multi-party human-robot interaction; e)
propose several ways to incorporate explainability and transparency in the
aforementioned architecture; and f) perform an online user study to analyze the
effect of various explanations on how human participants perceive the robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32pp (+6pp sup.mat.) Accepted in Computer Vision and Image
  Understanding Journal on January 23, 2025. This research received funding
  Horizon-Europe TERAIS project (G.A. 101079338) and Slovak Research and
  Development Agency, project no. APVV-21-0105</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning More With Less: Sample Efficient Dynamics Learning and
  Model-Based RL for Loco-Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Hoffman, Jin Cheng, Chenhao Li, Stelian Coros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining the agility of legged locomotion with the capabilities of
manipulation, loco-manipulation platforms have the potential to perform complex
tasks in real-world applications. To this end, state-of-the-art quadrupeds with
attached manipulators, such as the Boston Dynamics Spot, have emerged to
provide a capable and robust platform. However, both the complexity of
loco-manipulation control, as well as the black-box nature of commercial
platforms pose challenges for developing accurate dynamics models and control
policies. We address these challenges by developing a hand-crafted kinematic
model for a quadruped-with-arm platform and, together with recent advances in
Bayesian Neural Network (BNN)-based dynamics learning using physical priors,
efficiently learn an accurate dynamics model from data. We then derive control
policies for loco-manipulation via model-based reinforcement learning (RL). We
demonstrate the effectiveness of this approach on hardware using the Boston
Dynamics Spot with a manipulator, accurately performing dynamic end-effector
trajectory tracking even in low data regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Master Thesis at ETH Zurich</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpatialVLA: Exploring Spatial Representations for Visual-Language-Action
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15830v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15830v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we claim that spatial understanding is the keypoint in robot
manipulation, and propose SpatialVLA to explore effective spatial
representations for the robot foundation model. Specifically, we introduce
Ego3D Position Encoding to inject 3D information into the input observations of
the visual-language-action model, and propose Adaptive Action Grids to
represent spatial robot movement actions with adaptive discretized action
grids, facilitating learning generalizable and transferrable spatial action
knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a
vision-language model with 1.1 Million real-world robot episodes, to learn a
generalist manipulation policy across multiple robot environments and tasks.
After pre-training, SpatialVLA is directly applied to perform numerous tasks in
a zero-shot manner. The superior results in both simulation and real-world
robots demonstrate its advantage of inferring complex robot motion trajectories
and its strong in-domain multi-task generalization ability. We further show the
proposed Adaptive Action Grids offer a new and effective way to fine-tune the
pre-trained SpatialVLA model for new simulation and real-world setups, where
the pre-learned action grids are re-discretized to capture robot-specific
spatial action movements of new setups. The superior results from extensive
evaluations demonstrate the exceptional in-distribution generalization and
out-of-distribution adaptation capability, highlighting the crucial benefit of
the proposed spatial-aware representations for generalist robot policy
learning. All the details and codes will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Surgical Activity Grammar for Primary Intention Prediction in
  Laparoscopy Procedures <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Song Zhou, Yiwei Wang, Chidan Wan, Huan Zhao, Xiong Cai, Han Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical procedures are inherently complex and dynamic, with intricate
dependencies and various execution paths. Accurate identification of the
intentions behind critical actions, referred to as Primary Intentions (PIs), is
crucial to understanding and planning the procedure. This paper presents a
novel framework that advances PI recognition in instructional videos by
combining top-down grammatical structure with bottom-up visual cues. The
grammatical structure is based on a rich corpus of surgical procedures,
offering a hierarchical perspective on surgical activities. A grammar parser,
utilizing the surgical activity grammar, processes visual data obtained from
laparoscopic images through surgical action detectors, ensuring a more precise
interpretation of the visual information. Experimental results on the benchmark
dataset demonstrate that our method outperforms existing surgical activity
detectors that rely solely on visual features. Our research provides a
promising foundation for developing advanced robotic surgical systems with
enhanced planning and automation capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Control Barrier Functions using Uncertainty Estimation with
  Application to Mobile Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01881v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01881v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ersin Das, Joel W. Burdick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a safety-critical control design approach for nonlinear
control affine systems in the presence of matched and unmatched uncertainties.
Our constructive framework couples control barrier function (CBF) theory with a
new uncertainty estimator to ensure robust safety. We use the estimated
uncertainty, along with a derived upper bound on the estimation error, for
synthesizing CBFs and safety-critical controllers via a quadratic program-based
feedback control law that rigorously ensures robust safety while improving
disturbance rejection performance. We extend the method to higher-order CBFs
(HOCBFs) to achieve safety under unmatched uncertainty, which may cause
relative degree differences with respect to control input and disturbances. We
assume the relative degree difference is at most one, resulting in a
second-order cone constraint. We demonstrate the proposed robust HOCBF method
through a simulation of an uncertain elastic actuator control problem and
experimentally validate the efficacy of our robust CBF framework on a tracked
robot with slope-induced matched and unmatched perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-30T00:00:00Z">2025-01-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">34</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in Multimodal Adaptation and Generalization: From Traditional
  Approaches to Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/donghao51/Awesome-Multimodal-Adaptation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAM2Act: Integrating Visual Foundation Model with A Memory Architecture
  for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoquan Fang, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox, Ranjay Krishna, Jiafei Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation systems operating in diverse, dynamic environments must
exhibit three critical abilities: multitask interaction, generalization to
unseen scenarios, and spatial memory. While significant progress has been made
in robotic manipulation, existing approaches often fall short in generalization
to complex environmental variations and addressing memory-dependent tasks. To
bridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based
policy that leverages multi-resolution upsampling with visual representations
from large-scale foundation model. SAM2Act achieves a state-of-the-art average
success rate of 86.8% across 18 tasks in the RLBench benchmark, and
demonstrates robust generalization on The Colosseum benchmark, with only a 4.3%
performance gap under diverse environmental perturbations. Building on this
foundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2,
which incorporates a memory bank, an encoder, and an attention mechanism to
enhance spatial memory. To address the need for evaluating memory-dependent
tasks, we introduce MemoryBench, a novel benchmark designed to assess spatial
memory and action recall in robotic manipulation. SAM2Act+ achieves competitive
performance on MemoryBench, significantly outperforming existing approaches and
pushing the boundaries of memory-enabled robotic systems. Project page:
https://sam2act.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Including Appendix, Project page: https://sam2act.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Priors of Human Motion With Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Placido Falqueto, Alberto Sanfeliu, Luigi Palopoli, Daniele Fontanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A clear understanding of where humans move in a scenario, their usual paths
and speeds, and where they stop, is very important for different applications,
such as mobility studies in urban areas or robot navigation tasks within
human-populated environments. We propose in this article, a neural architecture
based on Vision Transformers (ViTs) to provide this information. This solution
can arguably capture spatial correlations more effectively than Convolutional
Neural Networks (CNNs). In the paper, we describe the methodology and proposed
neural architecture and show the experiments' results with a standard dataset.
We show that the proposed ViT architecture improves the metrics compared to a
method based on a CNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE 48th Annual Computers, Software, and Applications
  Conference (COMPSAC). IEEE, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn from the Past: Language-conditioned Object Rearrangement with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqun Cao, Ryan Mckenna, John Oyekan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object rearrangement is a significant task for collaborative robots, where
they are directed to manipulate objects into a specified goal state.
Determining the placement of objects is a major challenge that influences the
efficiency of the rearrangement process. Most current methods heavily rely on
pre-collected datasets to train the model for predicting the goal position and
are restricted to specific instructions, which limits their broader
applicability and effectiveness.In this paper, we propose a framework of
language-conditioned object rearrangement based on the Large Language Model
(LLM). Particularly, our approach mimics human reasoning by using past
successful experiences as a reference to infer the desired goal position. Based
on LLM's strong natural language comprehension and inference ability, our
method can generalise to handle various everyday objects and free-form language
instructions in a zero-shot manner. Experimental results demonstrate that our
methods can effectively execute the robotic rearrangement tasks, even those
involving long sequential orders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Planning and Optimization for Cuspidal 6R Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander J. Elias, John T. Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A cuspidal robot can move from one inverse kinematics (IK) solution to
another without crossing a singularity. Multiple industrial robots are
cuspidal. They tend to have a beautiful mechanical design, but they pose path
planning challenges. A task-space path may have a valid IK solution for each
point along the path, but a continuous joint-space path may depend on the
choice of the IK solution or even be infeasible. This paper presents new
analysis, path planning, and optimization methods to enhance the utility of
cuspidal robots. We first demonstrate an efficient method to identify cuspidal
robots and show, for the first time, that the ABB GoFa and certain robots with
three parallel joint axes are cuspidal. We then propose a new path planning
method for cuspidal robots by finding all IK solutions for each point along a
task-space path and constructing a graph to connect each vertex corresponding
to an IK solution. Graph edges are weighted based on the optimization metric,
such as minimizing joint velocity. The optimal feasible path is the shortest
path in the graph. This method can find non-singular paths as well as smooth
paths which pass through singularities. Finally, this path planning method is
incorporated into a path optimization algorithm. Given a fixed workspace
toolpath, we optimize the offset of the toolpath in the robot base frame while
ensuring continuous joint motion. Code examples are available in a publicly
accessible repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum-based Sample Efficient Reinforcement Learning for Robust
  Stabilization of a Quadrotor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fausto Mauricio Lagos Suarez, Akshit Saradagi, Vidya Sumathy, Shruti Kotpaliwar, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces a curriculum learning approach to develop a
reinforcement learning-based robust stabilizing controller for a Quadrotor that
meets predefined performance criteria. The learning objective is to achieve
desired positions from random initial conditions while adhering to both
transient and steady-state performance specifications. This objective is
challenging for conventional one-stage end-to-end reinforcement learning, due
to the strong coupling between position and orientation dynamics, the
complexity in designing and tuning the reward function, and poor sample
efficiency, which necessitates substantial computational resources and leads to
extended convergence times. To address these challenges, this work decomposes
the learning objective into a three-stage curriculum that incrementally
increases task complexity. The curriculum begins with learning to achieve
stable hovering from a fixed initial condition, followed by progressively
introducing randomization in initial positions, orientations and velocities. A
novel additive reward function is proposed, to incorporate transient and
steady-state performance specifications. The results demonstrate that the
Proximal Policy Optimization (PPO)-based curriculum learning approach, coupled
with the proposed reward structure, achieves superior performance compared to a
single-stage PPO-trained policy with the same reward function, while
significantly reducing computational resource requirements and convergence
time. The curriculum-trained policy's performance and robustness are thoroughly
validated under random initial conditions and in the presence of disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomy and Safety Assurance in the Early Development of Robotics and
  Autonomous Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhaminda B. Abeywickrama, Michael Fisher, Frederic Wheeler, Louise Dennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report provides an overview of the workshop titled Autonomy and Safety
Assurance in the Early Development of Robotics and Autonomous Systems, hosted
by the Centre for Robotic Autonomy in Demanding and Long-Lasting Environments
(CRADLE) on September 2, 2024, at The University of Manchester, UK. The event
brought together representatives from six regulatory and assurance bodies
across diverse sectors to discuss challenges and evidence for ensuring the
safety of autonomous and robotic systems, particularly autonomous inspection
robots (AIR). The workshop featured six invited talks by the regulatory and
assurance bodies. CRADLE aims to make assurance an integral part of engineering
reliable, transparent, and trustworthy autonomous systems. Key discussions
revolved around three research questions: (i) challenges in assuring safety for
AIR; (ii) evidence for safety assurance; and (iii) how assurance cases need to
differ for autonomous systems. Following the invited talks, the breakout groups
further discussed the research questions using case studies from ground (rail),
nuclear, underwater, and drone-based AIR. This workshop offered a valuable
opportunity for representatives from industry, academia, and regulatory bodies
to discuss challenges related to assured autonomy. Feedback from participants
indicated a strong willingness to adopt a design-for-assurance process to
ensure that robots are developed and verified to meet regulatory expectations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic
  Navigation in Unstructured Outdoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Zhang, Hanlin Dong, Jian Yang, Jiahui Liu, Shibo Huang, Ke Li, Xuan Tang, Xian Wei, Xiong You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path planning with strong environmental adaptability plays a crucial role in
robotic navigation in unstructured outdoor environments, especially in the case
of low-quality location and map information. The path planning ability of a
robot depends on the identification of the traversability of global and local
ground areas. In real-world scenarios, the complexity of outdoor open
environments makes it difficult for robots to identify the traversability of
ground areas that lack a clearly defined structure. Moreover, most existing
methods have rarely analyzed the integration of local and global traversability
identifications in unstructured outdoor scenarios. To address this problem, we
propose a novel method, Dual-BEV Nav, first introducing Bird's Eye View (BEV)
representations into local planning to generate high-quality traversable paths.
Then, these paths are projected onto the global traversability map generated by
the global BEV planning model to obtain the optimal waypoints. By integrating
the traversability from both local and global BEV, we establish a dual-layer
BEV heuristic planning paradigm, enabling long-distance navigation in
unstructured outdoor environments. We test our approach through both public
dataset evaluations and real-world robot deployments, yielding promising
results. Compared to baselines, the Dual-BEV Nav improved temporal distance
prediction accuracy by up to $18.7\%$. In the real-world deployment, under
conditions significantly different from the training set and with notable
occlusions in the global BEV, the Dual-BEV Nav successfully achieved a
65-meter-long outdoor navigation. Further analysis demonstrates that the local
BEV representation significantly enhances the rationality of the planning,
while the global BEV probability map ensures the robustness of the overall
planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surface Defect Identification using Bayesian Filtering on a 3D Mesh 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Dalle Vedove, Matteo Bonetto, Edoardo Lamon, Luigi Palopoli, Matteo Saveriano, Daniele Fontanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a CAD-based approach for automated surface defect
detection. We leverage the a-priori knowledge embedded in a CAD model and
integrate it with point cloud data acquired from commercially available stereo
and depth cameras. The proposed method first transforms the CAD model into a
high-density polygonal mesh, where each vertex represents a state variable in
3D space. Subsequently, a weighted least squares algorithm is employed to
iteratively estimate the state of the scanned workpiece based on the captured
point cloud measurements. This framework offers the potential to incorporate
information from diverse sensors into the CAD domain, facilitating a more
comprehensive analysis. Preliminary results demonstrate promising performance,
with the algorithm achieving convergence to a sub-millimeter standard deviation
in the region of interest using only approximately 50 point cloud samples. This
highlights the potential of utilising commercially available stereo cameras for
high-precision quality control applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at IMEKO2024 World Congress, Hamburg, Germany, 26-29
  October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge in multi-robot systems: an interplay of dynamics, computation
  and communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Cignarale, Stephan Felber, Eric Goubault, Bernardo Hummes Flores, Hugo Rincon Galeana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that the hybrid systems perspective of distributed multi-robot
systems is compatible with logical models of knowledge already used in
distributed computing, and demonstrate its usefulness by deriving sufficient
epistemic conditions for exploration and gathering robot tasks to be solvable.
We provide a separation of the physical and computational aspects of a robotic
system, allowing us to decouple the problems related to each and directly use
methods from control theory and distributed computing, fields that are
traditionally distant in the literature. Finally, we demonstrate a novel
approach for reasoning about the knowledge in multi-robot systems through a
principled method of converting a switched hybrid dynamical system into a
temporal-epistemic logic model, passing through an abstract state machine
representation. This creates space for methods and results to be exchanged
across the fields of control theory, distributed computing and
temporal-epistemic logic, while reasoning about multi-robot systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPD: Guided Polynomial Diffusion for Motion Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajit Srikanth, Parth Mahanjan, Kallol Saha, Vishal Mandadi, Pranjal Paul, Pawan Wadhwani, Brojeshwar Bhowmick, Arun Singh, Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based motion planners are becoming popular due to their
well-established performance improvements, stemming from sample diversity and
the ease of incorporating new constraints directly during inference. However, a
primary limitation of the diffusion process is the requirement for a
substantial number of denoising steps, especially when the denoising process is
coupled with gradient-based guidance. In this paper, we introduce, diffusion in
the parametric space of trajectories, where the parameters are represented as
Bernstein coefficients. We show that this representation greatly improves the
effectiveness of the cost function guidance and the inference speed. We also
introduce a novel stitching algorithm that leverages the diversity in
diffusion-generated trajectories to produce collision-free trajectories with
just a single cost function-guided model. We demonstrate that our approaches
outperform current SOTA diffusion-based motion planners for manipulators and
provide an ablation study on key components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-Line Learning for Planning and Control of Underactuated Robots with
  Uncertain Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Turrisi, Marco Capotondi, Claudio Gaz, Valerio Modugno, Giuseppe Oriolo, Alessandro De Luca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an iterative approach for planning and controlling motions of
underactuated robots with uncertain dynamics. At its core, there is a learning
process which estimates the perturbations induced by the model uncertainty on
the active and passive degrees of freedom. The generic iteration of the
algorithm makes use of the learned data in both the planning phase, which is
based on optimization, and the control phase, where partial feedback
linearization of the active dofs is performed on the model updated on-line. The
performance of the proposed approach is shown by comparative simulations and
experiments on a Pendubot executing various types of swing-up maneuvers. Very
few iterations are typically needed to generate dynamically feasible
trajectories and the tracking control that guarantees their accurate execution,
even in the presence of large model uncertainties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IROAM: Improving Roadside Monocular 3D Object Detection Learning from
  Autonomous Vehicle Data Domain <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Xiaoliang Huo, Siqi Fan, Jingjing Liu, Ya-Qin Zhang, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, The perception capabilities of the ego-vehicle can be
improved with roadside sensors, which can provide a holistic view of the
environment. However, existing monocular detection methods designed for vehicle
cameras are not suitable for roadside cameras due to viewpoint domain gaps. To
bridge this gap and Improve ROAdside Monocular 3D object detection, we propose
IROAM, a semantic-geometry decoupled contrastive learning framework, which
takes vehicle-side and roadside data as input simultaneously. IROAM has two
significant modules. In-Domain Query Interaction module utilizes a transformer
to learn content and depth information for each domain and outputs object
queries. Cross-Domain Query Enhancement To learn better feature representations
from two domains, Cross-Domain Query Enhancement decouples queries into
semantic and geometry parts and only the former is used for contrastive
learning. Experiments demonstrate the effectiveness of IROAM in improving
roadside detector's performance. The results validate that IROAM has the
capabilities to learn cross-domain information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifelong 3D Mapping Framework for Hand-held & Robot-mounted LiDAR
  Mapping Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liudi Yang, Sai Manoj Prakhya, Senhua Zhu, Ziyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a lifelong 3D mapping framework that is modular, cloud-native by
design and more importantly, works for both hand-held and robot-mounted 3D
LiDAR mapping systems. Our proposed framework comprises of dynamic point
removal, multi-session map alignment, map change detection and map version
control. First, our sensor-setup agnostic dynamic point removal algorithm works
seamlessly with both hand-held and robot-mounted setups to produce clean static
3D maps. Second, the multi-session map alignment aligns these clean static maps
automatically, without manual parameter fine-tuning, into a single reference
frame, using a two stage approach based on feature descriptor matching and fine
registration. Third, our novel map change detection identifies positive and
negative changes between two aligned maps. Finally, the map version control
maintains a single base map that represents the current state of the
environment, and stores the detected positive and negative changes, and
boundary information. Our unique map version control system can reconstruct any
of the previous clean session maps and allows users to query changes between
any two random mapping sessions, all without storing any input raw session
maps, making it very unique. Extensive experiments are performed using
hand-held commercial LiDAR mapping devices and open-source robot-mounted LiDAR
SLAM algorithms to evaluate each module and the whole 3D lifelong mapping
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward Prediction Error Prioritisation in Experience Replay: The RPE-PER
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoda Yamani, Yuning Xing, Lee Violet C. Ong, Bruce A. MacDonald, Henry Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning algorithms aim to learn optimal control strategies
through iterative interactions with an environment. A critical element in this
process is the experience replay buffer, which stores past experiences,
allowing the algorithm to learn from a diverse range of interactions rather
than just the most recent ones. This buffer is especially essential in dynamic
environments with limited experiences. However, efficiently selecting
high-value experiences to accelerate training remains a challenge. Drawing
inspiration from the role of reward prediction errors (RPEs) in biological
systems, where they are essential for adaptive behaviour and learning, we
introduce Reward Predictive Error Prioritised Experience Replay (RPE-PER). This
novel approach prioritises experiences in the buffer based on RPEs. Our method
employs a critic network, EMCN, that predicts rewards in addition to the
Q-values produced by standard critic networks. The discrepancy between these
predicted and actual rewards is computed as RPE and utilised as a signal for
experience prioritisation. Experimental evaluations across various continuous
control tasks demonstrate RPE-PER's effectiveness in enhancing the learning
speed and performance of off-policy actor-critic algorithms compared to
baseline approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted for presentation at the 2024 Australasian
  Conference on Robotics and Automation (ACRA 2024). It consists of 10 pages,
  including four figures and two tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIAL: Distribution-Informed Adaptive Learning of Multi-Task Constraints
  for Safety-Critical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-Wook Yoo, Seung-Woo Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe reinforcement learning has traditionally relied on predefined constraint
functions to ensure safety in complex real-world tasks, such as autonomous
driving. However, defining these functions accurately for varied tasks is a
persistent challenge. Recent research highlights the potential of leveraging
pre-acquired task-agnostic knowledge to enhance both safety and sample
efficiency in related tasks. Building on this insight, we propose a novel
method to learn shared constraint distributions across multiple tasks. Our
approach identifies the shared constraints through imitation learning and then
adapts to new tasks by adjusting risk levels within these learned
distributions. This adaptability addresses variations in risk sensitivity
stemming from expert-specific biases, ensuring consistent adherence to general
safety principles even with imperfect demonstrations. Our method can be applied
to control and navigation domains, including multi-task and meta-task
scenarios, accommodating constraints such as maintaining safe distances or
adhering to speed limits. Experimental results validate the efficacy of our
approach, demonstrating superior safety performance and success rates compared
to baselines, all without requiring task-specific constraint definitions. These
findings underscore the versatility and practicality of our method across a
wide range of real-world tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures, 6 tables, submission to T-RO in 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Grasps and Regrasps for Complex Manipulation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Patankar, Dasharadhan Mahalingam, Nilanjan Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In complex manipulation tasks, e.g., manipulation by pivoting, the motion of
the object being manipulated has to satisfy path constraints that can change
during the motion. Therefore, a single grasp may not be sufficient for the
entire path, and the object may need to be regrasped. Additionally, geometric
data for objects from a sensor are usually available in the form of point
clouds. The problem of computing grasps and regrasps from point-cloud
representation of objects for complex manipulation tasks is a key problem in
endowing robots with manipulation capabilities beyond pick-and-place. In this
paper, we formalize the problem of grasping/regrasping for complex manipulation
tasks with objects represented by (partial) point clouds and present an
algorithm to solve it. We represent a complex manipulation task as a sequence
of constant screw motions. Using a manipulation plan skeleton as a sequence of
constant screw motions, we use a grasp metric to find graspable regions on the
object for every constant screw segment. The overlap of the graspable regions
for contiguous screws are then used to determine when and how many times the
object needs to be regrasped. We present experimental results on point cloud
data collected from RGB-D sensors to illustrate our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agile and Cooperative Aerial Manipulation of a Cable-Suspended Load 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihao Sun, Xuerui Wang, Dario Sanalitro, Antonio Franchi, Marco Tognon, Javier Alonso-Mora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadrotors can carry slung loads to hard-to-reach locations at high speed.
Since a single quadrotor has limited payload capacities, using a team of
quadrotors to collaboratively manipulate a heavy object is a scalable and
promising solution. However, existing control algorithms for multi-lifting
systems only enable low-speed and low-acceleration operations due to the
complex dynamic coupling between quadrotors and the load, limiting their use in
time-critical missions such as search and rescue. In this work, we present a
solution to significantly enhance the agility of cable-suspended multi-lifting
systems. Unlike traditional cascaded solutions, we introduce a trajectory-based
framework that solves the whole-body kinodynamic motion planning problem
online, accounting for the dynamic coupling effects and constraints between the
quadrotors and the load. The planned trajectory is provided to the quadrotors
as a reference in a receding-horizon fashion and is tracked by an onboard
controller that observes and compensates for the cable tension. Real-world
experiments demonstrate that our framework can achieve at least eight times
greater acceleration than state-of-the-art methods to follow agile
trajectories. Our method can even perform complex maneuvers such as flying
through narrow passages at high speed. Additionally, it exhibits high
robustness against load uncertainties and does not require adding any sensors
to the load, demonstrating strong practicality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Kresling Origami for Personalised Wrist Orthosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenying Liu, Shuai Mao, Yixing Lei, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wrist plays a pivotal role in facilitating motion dexterity and hand
functions. Wrist orthoses, from passive braces to active exoskeletons, provide
an effective solution for the assistance and rehabilitation of motor abilities.
However, the type of motions facilitated by currently available orthoses is
limited, with little emphasis on personalised design. To address these gaps,
this paper proposes a novel wrist orthosis design inspired by the Kresling
origami. The design can be adapted to accommodate various individual shape
parameters, which benefits from the topological variations and intrinsic
compliance of origami. Heat-sealable fabrics are used to replicate the
non-rigid nature of the Kresling origami. The orthosis is capable of six
distinct motion modes with a detachable tendon-based actuation system.
Experimental characterisation of the workspace has been conducted by activating
tendons individually. The maximum bending angle in each direction ranges from
18.81{\deg} to 32.63{\deg}. When tendons are pulled in combination, the maximum
bending angles in the dorsal, palmar, radial, and ulnar directions are
31.66{\deg}, 30.38{\deg}, 27.14{\deg}, and 14.92{\deg}, respectively. The
capability to generate complex motions such as the dart-throwing motion and
circumduction has also been experimentally validated. The work presents a
promising foundation for the development of personalised wrist orthoses for
training and rehabilitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the 2025 IEEE/RAS International
  Conference on Soft Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Stack, Diverse Vehicles: Checking Safe Portability of Automated
  Driving Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Nenchev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating an automated driving software stack into vehicles with variable
configuration is challenging, especially due to different hardware
characteristics. Further, to provide software updates to a vehicle fleet in the
field, the functional safety of every affected configuration has to be ensured.
These additional demands for dependability and the increasing hardware
diversity in automated driving make rigorous automatic analysis essential. This
paper addresses this challenge by using formal portability checking of adaptive
cruise controller code for different vehicle configurations. Given a formal
specification of the safe behavior, models of target configurations are
derived, which capture relevant effects of sensors, actuators and computing
platforms. A corresponding safe set is obtained and used to check if the
desired behavior is achievable on all targets. In a case study, portability
checking of a traditional and a neural network controller are performed
automatically within minutes for each vehicle hardware configuration. The check
provides feedback for necessary adaptations of the controllers, thus, allowing
rapid integration and testing of software or parameter changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint to appear in 2025 IEEE/SICE International Symposium on
  System Integration (SII)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating LMM Planners and 3D Skill Policies for Generalizable
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelei Li, Ge Yan, Annabella Macaluso, Mazeyu Ji, Xueyan Zou, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in visual reasoning capabilities of large multimodal
models (LMMs) and the semantic enrichment of 3D feature fields have expanded
the horizons of robotic capabilities. These developments hold significant
potential for bridging the gap between high-level reasoning from LMMs and
low-level control policies utilizing 3D feature fields. In this work, we
introduce LMM-3DP, a framework that can integrate LMM planners and 3D skill
Policies. Our approach consists of three key perspectives: high-level planning,
low-level control, and effective integration. For high-level planning, LMM-3DP
supports dynamic scene understanding for environment disturbances, a critic
agent with self-feedback, history policy memorization, and reattempts after
failures. For low-level control, LMM-3DP utilizes a semantic-aware 3D feature
field for accurate manipulation. In aligning high-level and low-level control
for robot actions, language embeddings representing the high-level policy are
jointly attended with the 3D feature field in the 3D transformer for seamless
integration. We extensively evaluate our approach across multiple skills and
long-horizon tasks in a real-world kitchen environment. Our results show a
significant 1.45x success rate increase in low-level control and an approximate
1.5x improvement in high-level planning accuracy compared to LLM-based
baselines. Demo videos and an overview of LMM-3DP are available at
https://lmm-3dp-release.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Policy Gradient Quality-Diversity with Massive Parallelization
  via Behavioral Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Mitsides, Maxence Faldor, Antoine Cully
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality-Diversity optimization comprises a family of evolutionary algorithms
aimed at generating a collection of diverse and high-performing solutions.
MAP-Elites (ME), a notable example, is used effectively in fields like
evolutionary robotics. However, the reliance of ME on random mutations from
Genetic Algorithms limits its ability to evolve high-dimensional solutions.
Methods proposed to overcome this include using gradient-based operators like
policy gradients or natural evolution strategies. While successful at scaling
ME for neuroevolution, these methods often suffer from slow training speeds, or
difficulties in scaling with massive parallelization due to high computational
demands or reliance on centralized actor-critic training. In this work, we
introduce a fast, sample-efficient ME based algorithm capable of scaling up
with massive parallelization, significantly reducing runtimes without
compromising performance. Our method, ASCII-ME, unlike existing policy gradient
quality-diversity methods, does not rely on centralized actor-critic training.
It performs behavioral variations based on time step performance metrics and
maps these variations to solutions using policy gradients. Our experiments show
that ASCII-ME can generate a diverse collection of high-performing deep neural
network policies in less than 250 seconds on a single GPU. Additionally, it
operates on average, five times faster than state-of-the-art algorithms while
still maintaining competitive sample efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Preference Optimization for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have recently been leveraged to generate
robotic actions, forming Vision-Language-Action (VLA) models. However, directly
adapting a pretrained VLM for robotic control remains challenging, particularly
when constrained by a limited number of robot demonstrations. In this work, we
introduce LLaRA: Large Language and Robotics Assistant, a framework that
formulates robot action policy as visuo-textual conversations and enables an
efficient transfer of a pretrained VLM into a powerful VLA, motivated by the
success of visual instruction tuning in Computer Vision. First, we present an
automated pipeline to generate conversation-style instruction tuning data for
robots from existing behavior cloning datasets, aligning robotic actions with
image pixel coordinates. Further, we enhance this dataset in a self-supervised
manner by defining six auxiliary tasks, without requiring any additional action
annotations. We show that a VLM finetuned with a limited amount of such
datasets can produce meaningful action decisions for robotic control. Through
experiments across multiple simulated and real-world tasks, we demonstrate that
LLaRA achieves state-of-the-art performance while preserving the generalization
capabilities of large language models. The code, datasets, and pretrained
models are available at https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeshDMP: Motion Planning on Discrete Manifolds using Dynamic Movement
  Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Dalle Vedove, Fares J. Abu-Dakka, Luigi Palopoli, Daniele Fontanelli, Matteo Saveriano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An open problem in industrial automation is to reliably perform tasks
requiring in-contact movements with complex workpieces, as current solutions
lack the ability to seamlessly adapt to the workpiece geometry. In this paper,
we propose a Learning from Demonstration approach that allows a robot
manipulator to learn and generalise motions across complex surfaces by
leveraging differential mathematical operators on discrete manifolds to embed
information on the geometry of the workpiece extracted from triangular meshes,
and extend the Dynamic Movement Primitives (DMPs) framework to generate motions
on the mesh surfaces. We also propose an effective strategy to adapt the motion
to different surfaces, by introducing an isometric transformation of the
learned forcing term. The resulting approach, namely MeshDMP, is evaluated both
in simulation and real experiments, showing promising results in typical
industrial automation tasks like car surface polishing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2025 IEEE International Conference on Robotics and
  Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Realtime Limb Trajectory Optimization for Humanoid Running Through
  Centroidal Angular Momentum Dynamics <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sait Sovukluk, Robert Schuller, Johannes Englsberger, Christian Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the essential aspects of humanoid robot running is determining the
limb-swinging trajectories. During the flight phases, where the ground reaction
forces are not available for regulation, the limb swinging trajectories are
significant for the stability of the next stance phase. Due to the conservation
of angular momentum, improper leg and arm swinging results in highly tilted and
unsustainable body configurations at the next stance phase landing. In such
cases, the robotic system fails to maintain locomotion independent of the
stability of the center of mass trajectories. This problem is more apparent for
fast and high flight time trajectories. This paper proposes a real-time
nonlinear limb trajectory optimization problem for humanoid running. The
optimization problem is tested on two different humanoid robot models, and the
generated trajectories are verified using a running algorithm for both robots
in a simulation environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the IEEE
  International Conference on Robotics and Automation (ICRA), Atlanta 2025. v2:
  - A Github link to the proposed optimization tool is added. - There are no
  changes in the method and results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual
  perception beyond human capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengbo Wang, Jingwen Zhao, Tongming Pu, Liangbing Zhao, Xiaoyu Guo, Yue Cheng, Cong Li, Weihao Ma, Chenyu Tang, Zhenyu Xu, Ningli Wang, Luigi Occhipinti, Arokia Nathan, Ravinder Dahiya, Huaqiang Wu, Li Tao, Shuo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow, inspired by the mechanisms of biological visual systems,
calculates spatial motion vectors within visual scenes that are necessary for
enabling robotics to excel in complex and dynamic working environments.
However, current optical flow algorithms, despite human-competitive task
performance on benchmark datasets, remain constrained by unacceptable time
delays (~0.6 seconds per inference, 4X human processing speed) in practical
deployment. Here, we introduce a neuromorphic optical flow approach that
addresses delay bottlenecks by encoding temporal information directly in a
synaptic transistor array to assist spatial motion analysis. Compared to
conventional spatial-only optical flow methods, our spatiotemporal neuromorphic
optical flow offers the spatial-temporal consistency of motion information,
rapidly identifying regions of interest in as little as 1-2 ms using the
temporal motion cues derived from the embedded temporal information in the
two-dimensional floating gate synaptic transistors. Thus, the visual input can
be selectively filtered to achieve faster velocity calculations and various
task execution. At the hardware level, due to the atomically sharp interfaces
between distinct functional layers in two-dimensional van der Waals
heterostructures, the synaptic transistor offers high-frequency response (~100
{\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000
cycles), enabling robust visual processing. In software benchmarks, our system
outperforms state-of-the-art algorithms with a 400% speedup, frequently
surpassing human-level performance while maintaining or enhancing accuracy by
utilizing the temporal priors provided by the embedded temporal information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Numerical Function Optimization Framework for Constrained
  Nonlinear Robotic Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sait Sovukluk, Christian Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a numerical function optimization framework designed for
constrained optimization problems in robotics. The tool is designed with
real-time considerations and is suitable for online trajectory and control
input optimization problems. The proposed framework does not require any
analytical representation of the problem and works with constrained block-box
optimization functions. The method combines first-order gradient-based line
search algorithms with constraint prioritization through nullspace projections
onto constraint Jacobian space. The tool is implemented in C++ and provided
online for community use, along with some numerical and robotic example
implementations presented in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to IFAC for possible publication. v2: -
  A link to the GitHub repository is added for the proposed optimization tool.
  - A reference is included for the Humanoid Robot Posture Optimization
  discussion. - There are no changes in the results or method - Implementation:
  https://github.com/ssovukluk/ENFORCpp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Explanations for Autonomous Robots: a Systematic <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Sobrín-Hidalgo, Ángel Manuel Guerrero-Higueras, Vicente Matellán-Olivera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building trust between humans and robots has long interested the robotics
community. Various studies have aimed to clarify the factors that influence the
development of user trust. In Human-Robot Interaction (HRI) environments, a
critical aspect of trust development is the robot's ability to make its
behavior understandable. The concept of an eXplainable Autonomous Robot (XAR)
addresses this requirement. However, giving a robot self-explanatory abilities
is a complex task. Robot behavior includes multiple skills and diverse
subsystems. This complexity led to research into a wide range of methods for
generating explanations about robot behavior. This paper presents a systematic
literature review that analyzes existing strategies for generating explanations
in robots and studies the current XAR trends. Results indicate promising
advancements in explainability systems. However, these systems are still unable
to fully cover the complex behavior of autonomous robots. Furthermore, we also
identify a lack of consensus on the theoretical concept of explainability, and
the need for a robust methodology to assess explainability methods and tools
has been identified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures, 10 tables. This paper is a preprint of an
  article submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation
  with Language Models <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10027v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10027v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Kim, Keonwoo Kim, Mintaek Oh, Hanbi Baek, Jiyang Lee, Donghwi Jung, Soojin Woo, Younkyung Woo, John Tucker, Roya Firoozi, Seung-Woo Seo, Mac Schwager, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown significant potential in guiding
embodied agents to execute language instructions across a range of tasks,
including robotic manipulation and navigation. However, existing methods are
primarily designed for static environments and do not leverage the agent's own
experiences to refine its initial plans. Given that real-world environments are
inherently stochastic, initial plans based solely on LLMs' general knowledge
may fail to achieve their objectives, unlike in static scenarios. To address
this limitation, this study introduces the Experience-and-Emotion Map (E2Map),
which integrates not only LLM knowledge but also the agent's real-world
experiences, drawing inspiration from human emotional responses. The proposed
methodology enables one-shot behavior adjustments by updating the E2Map based
on the agent's experiences. Our evaluation in stochastic navigation
environments, including both simulations and real-world scenarios, demonstrates
that the proposed method significantly enhances performance in stochastic
environments compared to existing LLM-based approaches. Code and supplementary
materials are available at https://e2map.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 28 figures. Project page: https://e2map.github.io. Accepted
  to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Use of Immersive Digital Technologies for Designing and Operating
  UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yousef Emami, Kai Li, Luis Almeida, Sai Zou, Wei Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) offer agile, secure and efficient solutions
for communication relay networks. However, their modeling and control are
challenging, and the mismatch between simulations and actual conditions limits
real-world deployment. Moreover, improving situational awareness is essential.
Several studies proposed integrating the operation of UAVs with immersive
digital technologies such as Digital Twin (DT) and Extended Reality (XR) to
overcome these challenges. This paper provides a comprehensive overview of the
latest research and developments involving immersive digital technologies for
UAVs. We explore the use of Machine Learning (ML) techniques, particularly Deep
Reinforcement Learning (DRL), to improve the capabilities of DT for UAV
systems. We provide discussion, identify key research gaps, and propose
countermeasures based on Generative AI (GAI), emphasizing the significant role
of AI in advancing DT technology for UAVs. Furthermore, we review the
literature, provide discussion, and examine how the XR technology can transform
UAV operations with the support of GAI, and explore its practical challenges.
Finally, we propose future research directions to further develop the
application of immersive digital technologies for UAV operation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARDuP: Active Region Video Diffusion for Universal Policies <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaiyi Huang, Mara Levy, Zhenyu Jiang, Anima Anandkumar, Yuke Zhu, Linxi Fan, De-An Huang, Abhinav Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential decision-making can be formulated as a text-conditioned video
generation problem, where a video planner, guided by a text-defined goal,
generates future frames visualizing planned actions, from which control actions
are subsequently derived. In this work, we introduce Active Region Video
Diffusion for Universal Policies (ARDuP), a novel framework for video-based
policy learning that emphasizes the generation of active regions, i.e.
potential interaction areas, enhancing the conditional policy's focus on
interactive areas critical for task execution. This innovative framework
integrates active region conditioning with latent diffusion models for video
planning and employs latent representations for direct action decoding during
inverse dynamic modeling. By utilizing motion cues in videos for automatic
active region discovery, our method eliminates the need for manual annotations
of active regions. We validate ARDuP's efficacy via extensive experiments on
simulator CLIPort and the real-world dataset BridgeData v2, achieving notable
improvements in success rates and generating convincingly realistic video
plans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IROS 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Robust Optimization for Multi-Agent Robotic Systems: A
  Distributed Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshiya Taj Abdul, Augustinos D. Saravanos, Evangelos A. Theodorou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel distributed robust optimization scheme for
steering distributions of multi-agent systems under stochastic and
deterministic uncertainty. Robust optimization is a subfield of optimization
which aims to discover an optimal solution that remains robustly feasible for
all possible realizations of the problem parameters within a given uncertainty
set. Such approaches would naturally constitute an ideal candidate for
multi-robot control, where in addition to stochastic noise, there might be
exogenous deterministic disturbances. Nevertheless, as these methods are
usually associated with significantly high computational demands, their
application to multi-agent robotics has remained limited. The scope of this
work is to propose a scalable robust optimization framework that effectively
addresses both types of uncertainties, while retaining computational efficiency
and scalability. In this direction, we provide tractable approximations for
robust constraints that are relevant in multi-robot settings. Subsequently, we
demonstrate how computations can be distributed through an Alternating
Direction Method of Multipliers (ADMM) approach towards achieving scalability
and communication efficiency. All improvements are also theoretically justified
by establishing and comparing the resulting computational complexities.
Simulation results highlight the performance of the proposed algorithm in
effectively handling both stochastic and deterministic uncertainty in
multi-robot systems. The scalability of the method is also emphasized by
showcasing tasks with up to hundreds of agents. The results of this work
indicate the promise of blending robust optimization, distribution steering and
distributed optimization towards achieving scalable, safe and robust
multi-robot control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clarke Transform and Encoder-Decoder Architecture for Arbitrary Joints
  Locations in Displacement-Actuated Continuum Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reinhard M. Grassmann, Jessica Burgner-Kahrs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider an arbitrary number of joints and their arbitrary
joint locations along the center line of a displacement-actuated continuum
robot. To achieve this, we revisit the derivation of the Clarke transform
leading to a formulation capable of considering arbitrary joint locations. The
proposed modified Clarke transform opens new opportunities in mechanical design
and algorithmic approaches beyond the current limiting dependency on symmetric
arranged joint locations. By presenting an encoder-decoder architecture based
on the Clarke transform, joint values between different robot designs can be
transformed enabling the use of an analogous robot design and direct knowledge
transfer. To demonstrate its versatility, applications of control and
trajectory generation in simulation are presented, which can be easily
integrated into an existing framework designed, for instance, for three
symmetric arranged joints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE International Conference on Soft
  Robotics (RoboSoft 2025), 8 pages, 11 figures, and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-29T00:00:00Z">2025-01-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">37</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRACE: Generalizing Robot-Assisted Caregiving with User Functionality
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziang Liu, Yuanchen Ju, Yu Da, Tom Silver, Pranav N. Thakkar, Jenna Li, Justin Guo, Katherine Dimitropoulou, Tapomayukh Bhattacharjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot caregiving should be personalized to meet the diverse needs of care
recipients -- assisting with tasks as needed, while taking user agency in
action into account. In physical tasks such as handover, bathing, dressing, and
rehabilitation, a key aspect of this diversity is the functional range of
motion (fROM), which can vary significantly between individuals. In this work,
we learn to predict personalized fROM as a way to generalize robot
decision-making in a wide range of caregiving tasks. We propose a novel
data-driven method for predicting personalized fROM using functional assessment
scores from occupational therapy. We develop a neural model that learns to
embed functional assessment scores into a latent representation of the user's
physical function. The model is trained using motion capture data collected
from users with emulated mobility limitations. After training, the model
predicts personalized fROM for new users without motion capture. Through
simulated experiments and a real-robot user study, we show that the
personalized fROM predictions from our model enable the robot to provide
personalized and effective assistance while improving the user's agency in
action. See our website for more visualizations:
https://emprise.cs.cornell.edu/grace/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, Accepted to IEEE/ACM International Conference on
  Human-Robot Interaction (HRI), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UGSim: Autonomous Buoyancy-Driven Underwater Glider Simulator with LQR
  Control Strategy and Recursive Guidance System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizun Xu, Yang Song, Jiabao Zhu, Weichao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the UGSim, a simulator for buoyancy-driven gliders, with
a LQR control strategy, and a recursive guidance system. Building on the top of
the DAVE and the UUVsim, it is designed to address unique challenges that come
from the complex hydrodynamic and hydrostatic impacts on buoyancy-driven
gliders, which conventional robotics simulators can't deal with. Since
distinguishing features of the class of vehicles, general controllers and
guidance systems developed for underwater robotics are infeasible. The
simulator is provided to accelerate the development and the evaluation of
algorithms that would otherwise require expensive and time-consuming operations
at sea. It consists of a basic kinetic module, a LQR control module and a
recursive guidance module, which allows the user to concentrate on the single
problem rather than the whole robotics system and the software infrastructure.
We demonstrate the usage of the simulator through an example, loading the
configuration of the buoyancy-driven glider named Petrel-II, presenting its
dynamics simulation, performances of the control strategy and the guidance
system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Sparse to Dense: Toddler-inspired Reward Transition in
  Goal-Oriented Reinforcement Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junseok Park, Hyeonseo Yang, Min Whoo Lee, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) agents often face challenges in balancing
exploration and exploitation, particularly in environments where sparse or
dense rewards bias learning. Biological systems, such as human toddlers,
naturally navigate this balance by transitioning from free exploration with
sparse rewards to goal-directed behavior guided by increasingly dense rewards.
Inspired by this natural progression, we investigate the Toddler-Inspired
Reward Transition in goal-oriented RL tasks. Our study focuses on transitioning
from sparse to potential-based dense (S2D) rewards while preserving optimal
strategies. Through experiments on dynamic robotic arm manipulation and
egocentric 3D navigation tasks, we demonstrate that effective S2D reward
transitions significantly enhance learning performance and sample efficiency.
Additionally, using a Cross-Density Visualizer, we show that S2D transitions
smooth the policy loss landscape, resulting in wider minima that improve
generalization in RL models. In addition, we reinterpret Tolman's maze
experiments, underscoring the critical role of early free exploratory learning
in the context of S2D rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of AAAI 2024 paper: Unveiling the Significance of
  Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning.
  This manuscript is currently being prepared for journal submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Propeller Motion of a Devil-Stick using Normal Forcing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Khandelwal, Ranjan Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of realizing rotary propeller motion of a devil-stick in the
vertical plane using forces purely normal to the stick is considered. This
problem represents a nonprehensile manipulation task of an underactuated
system. In contrast with previous approaches, the devil-stick is manipulated by
controlling the normal force and its point of application. Virtual holonomic
constraints are used to design the trajectory of the center-of-mass of the
devil-stick in terms of its orientation angle, and conditions for stable
propeller motion are derived. Intermittent large-amplitude forces are used to
asymptotically stabilize a desired propeller motion. Simulations demonstrate
the efficacy of the approach in realizing stable propeller motion without loss
of contact between the actuator and devil-stick.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafePR: Unified Approach for Safe Parallel Robots by Contact Detection
  and Reaction with Redundancy Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aran Mohammad, Tim-Lukas Habich, Thomas Seel, Moritz Schappler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast and safe motion is crucial for the successful deployment of physically
interactive robots. Parallel robots (PRs) offer the potential for higher speeds
while maintaining the same energy limits due to their low moving masses.
However, they require methods for contact detection and reaction while avoiding
singularities and self-collisions. We address this issue and present SafePR - a
unified approach for the detection and localization, including the distinction
between collision and clamping to perform a reaction that is safe for humans
and feasible for PRs. Our approach uses information from the encoders and motor
currents to estimate forces via a generalized-momentum observer. Neural
networks and particle filters classify and localize the contacts. We introduce
reactions with redundancy resolution to avoid type-II singularities and
self-collisions. Our approach detected and terminated 72 real-world collision
and clamping contacts with end-effector speeds of up to 1.5 m/s, each within
25-275 ms. The forces were below the thresholds from ISO/TS 15066. By using
built-in sensors, SafePR enables safe interaction with already assembled PRs
without the need for new hardware components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of the navigation of magnetic microrobots through cerebral
  bifurcations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro G. Alves, Maria Pinto, Rosa Moreira, Derick Sivakumaran, Fabian C. Landers, Maria Guix, Bradley J. Nelson, Andreas D. Flouris, Salvador Pané, Josep Puigmartí-Luis, Tiago Sotto Mayor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local administration of thrombolytics in ischemic stroke could accelerate
clot lysis and the ensuing reperfusion while minimizing the side effects of
systemic administration. Medical microrobots could be injected into the
bloodstream and magnetically navigated to the clot for administering the drugs
directly to the target. The magnetic manipulation required to navigate medical
microrobots will depend on various parameters such as the microrobots size, the
blood velocity, and the imposed magnetic field gradients. Numerical simulation
was used to study the motion of magnetically controlled microrobots flowing
through representative cerebral bifurcations, for predicting the magnetic
gradients required to navigate the microrobots from the injection point until
the target location. Upon thorough validation of the model against several
independent analytical and experimental results, the model was used to generate
maps and a predictive equation providing quantitative information on the
required magnetic gradients, for different scenarios. The developed maps and
predictive equation are crucial to inform the design, operation and
optimization of magnetic navigation systems for healthcare applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring Implicit Goals Across Differing Task Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Tulli, Stylianos Loukas Vasileiou, Mohamed Chetouani, Sarath Sreedharan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the significant challenges to generating value-aligned behavior is to
not only account for the specified user objectives but also any implicit or
unspecified user requirements. The existence of such implicit requirements
could be particularly common in settings where the user's understanding of the
task model may differ from the agent's estimate of the model. Under this
scenario, the user may incorrectly expect some agent behavior to be inevitable
or guaranteed. This paper addresses such expectation mismatch in the presence
of differing models by capturing the possibility of unspecified user subgoal in
the context of a task captured as a Markov Decision Process (MDP) and querying
for it as required. Our method identifies bottleneck states and uses them as
candidates for potential implicit subgoals. We then introduce a querying
strategy that will generate the minimal number of queries required to identify
a policy guaranteed to achieve the underlying goal. Our empirical evaluations
demonstrate the effectiveness of our approach in inferring and achieving
unstated goals across various tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intelligent System-on-a-Chip for a Real-Time Assessment of Fuel
  Consumption to Promote Eco-Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Óscar Mata-Carballeira, Mikel Díaz-Rodríguez, Inés del Campo, Victoria Martínez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pollution that originates from automobiles is a concern in the current world,
not only because of global warming, but also due to the harmful effects on
people's health and lives. Despite regulations on exhaust gas emissions being
applied, minimizing unsuitable driving habits that cause elevated fuel
consumption and emissions would achieve further reductions. For that reason,
this work proposes a self-organized map (SOM)-based intelligent system in order
to provide drivers with eco-driving-intended driving style (DS)
recommendations. The development of the DS advisor uses driving data from the
Uyanik instrumented car. The system classifies drivers regarding the underlying
causes of non-optimal DSs from the eco-driving viewpoint. When compared with
other solutions, the main advantage of this approach is the personalization of
the recommendations that are provided to motorists, comprising the handling of
the pedals and the gearbox, with potential improvements in both fuel
consumption and emissions ranging from the 9.5\% to the 31.5\%, or even higher
for drivers that are strongly engaged with the system. It was successfully
implemented using a field-programmable gate array (FPGA) device of the Xilinx
ZynQ programmable system-on-a-chip (PSoC) family. This SOM-based system allows
for real-time implementation, state-of-the-art timing performances, and low
power consumption, which are suitable for developing advanced driving
assistance systems (ADASs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning with Vision-Language Models and a Use Case in Robot-Assisted
  Teaching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuzhe Dang, Lada Kudláčková, Stefan Edelkamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating the generation of Planning Domain Definition Language (PDDL) with
Large Language Model (LLM) opens new research topic in AI planning,
particularly for complex real-world tasks. This paper introduces Image2PDDL, a
novel framework that leverages Vision-Language Models (VLMs) to automatically
convert images of initial states and descriptions of goal states into PDDL
problems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL
addresses key challenges in bridging perceptual understanding with symbolic
planning, reducing the expertise required to create structured problem
instances, and improving scalability across tasks of varying complexity. We
evaluate the framework on various domains, including standard planning domains
like blocksworld and sliding tile puzzles, using datasets with multiple
difficulty levels. Performance is assessed on syntax correctness, ensuring
grammar and executability, and content correctness, verifying accurate state
representation in generated PDDL problems. The proposed approach demonstrates
promising results across diverse task complexities, suggesting its potential
for broader applications in AI planning. We will discuss a potential use case
in robot-assisted teaching of students with Autism Spectrum Disorder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of the Motion Sickness and the Lack of Comfort in Car
  Passengers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Estibaliz Asua, Jon Gutiérrez-Zaballa, Óscar Mata-Carballeira, Jon Ander Ruiz, Inés del Campo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced driving assistance systems (ADAS) are primarily designed to increase
driving safety and reduce traffic congestion without paying too much attention
to passenger comfort or motion sickness. However, in view of autonomous cars,
and taking into account that the lack of comfort and motion sickness increase
in passengers, analysis from a comfort perspective is essential in the future
car investigation. The aim of this work is to study in detail how passenger's
comfort evaluation parameters vary depending on the driving style, car or road.
The database used has been developed by compiling the accelerations suffered by
passengers when three drivers cruise two different vehicles on different types
of routes. In order to evaluate both comfort and motion sickness, first, the
numerical values of the main comfort evaluation variables reported in the
literature have been analyzed. Moreover, a complementary statistical analysis
of probability density and a power spectral analysis are performed. Finally,
quantitative results are compared with passenger qualitative feedback. The
results show the high dependence of comfort evaluation variables' value with
the road type. In addition, it has been demonstrated that the driving style and
vehicle dynamics amplify or attenuate those values. Additionally, it has been
demonstrated that contributions from longitudinal and lateral accelerations
have a much greater effect in the lack of comfort than vertical ones. Finally,
based on the concrete results obtained, a new experimental campaign is
proposed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Path Finding Using Conflict-Based Search and
  Structural-Semantic Topometric Maps <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Fredriksson, Yifan Bai, Akshit Saradagi, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As industries increasingly adopt large robotic fleets, there is a pressing
need for computationally efficient, practical, and optimal conflict-free path
planning for multiple robots. Conflict-Based Search (CBS) is a popular method
for multi-agent path finding (MAPF) due to its completeness and optimality;
however, it is often impractical for real-world applications, as it is
computationally intensive to solve and relies on assumptions about agents and
operating environments that are difficult to realize. This article proposes a
solution to overcome computational challenges and practicality issues of CBS by
utilizing structural-semantic topometric maps. Instead of running CBS over
large grid-based maps, the proposed solution runs CBS over a sparse topometric
map containing structural-semantic cells representing intersections, pathways,
and dead ends. This approach significantly accelerates the MAPF process and
reduces the number of conflict resolutions handled by CBS while operating in
continuous time. In the proposed method, robots are assigned time ranges to
move between topometric regions, departing from the traditional CBS assumption
that a robot can move to any connected cell in a single time step. The approach
is validated through real-world multi-robot path-finding experiments and
benchmarking simulations. The results demonstrate that the proposed MAPF method
can be applied to real-world non-holonomic robots and yields significant
improvement in computational efficiency compared to traditional CBS methods
while improving conflict detection and resolution in cases of corridor
symmetries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 2025 IEEE International Conference on Robotics and
  Automation (ICRA), May 19-23, 2025, Atlanta, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An eco-driving approach for ride comfort improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Óscar Mata-Carballeira, Inés del Campo, Estibalitz Asua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New challenges on transport systems are emerging due to the advances that the
current paradigm is experiencing. The breakthrough of the autonomous car brings
concerns about ride comfort, while the pollution concerns have arisen in recent
years. In the model of automated automobiles, drivers are expected to become
passengers, so, they will be more prone to suffer from ride discomfort or
motion sickness. Conversely, the eco-driving implications should not be set
aside because of the influence of pollution on climate and people's health. For
that reason, a joint assessment of the aforementioned points would have a
positive impact. Thus, this work presents a self-organised map-based solution
to assess ride comfort features of individuals considering their driving style
from the viewpoint of eco-driving. For this purpose, a previously acquired
dataset from an instrumented car was used to classify drivers regarding the
causes of their lack of ride comfort and eco-friendliness. Once drivers are
classified regarding their driving style, natural-language-based
recommendations are proposed to increase the engagement with the system. Hence,
potential improvements of up to the 57.7% for ride comfort evaluation
parameters, as well as up to the 47.1% in greenhouse-gasses emissions are
expected to be reached.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Watch Your STEPP: Semantic Traversability Estimation using Pose
  Projected Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Ægidius, Dennis Hadjivelichkov, Jianhao Jiao, Jonathan Embley-Riches, Dimitrios Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the traversability of terrain is essential for autonomous robot
navigation, particularly in unstructured environments such as natural
landscapes. Although traditional methods, such as occupancy mapping, provide a
basic framework, they often fail to account for the complex mobility
capabilities of some platforms such as legged robots. In this work, we propose
a method for estimating terrain traversability by learning from demonstrations
of human walking. Our approach leverages dense, pixel-wise feature embeddings
generated using the DINOv2 vision Transformer model, which are processed
through an encoder-decoder MLP architecture to analyze terrain segments. The
averaged feature vectors, extracted from the masked regions of interest, are
used to train the model in a reconstruction-based framework. By minimizing
reconstruction loss, the network distinguishes between familiar terrain with a
low reconstruction error and unfamiliar or hazardous terrain with a higher
reconstruction error. This approach facilitates the detection of anomalies,
allowing a legged robot to navigate more effectively through challenging
terrain. We run real-world experiments on the ANYmal legged robot both indoor
and outdoor to prove our proposed method. The code is open-source, while video
demonstrations can be found on our website: https://rpl-cs-ucl.github.io/STEPP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian BIM-Guided Construction Robot Navigation with NLP Safety
  <span class="highlight-title">Prompt</span>s in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mani Amani, Reza Akhavian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Construction robotics increasingly relies on natural language processing for
task execution, creating a need for robust methods to interpret commands in
complex, dynamic environments. While existing research primarily focuses on
what tasks robots should perform, less attention has been paid to how these
tasks should be executed safely and efficiently. This paper presents a novel
probabilistic framework that uses sentiment analysis from natural language
commands to dynamically adjust robot navigation policies in construction
environments. The framework leverages Building Information Modeling (BIM) data
and natural language prompts to create adaptive navigation strategies that
account for varying levels of environmental risk and uncertainty. We introduce
an object-aware path planning approach that combines exponential potential
fields with a grid-based representation of the environment, where the potential
fields are dynamically adjusted based on the semantic analysis of user prompts.
The framework employs Bayesian inference to consolidate multiple information
sources: the static data from BIM, the semantic content of natural language
commands, and the implied safety constraints from user prompts. We demonstrate
our approach through experiments comparing three scenarios: baseline
shortest-path planning, safety-oriented navigation, and risk-aware routing.
Results show that our method successfully adapts path planning based on natural
language sentiment, achieving a 50\% improvement in minimum distance to
obstacles when safety is prioritized, while maintaining reasonable path
lengths. Scenarios with contrasting prompts, such as "dangerous" and "safe",
demonstrate the framework's ability to modify paths. This approach provides a
flexible foundation for integrating human knowledge and safety considerations
into construction robot navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Symposium on Automation and Robotics in
  Construction (ISARC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Aligned Skill Discovery: Balancing Behaviour Exploration and
  Alignment <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxence Hussonnois, Thommen George Karimpanal, Santu Rana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised skill discovery in Reinforcement Learning aims to mimic humans'
ability to autonomously discover diverse behaviors. However, existing methods
are often unconstrained, making it difficult to find useful skills, especially
in complex environments, where discovered skills are frequently unsafe or
impractical. We address this issue by proposing Human-aligned Skill Discovery
(HaSD), a framework that incorporates human feedback to discover safer, more
aligned skills. HaSD simultaneously optimises skill diversity and alignment
with human values. This approach ensures that alignment is maintained
throughout the skill discovery process, eliminating the inefficiencies
associated with exploring unaligned skills. We demonstrate its effectiveness in
both 2D navigation and SafetyGymnasium environments, showing that HaSD
discovers diverse, human-aligned skills that are safe and useful for downstream
tasks. Finally, we extend HaSD by learning a range of configurable skills with
varying degrees of diversity alignment trade-offs that could be useful in
practical scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 24th International Conference on Autonomous Agents
  and Multiagent Systems (AAMAS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certificated Actor-Critic: Hierarchical Reinforcement Learning with
  Control Barrier Functions for Safe Navigation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjun Xie, Shuhao Zhao, Liang Hu, Huijun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control Barrier Functions (CBFs) have emerged as a prominent approach to
designing safe navigation systems of robots. Despite their popularity, current
CBF-based methods exhibit some limitations: optimization-based safe control
techniques tend to be either myopic or computationally intensive, and they rely
on simplified system models; conversely, the learning-based methods suffer from
the lack of quantitative indication in terms of navigation performance and
safety. In this paper, we present a new model-free reinforcement learning
algorithm called Certificated Actor-Critic (CAC), which introduces a
hierarchical reinforcement learning framework and well-defined reward functions
derived from CBFs. We carry out theoretical analysis and proof of our
algorithm, and propose several improvements in algorithm implementation. Our
analysis is validated by two simulation experiments, showing the effectiveness
of our proposed CAC algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realtime Limb Trajectory Optimization for Humanoid Running Through
  Centroidal Angular Momentum Dynamics <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sait Sovukluk, Robert Schuller, Johannes Englsberger, Christian Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the essential aspects of humanoid robot running is determining the
limb-swinging trajectories. During the flight phases, where the ground reaction
forces are not available for regulation, the limb swinging trajectories are
significant for the stability of the next stance phase. Due to the conservation
of angular momentum, improper leg and arm swinging results in highly tilted and
unsustainable body configurations at the next stance phase landing. In such
cases, the robotic system fails to maintain locomotion independent of the
stability of the center of mass trajectories. This problem is more apparent for
fast and high flight time trajectories. This paper proposes a real-time
nonlinear limb trajectory optimization problem for humanoid running. The
optimization problem is tested on two different humanoid robot models, and the
generated trajectories are verified using a running algorithm for both robots
in a simulation environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twin-Enabled Real-Time Control in Robotic Additive Manufacturing
  via Soft Actor-Critic Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matsive Ali, Sandesh Giri, Sen Liu, Qin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart manufacturing systems increasingly rely on adaptive control mechanisms
to optimize complex processes. This research presents a novel approach
integrating Soft Actor-Critic (SAC) reinforcement learning with digital twin
technology to enable real-time process control in robotic additive
manufacturing. We demonstrate our methodology using a Viper X300s robot arm,
implementing two distinct control scenarios: static target acquisition and
dynamic trajectory following. The system architecture combines Unity's
simulation environment with ROS2 for seamless digital twin synchronization,
while leveraging transfer learning to efficiently adapt trained models across
tasks. Our hierarchical reward structure addresses common reinforcement
learning challenges including local minima avoidance, convergence acceleration,
and training stability. Experimental results show rapid policy convergence and
robust task execution in both simulated and physical environments, with
performance metrics including cumulative reward, value prediction accuracy,
policy loss, and discrete entropy coefficient demonstrating the effectiveness
of our approach. This work advances the integration of reinforcement learning
with digital twins for industrial robotics applications, providing a framework
for enhanced adaptive real-time control for smart additive manufacturing
process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belief Roadmaps with Uncertain Landmark Evanescence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erick Fuentes, Jared Strader, Ethan Fahnestock, Nicholas Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We would like a robot to navigate to a goal location while minimizing state
uncertainty. To aid the robot in this endeavor, maps provide a prior belief
over the location of objects and regions of interest. To localize itself within
the map, a robot identifies mapped landmarks using its sensors. However, as the
time between map creation and robot deployment increases, portions of the map
can become stale, and landmarks, once believed to be permanent, may disappear.
We refer to the propensity of a landmark to disappear as landmark evanescence.
Reasoning about landmark evanescence during path planning, and the associated
impact on localization accuracy, requires analyzing the presence or absence of
each landmark, leading to an exponential number of possible outcomes of a given
motion plan. To address this complexity, we develop BRULE, an extension of the
Belief Roadmap. During planning, we replace the belief over future robot poses
with a Gaussian mixture which is able to capture the effects of landmark
evanescence. Furthermore, we show that belief updates can be made efficient,
and that maintaining a random subset of mixture components is sufficient to
find high quality solutions. We demonstrate performance in simulated and
real-world experiments. Software is available at https://bit.ly/BRULE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Trajectory Replanner for Dynamically Grasping Irregular Objects <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Nhat Vu, Florian Grander, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new trajectory replanner for grasping irregular
objects. Unlike conventional grasping tasks where the object's geometry is
assumed simple, we aim to achieve a "dynamic grasp" of the irregular objects,
which requires continuous adjustment during the grasping process. To
effectively handle irregular objects, we propose a trajectory optimization
framework that comprises two phases. Firstly, in a specified time limit of 10s,
initial offline trajectories are computed for a seamless motion from an initial
configuration of the robot to grasp the object and deliver it to a pre-defined
target location. Secondly, fast online trajectory optimization is implemented
to update robot trajectories in real-time within 100 ms. This helps to mitigate
pose estimation errors from the vision system. To account for model
inaccuracies, disturbances, and other non-modeled effects, trajectory tracking
controllers for both the robot and the gripper are implemented to execute the
optimal trajectories from the proposed framework. The intensive experimental
results effectively demonstrate the performance of our trajectory planning
framework in both simulation and real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages. Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Grounded Differentiable Simulation for Soft Growing Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Chen, Yitian Gao, Sicheng Wang, Francesco Fuentes, Laura H. Blumenschein, Zachary Kingston
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft-growing robots (i.e., vine robots) are a promising class of soft robots
that allow for navigation and growth in tightly confined environments. However,
these robots remain challenging to model and control due to the complex
interplay of the inflated structure and inextensible materials, which leads to
obstacles for autonomous operation and design optimization. Although there
exist simulators for these systems that have achieved qualitative and
quantitative success in matching high-level behavior, they still often fail to
capture realistic vine robot shapes using simplified parameter models and have
difficulties in high-throughput simulation necessary for planning and parameter
optimization. We propose a differentiable simulator for these systems, enabling
the use of the simulator "in-the-loop" of gradient-based optimization
approaches to address the issues listed above. With the more complex parameter
fitting made possible by this approach, we experimentally validate and
integrate a closed-form nonlinear stiffness model for thin-walled inflated
tubes based on a first-principles approach to local material wrinkling. Our
simulator also takes advantage of data-parallel operations by leveraging
existing differentiable computation frameworks, allowing multiple simultaneous
rollouts. We demonstrate the feasibility of using a physics-grounded nonlinear
stiffness model within our simulator, and how it can be an effective tool in
sim-to-real transfer. We provide our implementation open source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures. IEEE-RAS International Conference on Soft
  Robotics (RoboSoft) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agricultural Industry Initiatives on Autonomy: How collaborative
  initiatives of VDMA and AEF can facilitate complexity in domain crossing
  harmonization needs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Happich, Alexander Grever, Julius Schöning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The agricultural industry is undergoing a significant transformation with the
increasing adoption of autonomous technologies. Addressing complex challenges
related to safety and security, components and validation procedures, and
liability distribution is essential to facilitate the adoption of autonomous
technologies. This paper explores the collaborative groups and initiatives
undertaken to address these challenges. These groups investigate inter alia
three focal topics: 1) describe the functional architecture of the operational
range, 2) define the work context, i.e., the realistic scenarios that emerge in
various agricultural applications, and 3) the static and dynamic detection
cases that need to be detected by sensor sets. Linked by the Agricultural
Operational Design Domain (Agri-ODD), use case descriptions, risk analysis, and
questions of liability can be handled. By providing an overview of these
collaborative initiatives, this paper aims to highlight the joint development
of autonomous agricultural systems that enhance the overall efficiency of
farming operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younggyo Seo, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning (RL), we train a value function to understand the
long-term consequence of executing a single action. However, the value of
taking each action can be ambiguous in robotics as robot movements are
typically the aggregate result of executing multiple small actions. Moreover,
robotic training data often consists of noisy trajectories, in which each
action is noisy but executing a series of actions results in a meaningful robot
movement. This further makes it difficult for the value function to understand
the effect of individual actions. To address this, we introduce Coarse-to-fine
Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that
learns a critic network that outputs Q-values over a sequence of actions, i.e.,
explicitly training the value function to learn the consequence of executing
action sequences. We study our algorithm on 53 robotic tasks with sparse and
dense rewards, as well as with and without demonstrations, from BiGym,
HumanoidBench, and RLBench. We find that CQN-AS outperforms various baselines,
in particular on humanoid control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages. Website: https://younggyo.me/cqn-as/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Model Predictive Control for Legged Robots through
  Distributed Optimization <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11742v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11742v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Amatucci, Giulio Turrisi, Angelo Bratta, Victor Barasuol, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to enhance Model Predictive Control
(MPC) for legged robots through Distributed Optimization. Our method focuses on
decomposing the robot dynamics into smaller, parallelizable subsystems, and
utilizing the Alternating Direction Method of Multipliers (ADMM) to ensure
consensus among them. Each subsystem is managed by its own Optimal Control
Problem, with ADMM facilitating consistency between their optimizations. This
approach not only decreases the computational time but also allows for
effective scaling with more complex robot configurations, facilitating the
integration of additional subsystems such as articulated arms on a quadruped
robot. We demonstrate, through numerical evaluations, the convergence of our
approach on two systems with increasing complexity. In addition, we showcase
that our approach converges towards the same solution when compared to a
state-of-the-art centralized whole-body MPC implementation. Moreover, we
quantitatively compare the computational efficiency of our method to the
centralized approach, revealing up to a 75% reduction in computational time.
Overall, our approach offers a promising avenue for accelerating MPC solutions
for legged robots, paving the way for more effective utilization of the
computational performance of modern hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Benefits of GPU Sample-Based Stochastic Predictive Controllers
  for Legged Locomotion <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11383v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11383v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Turrisi, Valerio Modugno, Lorenzo Amatucci, Dimitrios Kanoulas, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadrupedal robots excel in mobility, navigating complex terrains with
agility. However, their complex control systems present challenges that are
still far from being fully addressed. In this paper, we introduce the use of
Sample-Based Stochastic control strategies for quadrupedal robots, as an
alternative to traditional optimal control laws. We show that Sample-Based
Stochastic methods, supported by GPU acceleration, can be effectively applied
to real quadruped robots. In particular, in this work, we focus on achieving
gait frequency adaptation, a notable challenge in quadrupedal locomotion for
gradient-based methods. To validate the effectiveness of Sample-Based
Stochastic controllers we test two distinct approaches for quadrupedal robots
and compare them against a conventional gradient-based Model Predictive Control
system. Our findings, validated both in simulation and on a real 21Kg Aliengo
quadruped, demonstrate that our method is on par with a traditional Model
Predictive Control strategy when the robot is subject to zero or moderate
disturbance, while it surpasses gradient-based methods in handling sustained
external disturbances, thanks to the straightforward gait adaptation strategy
that is possible to achieve within their formulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quadrupedal Footstep Planning using Learned Motion Models of a Black-Box
  Controller 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyass Taouil, Giulio Turrisi, Daniel Schleich, Victor Barasuol, Claudio Semini, Sven Behnke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legged robots are increasingly entering new domains and applications,
including search and rescue, inspection, and logistics. However, for such
systems to be valuable in real-world scenarios, they must be able to
autonomously and robustly navigate irregular terrains. In many cases, robots
that are sold on the market do not provide such abilities, being able to
perform only blind locomotion. Furthermore, their controller cannot be easily
modified by the end-user, requiring a new and time-consuming control synthesis.
In this work, we present a fast local motion planning pipeline that extends the
capabilities of a black-box walking controller that is only able to track
high-level reference velocities. More precisely, we learn a set of motion
models for such a controller that maps high-level velocity commands to Center
of Mass (CoM) and footstep motions. We then integrate these models with a
variant of the A star algorithm to plan the CoM trajectory, footstep sequences,
and corresponding high-level velocity commands based on visual information,
allowing the quadruped to safely traverse irregular terrains at demand.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeSteps: Learning Safer Footstep Planning Policies for Legged Robots
  via Model-Based Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12664v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12664v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shafeef Omar, Lorenzo Amatucci, Victor Barasuol, Giulio Turrisi, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a footstep planning policy for quadrupedal locomotion that is able
to directly take into consideration a-priori safety information in its
decisions. At its core, a learning process analyzes terrain patches,
classifying each landing location by its kinematic feasibility, shin collision,
and terrain roughness. This information is then encoded into a small vector
representation and passed as an additional state to the footstep planning
policy, which furthermore proposes only safe footstep location by applying a
masked variant of the Proximal Policy Optimization algorithm. The performance
of the proposed approach is shown by comparative simulations and experiments on
an electric quadruped robot walking in different rough terrain scenarios. We
show that violations of the above safety conditions are greatly reduced both
during training and the successive deployment of the policy, resulting in an
inherently safer footstep planner. Furthermore, we show how, as a byproduct,
fewer reward terms are needed to shape the behavior of the policy, which in
return is able to achieve both better final performances and sample efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2023 IEEE-RAS International
  Conference on Humanoid Robots (Humanoids)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP-Motion: Learning Reward Functions for Robotic Actions Using
  Consecutive Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuzhe Dang, Stefan Edelkamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel method for learning reward functions for robotic
motions by harnessing the power of a CLIP-based model. Traditional reward
function design often hinges on manual feature engineering, which can struggle
to generalize across an array of tasks. Our approach circumvents this challenge
by capitalizing on CLIP's capability to process both state features and image
inputs effectively. Given a pair of consecutive observations, our model excels
in identifying the motion executed between them. We showcase results spanning
various robotic activities, such as directing a gripper to a designated target
and adjusting the position of a cube. Through experimental evaluations, we
underline the proficiency of our method in precisely deducing motion and its
promise to enhance reinforcement learning training in the realm of robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closed-loop Multi-step Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15384v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15384v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Lafratta, Bernd Porr, Christopher Chandler, Alice Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Living organisms interact with their surroundings in a closed-loop fashion,
where sensory inputs dictate the initiation and termination of behaviours. Even
simple animals are able to develop and execute complex plans, which has not yet
been replicated in robotics using pure closed-loop input control. We propose a
solution to this problem by defining a set of discrete and temporary
closed-loop controllers, called ``Tasks'', each representing a closed-loop
behaviour. We further introduce a supervisory module which has an innate
understanding of physics and causality, through which it can simulate the
execution of Task sequences over time and store the results in a model of the
environment. On the basis of this model, plans can be made by chaining
temporary closed-loop controllers. Our proposed framework was implemented for a
real robot and tested in two scenarios as proof of concept.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Decision to Action in Surgical Autonomy: Multi-Modal Large Language
  Models for Robot-Assisted Blood Suction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadra Zargarzadeh, Maryam Mirzaei, Yafei Ou, Mahdi Tavakoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs) has impacted research in robotics
and automation. While progress has been made in integrating LLMs into general
robotics tasks, a noticeable void persists in their adoption in more specific
domains such as surgery, where critical factors such as reasoning,
explainability, and safety are paramount. Achieving autonomy in robotic
surgery, which entails the ability to reason and adapt to changes in the
environment, remains a significant challenge. In this work, we propose a
multi-modal LLM integration in robot-assisted surgery for autonomous blood
suction. The reasoning and prioritization are delegated to the higher-level
task-planning LLM, and the motion planning and execution are handled by the
lower-level deep reinforcement learning model, creating a distributed agency
between the two components. As surgical operations are highly dynamic and may
encounter unforeseen circumstances, blood clots and active bleeding were
introduced to influence decision-making. Results showed that using a
multi-modal LLM as a higher-level reasoning unit can account for these surgical
complexities to achieve a level of reasoning previously unattainable in
robot-assisted surgeries. These findings demonstrate the potential of
multi-modal LLMs to significantly enhance contextual understanding and
decision-making in robotic-assisted surgeries, marking a step toward autonomous
surgical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication in IEEE Robotics and Automation Letters,
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysBench: Benchmarking and Enhancing Vision-Language Models for
  Physical World Understanding <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the physical world is a fundamental challenge in embodied AI,
critical for enabling agents to perform complex tasks and operate safely in
real-world environments. While Vision-Language Models (VLMs) have shown great
promise in reasoning and task planning for embodied agents, their ability to
comprehend physical phenomena remains extremely limited. To close this gap, we
introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'
physical world understanding capability across a diverse set of tasks.
PhysBench contains 10,002 entries of interleaved video-image-text data,
categorized into four major domains: physical object properties, physical
object relationships, physical scene understanding, and physics-based dynamics,
further divided into 19 subclasses and 8 distinct capability dimensions. Our
extensive experiments, conducted on 75 representative VLMs, reveal that while
these models excel in common-sense reasoning, they struggle with understanding
the physical world -- likely due to the absence of physical knowledge in their
training data and the lack of embedded physical priors. To tackle the
shortfall, we introduce PhysAgent, a novel framework that combines the
generalization strengths of VLMs with the specialized expertise of vision
models, significantly enhancing VLMs' physical understanding across a variety
of tasks, including an 18.4\% improvement on GPT-4o. Furthermore, our results
demonstrate that enhancing VLMs' physical world understanding capabilities can
help embodied agents such as MOKA. We believe that PhysBench and PhysAgent
offer valuable insights and contribute to bridging the gap between VLMs and
physical world understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://physbench.github.io/ Dataset:
  https://huggingface.co/datasets/USC-GVL/PhysBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmark Evaluations, Applications, and Challenges of Large Vision
  Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, Guangyao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Vision Language Models (VLMs) have emerged as a transformative
technology at the intersection of computer vision and natural language
processing, enabling machines to perceive and reason about the world through
both visual and textual modalities. For example, models such as CLIP, Claude,
and GPT-4V demonstrate strong reasoning and understanding abilities on visual
and textual data and beat classical single modality vision models on zero-shot
classification. Despite their rapid advancements in research and growing
popularity in applications, a comprehensive survey of existing studies on VLMs
is notably lacking, particularly for researchers aiming to leverage VLMs in
their specific domains. To this end, we provide a systematic overview of VLMs
in the following aspects: model information of the major VLMs developed over
the past five years (2019-2024); the main architectures and training methods of
these VLMs; summary and categorization of the popular benchmarks and evaluation
metrics of VLMs; the applications of VLMs including embodied agents, robotics,
and video generation; the challenges and issues faced by current VLMs such as
hallucination, fairness, and safety. Detailed collections including papers and
model repository links are listed in
https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Synergistic Framework for Learning Shape Estimation and Shape-Aware
  Whole-Body Control Policy for Continuum Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Kasaei, Farshid Alambeigi, Mohsen Khadem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel synergistic framework for learning shape
estimation and a shape-aware whole-body control policy for tendon-driven
continuum robots. Our approach leverages the interaction between two Augmented
Neural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and
Control-NODE -- to achieve continuous shape estimation and shape-aware control.
The Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it
to adapt and account for model mismatches, while the Control-NODE uses this
shape information to optimize a whole-body control policy, trained in a Model
Predictive Control (MPC) fashion. This unified framework effectively overcomes
limitations of existing data-driven methods, such as poor shape awareness and
challenges in capturing complex nonlinear dynamics. Extensive evaluations in
both simulation and real-world environments demonstrate the framework's robust
performance in shape estimation, trajectory tracking, and obstacle avoidance.
The proposed method consistently outperforms state-of-the-art end-to-end,
Neural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of
tracking accuracy and generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hand-Object Contact Detection using Grasp Quality Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh Vinh Nguyen, Akansel Cosgun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel hand-object contact detection system based on grasp
quality metrics extracted from object and hand poses, and evaluated its
performance using the DexYCB dataset. Our evaluation demonstrated the system's
high accuracy (approaching 90%). Future work will focus on a real-time
implementation using vision-based estimation, and integrating it to a
robot-to-human handover system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2025 IEEE/ACM International Conference on
  Human-Robot Interaction (HRI'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEVO: Memory-Efficient Monocular Visual Odometry Using Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dasong Gao, Peter Zhi Xuan Li, Vivienne Sze, Sertac Karaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing a high-fidelity representation of the 3D scene using a monocular
camera can enable a wide range of applications on mobile devices, such as
micro-robots, smartphones, and AR/VR headsets. On these devices, memory is
often limited in capacity and its access often dominates the consumption of
compute energy. Although Gaussian Splatting (GS) allows for high-fidelity
reconstruction of 3D scenes, current GS-based SLAM is not memory efficient as a
large number of past images is stored to retrain Gaussians for reducing
catastrophic forgetting. These images often require two-orders-of-magnitude
higher memory than the map itself and thus dominate the total memory usage. In
this work, we present GEVO, a GS-based monocular SLAM framework that achieves
comparable fidelity as prior methods by rendering (instead of storing) them
from the existing map. Novel Gaussian initialization and optimization
techniques are proposed to remove artifacts from the map and delay the
degradation of the rendered images over time. Across a variety of environments,
GEVO achieves comparable map fidelity while reducing the memory overhead to
around 58 MBs, which is up to 94x lower than prior works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon
  Visuomotor Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09905v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09905v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Break Yang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a low-cost legged mobile manipulation system that solves
long-horizon real-world tasks, trained by reinforcement learning purely in
simulation. This system is made possible by 1) a hierarchical design of a
high-level policy for visual-mobile manipulation following task instructions,
and a low-level quadruped locomotion policy, 2) a teacher and student training
pipeline for the high level, which trains a teacher to tackle long-horizon
tasks using privileged task decomposition and target object information, and
further trains a student for visual-mobile manipulation via RL guided by the
teacher's behavior, and 3) a suite of techniques for minimizing the sim-to-real
gap.
  In contrast to many previous works that use high-end equipments, our system
demonstrates effective performance with more accessible hardware --
specifically, a Unitree Go1 quadruped, a WidowX-250S arm, and a single
wrist-mounted RGB camera -- despite the increased challenges of sim-to-real
transfer. Trained fully in simulation, a single policy autonomously solves
long-horizon tasks involving search, move to, grasp, transport, and drop into,
achieving nearly 80% real-world success. This performance is comparable to that
of expert human teleoperation on the same tasks while the robot is more
efficient, operating at about 1.5x the speed of the teleoperation. Finally, we
perform extensive ablations on key techniques for efficient RL training and
effective sim-to-real transfer, and demonstrate effective deployment across
diverse indoor and outdoor scenes under various lighting conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Gaited Legged Locomotion with Monte-Carlo Tree Search and Supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyass Taouil, Lorenzo Amatucci, Majid Khadiv, Angela Dai, Victor Barasuol, Giulio Turrisi, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legged robots are able to navigate complex terrains by continuously
interacting with the environment through careful selection of contact sequences
and timings. However, the combinatorial nature behind contact planning hinders
the applicability of such optimization problems on hardware. In this work, we
present a novel approach that optimizes gait sequences and respective timings
for legged robots in the context of optimization-based controllers through the
use of sampling-based methods and supervised learning techniques. We propose to
bootstrap the search by learning an optimal value function in order to speed-up
the gait planning procedure making it applicable in real-time. To validate our
proposed method, we showcase its performance both in simulation and on hardware
using a 22 kg electric quadruped robot. The method is assessed on different
terrains, under external perturbations, and in comparison to a standard control
approach where the gait sequence is fixed a priori.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-28T00:00:00Z">2025-01-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">33</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile Manipulation Instruction Generation from Multiple Images with
  Automatic Metric Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kei Katsumata, Motonari Kambara, Daichi Yashima, Ryosuke Korekata, Komei Sugiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of generating free-form mobile manipulation
instructions based on a target object image and receptacle image. Conventional
image captioning models are not able to generate appropriate instructions
because their architectures are typically optimized for single-image. In this
study, we propose a model that handles both the target object and receptacle to
generate free-form instruction sentences for mobile manipulation tasks.
Moreover, we introduce a novel training method that effectively incorporates
the scores from both learning-based and n-gram based automatic evaluation
metrics as rewards. This method enables the model to learn the co-occurrence
relationships between words and appropriate paraphrases. Results demonstrate
that our proposed method outperforms baseline methods including representative
multimodal large language models on standard automatic evaluation metrics.
Moreover, physical experiments reveal that using our method to augment data on
language instructions improves the performance of an existing multimodal
language understanding model for mobile manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for IEEE RA-L 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Six-Degree-of-Freedom Motion Emulation for Data-Driven Modeling of
  Underwater Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juliana Danesi Ruiz, Michael Swafford, Austin Krebill, Rachel Vitali, Casey Harwood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a collaborative research effort aimed at developing a
novel six-degree-of-freedom (6-DOF) motion platform for the empirical
characterization of hydrodynamic forces crucial for the control and stability
of surface and subsurface vehicles. Traditional experimental methods, such as
the Planar Motion Mechanism (PMM), are limited by the number of simultaneously
articulated DOFs and are limited to single-frequency testing, making such
systems impractical for resolving frequency-dependent added mass or damping
matrices. The 6 DOF platform, termed a hexapod, overcomes these limitations by
offering enhanced maneuverability and the ability to test broad-banded
frequency spectra in multiple degrees of freedom in a single experiment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisit Mixture Models for Multi-Agent Simulation: Experimental Study
  within a Unified Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longzhong Lin, Xuewu Lin, Kechun Xu, Haojian Lu, Lichao Huang, Rong Xiong, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation plays a crucial role in assessing autonomous driving systems,
where the generation of realistic multi-agent behaviors is a key aspect. In
multi-agent simulation, the primary challenges include behavioral multimodality
and closed-loop distributional shifts. In this study, we revisit mixture models
for generating multimodal agent behaviors, which can cover the mainstream
methods including continuous mixture models and GPT-like discrete models.
Furthermore, we introduce a closed-loop sample generation approach tailored for
mixture models to mitigate distributional shifts. Within the unified mixture
model~(UniMM) framework, we recognize critical configurations from both model
and data perspectives. We conduct a systematic examination of various model
configurations, including positive component matching, continuous regression,
prediction horizon, and the number of components. Moreover, our investigation
into the data configuration highlights the pivotal role of closed-loop samples
in achieving realistic simulations. To extend the benefits of closed-loop
samples across a broader range of mixture models, we further address the
shortcut learning and off-policy learning issues. Leveraging insights from our
exploration, the distinct variants proposed within the UniMM framework,
including discrete, anchor-free, and anchor-based models, all achieve
state-of-the-art performance on the WOSAC benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAUCell: An Adaptive Multi-Attention Framework for Video Frame
  Prediction <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyam Gupta, P. Agrawal, Priyam Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sequence modeling stands as the fundamental foundation for video
prediction systems and real-time forecasting operations as well as anomaly
detection applications. The achievement of accurate predictions through
efficient resource consumption remains an ongoing issue in contemporary
temporal sequence modeling. We introduce the Multi-Attention Unit (MAUCell)
which combines Generative Adversarial Networks (GANs) and spatio-temporal
attention mechanisms to improve video frame prediction capabilities. Our
approach implements three types of attention models to capture intricate motion
sequences. A dynamic combination of these attention outputs allows the model to
reach both advanced decision accuracy along with superior quality while
remaining computationally efficient. The integration of GAN elements makes
generated frames appear more true to life therefore the framework creates
output sequences which mimic real-world footage. The new design system
maintains equilibrium between temporal continuity and spatial accuracy to
deliver reliable video prediction. Through a comprehensive evaluation
methodology which merged the perceptual LPIPS measurement together with classic
tests MSE, MAE, SSIM and PSNR exhibited enhancing capabilities than
contemporary approaches based on direct benchmark tests of Moving MNIST, KTH
Action, and CASIA-B (Preprocessed) datasets. Our examination indicates that
MAUCell shows promise for operational time requirements. The research findings
demonstrate how GANs work best with attention mechanisms to create better
applications for predicting video sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IJCAI 2025 Conference for review.
  It contains: 11 pages, 4 figures, 7 tables, and 3 Algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Open-Source and Modular Space Systems with ATMOS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Roque, Sujet Phodapol, Elias Krantz, Jaeyoung Lim, Joris Verhagen, Frank Jiang, David Dorner, Roland Siegwart, Ivan Stenius, Gunnar Tibert, Huina Mao, Jana Tumova, Christer Fuglesang, Dimos V. Dimarogonas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the near future, autonomous space systems will compose a large number of
the spacecraft being deployed. Their tasks will involve autonomous rendezvous
and proximity operations with large structures, such as inspections or assembly
of orbiting space stations and maintenance and human-assistance tasks over
shared workspaces. To promote replicable and reliable scientific results for
autonomous control of spacecraft, we present the design of a space systems
laboratory based on open-source and modular software and hardware. The
simulation software provides a software-in-the-loop (SITL) architecture that
seamlessly transfers simulated results to the ATMOS platforms, developed for
testing of multi-agent autonomy schemes for microgravity. The manuscript
presents the KTH space systems laboratory facilities and the ATMOS platform as
open-source hardware and software contributions. Preliminary results showcase
SITL and real testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary release, to be submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Geo-localization for Robotics: Are Black-box Vision-Language
  Models there yet? <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Waheed, Bruno Ferrarini, Michael Milford, Sarvapali D. Ramchurn, Shoaib Ehsan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advances in Vision-Language models (VLMs) offer exciting opportunities
for robotic applications involving image geo-localization, the problem of
identifying the geo-coordinates of a place based on visual data only. Recent
research works have focused on using a VLM as embeddings extractor for
geo-localization, however, the most sophisticated VLMs may only be available as
black boxes that are accessible through an API, and come with a number of
limitations: there is no access to training data, model features and gradients;
retraining is not possible; the number of predictions may be limited by the
API; training on model outputs is often prohibited; and queries are open-ended.
The utilization of a VLM as a stand-alone, zero-shot geo-localization system
using a single text-based prompt is largely unexplored. To bridge this gap,
this paper undertakes the first systematic study, to the best of our knowledge,
to investigate the potential of some of the state-of-the-art VLMs as
stand-alone, zero-shot geo-localization systems in a black-box setting with
realistic constraints. We consider three main scenarios for this thorough
investigation: a) fixed text-based prompt; b) semantically-equivalent
text-based prompts; and c) semantically-equivalent query images. We also take
into account the auto-regressive and probabilistic generation process of the
VLMs when investigating their utility for geo-localization task by using model
consistency as a metric in addition to traditional accuracy. Our work provides
new insights in the capabilities of different VLMs for the above-mentioned
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Giving Sense to Inputs: Toward an Accessible Control Framework for
  Shared Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shalutha Rajapakshe, Jean-Marc Odobez, Emmanuel Senft
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While shared autonomy offers significant potential for assistive robotics,
key questions remain about how to effectively map 2D control inputs to 6D robot
motions. An intuitive framework should allow users to input commands
effortlessly, with the robot responding as expected, without users needing to
anticipate the impact of their inputs. In this article, we propose a dynamic
input mapping framework that links joystick movements to motions on control
frames defined along a trajectory encoded with canal surfaces. We evaluate our
method in a user study with 20 participants, demonstrating that our input
mapping framework reduces the workload and improves usability compared to a
baseline mapping with similar motion encoding. To prepare for deployment in
assistive scenarios, we built on the development from the accessible gaming
community to select an accessible control interface. We then tested the system
in an exploratory study, where three wheelchair users controlled the robot for
both daily living activities and a creative painting task, demonstrating its
feasibility for users closer to our target population.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with
  Enhanced Contextual Awareness in Specific Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shady Nasrat, Myungsu Kim, Seonil Lee, Jiho Lee, Yeoncheol Jang, Seung-joon Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) represent a significant advancement in
integrating physical robots with AI-driven systems. We showcase the
capabilities of our framework within the context of the real-world household
competition. This research introduces a framework that utilizes RDMM (Robotics
Decision-Making Models), which possess the capacity for decision-making within
domain-specific contexts, as well as an awareness of their personal knowledge
and capabilities. The framework leverages information to enhance the autonomous
decision-making of the system. In contrast to other approaches, our focus is on
real-time, on-device solutions, successfully operating on hardware with as
little as 8GB of memory. Our framework incorporates visual perception models
equipping robots with understanding of their environment. Additionally, the
framework has integrated real-time speech recognition capabilities, thus
enhancing the human-robot interaction experience. Experimental results
demonstrate that the RDMM framework can plan with an 93\% accuracy.
Furthermore, we introduce a new dataset consisting of 27k planning instances,
as well as 1.3k text-image annotated samples derived from the competition. The
framework, benchmarks, datasets, and models developed in this work are publicly
available on our GitHub repository at https://github.com/shadynasrat/RDMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-Based Adaptive Koopman Framework for Optic Flow-Guided Landing on
  Moving Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bazeela Banday, Chandan Kumar Sah, Jishnu Keshavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an optic flow-guided approach for achieving soft landings
by resource-constrained unmanned aerial vehicles (UAVs) on dynamic platforms.
An offline data-driven linear model based on Koopman operator theory is
developed to describe the underlying (nonlinear) dynamics of optic flow output
obtained from a single monocular camera that maps to vehicle acceleration as
the control input. Moreover, a novel adaptation scheme within the Koopman
framework is introduced online to handle uncertainties such as unknown platform
motion and ground effect, which exert a significant influence during the
terminal stage of the descent process. Further, to minimize computational
overhead, an event-based adaptation trigger is incorporated into an
event-driven Model Predictive Control (MPC) strategy to regulate optic flow and
track a desired reference. A detailed convergence analysis ensures global
convergence of the tracking error to a uniform ultimate bound while ensuring
Zeno-free behavior. Simulation results demonstrate the algorithm's robustness
and effectiveness in landing on dynamic platforms under ground effect and
sensor noise, which compares favorably to non-adaptive event-triggered and
time-triggered adaptive schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RG-Attn: Radian Glue Attention for Multi-modality Multi-agent
  Cooperative Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lantao Li, Kang Yang, Wenqi Zhang, Xiaoxue Wang, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative perception offers an optimal solution to overcome the perception
limitations of single-agent systems by leveraging Vehicle-to-Everything (V2X)
communication for data sharing and fusion across multiple agents. However, most
existing approaches focus on single-modality data exchange, limiting the
potential of both homogeneous and heterogeneous fusion across agents. This
overlooks the opportunity to utilize multi-modality data per agent, restricting
the system's performance. In the automotive industry, manufacturers adopt
diverse sensor configurations, resulting in heterogeneous combinations of
sensor modalities across agents. To harness the potential of every possible
data source for optimal performance, we design a robust LiDAR and camera
cross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to
both intra-agent cross-modality fusion and inter-agent cross-modality fusion
scenarios, owing to the convenient coordinate conversion by transformation
matrix and the unified sampling/inversion mechanism. We also propose two
different architectures, named Paint-To-Puzzle (PTP) and
Co-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception. PTP
aims for maximum precision performance and achieves smaller data packet size by
limiting cross-agent fusion to a single instance, but requiring all
participants to be equipped with LiDAR. In contrast, CoS-CoCo supports agents
with any configuration-LiDAR-only, camera-only, or LiDAR-camera-both,
presenting more generalization ability. Our approach achieves state-of-the-art
(SOTA) performance on both real and simulated cooperative perception datasets.
The code will be released at GitHub in early 2025.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIRIGENt: End-To-End Robotic Imitation of Human Demonstrations Based on
  a Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josua Spisak, Matthias Kerzel, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been substantial progress in humanoid robots, with new skills
continuously being taught, ranging from navigation to manipulation. While these
abilities may seem impressive, the teaching methods often remain inefficient.
To enhance the process of teaching robots, we propose leveraging a mechanism
effectively used by humans: teaching by demonstrating. In this paper, we
introduce DIRIGENt (DIrect Robotic Imitation GENeration model), a novel
end-to-end diffusion approach that directly generates joint values from
observing human demonstrations, enabling a robot to imitate these actions
without any existing mapping between it and humans. We create a dataset in
which humans imitate a robot and then use this collected data to train a
diffusion model that enables a robot to imitate humans. The following three
aspects are the core of our contribution. First is our novel dataset with
natural pairs between human and robot poses, allowing our approach to imitate
humans accurately despite the gap between their anatomies. Second, the
diffusion input to our model alleviates the challenge of redundant joint
configurations, limiting the search space. And finally, our end-to-end
architecture from perception to action leads to an improved learning
capability. Through our experimental analysis, we show that combining these
three aspects allows DIRIGENt to outperform existing state-of-the-art
approaches in the field of generating joint values from RGB images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSF-PAN: Semantic Scene Flow-Based Perception for Autonomous Navigation
  in Traffic Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinqi Chen, Meiying Zhang, Qi Hao, Guang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle detection and localization in complex traffic scenarios pose
significant challenges due to the interference of moving objects. Traditional
methods often rely on outlier exclusions or semantic segmentations, which
suffer from low computational efficiency and accuracy. The proposed SSF-PAN can
achieve the functionalities of LiDAR point cloud based object
detection/localization and SLAM (Simultaneous Localization and Mapping) with
high computational efficiency and accuracy, enabling map-free navigation
frameworks. The novelty of this work is threefold: 1) developing a neural
network which can achieve segmentation among static and dynamic objects within
the scene flows with different motion features, that is, semantic scene flow
(SSF); 2) developing an iterative framework which can further optimize the
quality of input scene flows and output segmentation results; 3) developing a
scene flow-based navigation platform which can test the performance of the SSF
perception system in the simulation environment. The proposed SSF-PAN method is
validated using the SUScape-CARLA and the KITTI datasets, as well as on the
CARLA simulator. Experimental results demonstrate that the proposed approach
outperforms traditional methods in terms of scene flow computation accuracy,
moving object detection accuracy, computational efficiency, and autonomous
navigation effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Trajectory (Re)Planning for a Large Scale Swarm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lishuo Pan, Yutong Wang, Nora Ayanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the trajectory replanning problem for a large-scale swarm in a
cluttered environment. Our path planner replans for robots by utilizing a
hierarchical approach, dividing the workspace, and computing collision-free
paths for robots within each cell in parallel. Distributed trajectory
optimization generates a deadlock-free trajectory for efficient execution and
maintains the control feasibility even when the optimization fails. Our
hierarchical approach combines the benefits of both centralized and
decentralized methods, achieving a high task success rate while providing
real-time replanning capability. Compared to decentralized approaches, our
approach effectively avoids deadlocks and collisions, significantly increasing
the task success rate. We demonstrate the real-time performance of our
algorithm with up to 142 robots in simulation, and a representative 24 physical
Crazyflie nano-quadrotor experiment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures. arXiv admin note: substantial text overlap with
  arXiv:2407.02777</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dream to Drive with Predictive Individual World Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinfeng Gao, Qichao Zhang, Da-wei Ding, Dongbin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is still a challenging topic to make reactive driving behaviors in complex
urban environments as road users' intentions are unknown. Model-based
reinforcement learning (MBRL) offers great potential to learn a reactive policy
by constructing a world model that can provide informative states and
imagination training. However, a critical limitation in relevant research lies
in the scene-level reconstruction representation learning, which may overlook
key interactive vehicles and hardly model the interactive features among
vehicles and their long-term intentions. Therefore, this paper presents a novel
MBRL method with a predictive individual world model (PIWM) for autonomous
driving. PIWM describes the driving environment from an individual-level
perspective and captures vehicles' interactive relations and their intentions
via trajectory prediction task. Meanwhile, a behavior policy is learned jointly
with PIWM. It is trained in PIWM's imagination and effectively navigates in the
urban driving scenes leveraging intention-aware latent states. The proposed
method is trained and evaluated on simulation environments built upon
real-world challenging interactive scenarios. Compared with popular model-free
and state-of-the-art model-based reinforcement learning methods, experimental
results show that the proposed method achieves the best performance in terms of
safety and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes: https://github.com/gaoyinfeng/PIWM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Efficiency of Mixed Traffic through Reinforcement Learning: A
  Topology-Independent Approach and Benchmark <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyang Xiao, Dawei Wang, Xinzheng Tang, Jia Pan, Yuexin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a mixed traffic control policy designed to optimize
traffic efficiency across diverse road topologies, addressing issues of
congestion prevalent in urban environments. A model-free reinforcement learning
(RL) approach is developed to manage large-scale traffic flow, using data
collected by autonomous vehicles to influence human-driven vehicles. A
real-world mixed traffic control benchmark is also released, which includes 444
scenarios from 20 countries, representing a wide geographic distribution and
covering a variety of scenarios and road topologies. This benchmark serves as a
foundation for future research, providing a realistic simulation environment
for the development of effective policies. Comprehensive experiments
demonstrate the effectiveness and adaptability of the proposed method,
achieving better performance than existing traffic control methods in both
intersection and roundabout scenarios. To the best of our knowledge, this is
the first project to introduce a real-world complex scenarios mixed traffic
control benchmark. Videos and code of our work are available at
https://sites.google.com/berkeley.edu/mixedtrafficplus/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety-Critical Control for Aerial Physical Interaction in Uncertain
  Environment <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonghyun Byun, Yeonjoon Kim, Dongjae Lee, H. Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial manipulation for safe physical interaction with their environments is
gaining significant momentum in robotics research. In this paper, we present a
disturbance-observer-based safety-critical control for a fully actuated aerial
manipulator interacting with both static and dynamic structures. Our approach
centers on a safety filter that dynamically adjusts the desired trajectory of
the vehicle's pose, accounting for the aerial manipulator's dynamics, the
disturbance observer's structure, and motor thrust limits. We provide rigorous
proof that the proposed safety filter ensures the forward invariance of the
safety set - representing motor thrust limits - even in the presence of
disturbance estimation errors. To demonstrate the superiority of our method
over existing control strategies for aerial physical interaction, we perform
comparative experiments involving complex tasks, such as pushing against a
static structure and pulling a plug firmly attached to an electric socket.
Furthermore, to highlight its repeatability in scenarios with sudden dynamic
changes, we perform repeated tests of pushing a movable cart and extracting a
plug from a socket. These experiments confirm that our method not only
outperforms existing methods but also excels in handling tasks with rapid
dynamic variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be presented in 2025 IEEE International Conference on Robotics and
  Automation (ICRA), Atlanta, USA, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strawberry Robotic Operation Interface: An Open-Source Device for
  Collecting Dexterous Manipulation Data in Robotic Strawberry Farming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linsheng Hou, Wenwu Lu, Yanan Wang, Chen Peng, Zhenghao Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The strawberry farming is labor-intensive, particularly in tasks requiring
dexterous manipulation such as picking occluded strawberries. To address this
challenge, we present the Strawberry Robotic Operation Interface (SROI), an
open-source device designed for collecting dexterous manipulation data in
robotic strawberry farming. The SROI features a handheld unit with a modular
end effector, a stereo robotic camera, enabling the easy collection of
demonstration data in field environments. A data post-processing pipeline is
introduced to extract spatial trajectories and gripper states from the
collected data. Additionally, we release an open-source dataset of strawberry
picking demonstrations to facilitate research in dexterous robotic
manipulation. The SROI represents a step toward automating complex strawberry
farming tasks, reducing reliance on manual labor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose
  Diffusion via Rectified Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueen Ma, Yuzheng Zhuang, Jianye Hao, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D vision and spatial reasoning have long been recognized as preferable for
accurately perceiving our three-dimensional world, especially when compared
with traditional visual reasoning based on 2D images. Due to the difficulties
in collecting high-quality 3D data, research in this area has only recently
gained momentum. With the advent of powerful large language models (LLMs),
multi-modal LLMs for 3D vision have been developed over the past few years.
However, most of these models focus primarily on the vision encoder for 3D
data. In this paper, we propose converting existing densely activated LLMs into
mixture-of-experts (MoE) models, which have proven effective for multi-modal
data processing. In addition to leveraging these models' instruction-following
capabilities, we further enable embodied task planning by attaching a diffusion
head, Pose-DiT, that employs a novel rectified flow diffusion scheduler.
Experimental results on 3D question answering and task-planning tasks
demonstrate that our 3D-MoE framework achieves improved performance with fewer
activated parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Vision-Language-Action Model with Online Reinforcement
  Learning <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, Jianyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have successfully integrated large vision-language models
(VLMs) into low-level robotic control by supervised fine-tuning (SFT) with
expert robotic datasets, resulting in what we term vision-language-action (VLA)
models. Although the VLA models are powerful, how to improve these large models
during interaction with environments remains an open question. In this paper,
we explore how to further improve these VLA models via Reinforcement Learning
(RL), a commonly used fine-tuning technique for large models. However, we find
that directly applying online RL to large VLA models presents significant
challenges, including training instability that severely impacts the
performance of large models, and computing burdens that exceed the capabilities
of most local machines. To address these challenges, we propose iRe-VLA
framework, which iterates between Reinforcement Learning and Supervised
Learning to effectively improve VLA models, leveraging the exploratory benefits
of RL while maintaining the stability of supervised learning. Experiments in
two simulated benchmarks and a real-world manipulation suite validate the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Model Predictive Control and Reinforcement Learning Based
  Control for Legged Robot Locomotion in MuJoCo Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivayogi Akki, Tan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Predictive Control (MPC) and Reinforcement Learning (RL) are two
prominent strategies for controlling legged robots, each with unique strengths.
RL learns control policies through system interaction, adapting to various
scenarios, whereas MPC relies on a predefined mathematical model to solve
optimization problems in real-time. Despite their widespread use, there is a
lack of direct comparative analysis under standardized conditions. This work
addresses this gap by benchmarking MPC and RL controllers on a Unitree Go1
quadruped robot within the MuJoCo simulation environment, focusing on a
standardized task-straight walking at a constant velocity. Performance is
evaluated based on disturbance rejection, energy efficiency, and terrain
adaptability. The results show that RL excels in handling disturbances and
maintaining energy efficiency but struggles with generalization to new terrains
due to its dependence on learned policies tailored to specific environments. In
contrast, MPC shows enhanced recovery capabilities from larger perturbations by
leveraging its optimization-based approach, allowing for a balanced
distribution of control efforts across the robot's joints. The results provide
a clear understanding of the advantages and limitations of both RL and MPC,
offering insights into selecting an appropriate control strategy for legged
robotic applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Numerical Function Optimization Framework for Constrained
  Nonlinear Robotic Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sait Sovukluk, Christian Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a numerical function optimization framework designed for
constrained optimization problems in robotics. The tool is designed with
real-time considerations and is suitable for online trajectory and control
input optimization problems. The proposed framework does not require any
analytical representation of the problem and works with constrained block-box
optimization functions. The method combines first-order gradient-based line
search algorithms with constraint prioritization through nullspace projections
onto constraint Jacobian space. The tool is implemented in C++ and provided
online for community use, along with some numerical and robotic example
implementations presented in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to IFAC for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surena-V: A Humanoid Robot for Human-Robot Collaboration with
  Optimization-based Control Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ali Bazrafshani, Aghil Yousefi-Koma, Amin Amani, Behnam Maleki, Shahab Batmani, Arezoo Dehestani Ardakani, Sajedeh Taheri, Parsa Yazdankhah, Mahdi Nozari, Amin Mozayyan, Alireza Naeini, Milad Shafiee, Amirhosein Vedadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Surena-V, a humanoid robot designed to enhance
human-robot collaboration capabilities. The robot features a range of sensors,
including barometric tactile sensors in its hands, to facilitate precise
environmental interaction. This is demonstrated through an experiment
showcasing the robot's ability to control a medical needle's movement through
soft material. Surena-V's operational framework emphasizes stability and
collaboration, employing various optimization-based control strategies such as
Zero Moment Point (ZMP) modification through upper body movement and stepping.
Notably, the robot's interaction with the environment is improved by detecting
and interpreting external forces at their point of effect, allowing for more
agile responses compared to methods that control overall balance based on
external forces. The efficacy of this architecture is substantiated through an
experiment illustrating the robot's collaboration with a human in moving a bar.
This work contributes to the field of humanoid robotics by presenting a
comprehensive system design and control architecture focused on human-robot
collaboration and environmental adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on
  Scaled Platforms <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Ghignone, Nicolas Baumann, Cheng Hu, Jonathan Wang, Lei Xie, Andrea Carron, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous racing presents a complex environment requiring robust controllers
capable of making rapid decisions under dynamic conditions. While traditional
controllers based on tire models are reliable, they often demand extensive
tuning or system identification. RL methods offer significant potential due to
their ability to learn directly from interaction, yet they typically suffer
from the Sim-to-Reall gap, where policies trained in simulation fail to perform
effectively in the real world. In this paper, we propose RLPP, a residual RL
framework that enhances a PP controller with an RL-based residual. This hybrid
approach leverages the reliability and interpretability of PP while using RL to
fine-tune the controller's performance in real-world scenarios. Extensive
testing on the F1TENTH platform demonstrates that RLPP improves lap times by up
to 6.37 %, closing the gap to the SotA methods by more than 52 % and providing
reliable performance in zero-shot real-world deployment, overcoming key
challenges associated with the Sim-to-Real transfer and reducing the
performance gap from simulation to reality by more than 8-fold when compared to
the baseline RL controller. The RLPP framework is made available as an
open-source tool, encouraging further exploration and advancement in autonomous
racing research. The code is available at: www.github.com/forzaeth/rlpp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the IEEE
  International Conference on Robotics and Automation (ICRA), Atlanta 2025. The
  code is available at: www.github.com/forzaeth/rlpp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Reinforcement Learning and AI Agents for Adaptive Robotic
  Interaction and Assistance in Dementia Care 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores a novel approach to advancing dementia care by
integrating socially assistive robotics, reinforcement learning (RL), large
language models (LLMs), and clinical domain expertise within a simulated
environment. This integration addresses the critical challenge of limited
experimental data in socially assistive robotics for dementia care, providing a
dynamic simulation environment that realistically models interactions between
persons living with dementia (PLWDs) and robotic caregivers. The proposed
framework introduces a probabilistic model to represent the cognitive and
emotional states of PLWDs, combined with an LLM-based behavior simulation to
emulate their responses. We further develop and train an adaptive RL system
enabling humanoid robots, such as Pepper, to deliver context-aware and
personalized interactions and assistance based on PLWDs' cognitive and
emotional states. The framework also generalizes to computer-based agents,
highlighting its versatility. Results demonstrate that the RL system, enhanced
by LLMs, effectively interprets and responds to the complex needs of PLWDs,
providing tailored caregiving strategies. This research contributes to
human-computer and human-robot interaction by offering a customizable AI-driven
caregiving platform, advancing understanding of dementia-related challenges,
and fostering collaborative innovation in assistive technologies. The proposed
approach has the potential to enhance the independence and quality of life for
PLWDs while alleviating caregiver burden, underscoring the transformative role
of interaction-focused AI systems in dementia care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decictor: Towards Evaluating the Robustness of Decision-Making in
  Autonomous Driving Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingfei Cheng, Yuan Zhou, Xiaofei Xie, Junjie Wang, Guozhu Meng, Kairui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Driving System (ADS) testing is crucial in ADS development, with
the current primary focus being on safety. However, the evaluation of
non-safety-critical performance, particularly the ADS's ability to make optimal
decisions and produce optimal paths for autonomous vehicles (AVs), is also
vital to ensure the intelligence and reduce risks of AVs. Currently, there is
little work dedicated to assessing the robustness of ADSs' path-planning
decisions (PPDs), i.e., whether an ADS can maintain the optimal PPD after an
insignificant change in the environment. The key challenges include the lack of
clear oracles for assessing PPD optimality and the difficulty in searching for
scenarios that lead to non-optimal PPDs. To fill this gap, in this paper, we
focus on evaluating the robustness of ADSs' PPDs and propose the first method,
Decictor, for generating non-optimal decision scenarios (NoDSs), where the ADS
does not plan optimal paths for AVs. Decictor comprises three main components:
Non-invasive Mutation, Consistency Check, and Feedback. To overcome the oracle
challenge, Non-invasive Mutation is devised to implement conservative
modifications, ensuring the preservation of the original optimal path in the
mutated scenarios. Subsequently, the Consistency Check is applied to determine
the presence of non-optimal PPDs by comparing the driving paths in the original
and mutated scenarios. To deal with the challenge of large environment space,
we design Feedback metrics that integrate spatial and temporal dimensions of
the AV's movement. These metrics are crucial for effectively steering the
generation of NoDSs. We evaluate Decictor on Baidu Apollo, an open-source and
production-grade ADS. The experimental results validate the effectiveness of
Decictor in detecting non-optimal PPDs of ADSs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PokeFlex: A Real-World <span class="highlight-title">Dataset</span> of Volumetric Deformable Objects for
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Obrist, Miguel Zamora, Hehui Zheng, Ronan Hinchet, Firat Ozdemir, Juan Zarate, Robert K. Katzschmann, Stelian Coros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven methods have shown great potential in solving challenging
manipulation tasks; however, their application in the domain of deformable
objects has been constrained, in part, by the lack of data. To address this
lack, we propose PokeFlex, a dataset featuring real-world multimodal data that
is paired and annotated. The modalities include 3D textured meshes, point
clouds, RGB images, and depth maps. Such data can be leveraged for several
downstream tasks, such as online 3D mesh reconstruction, and it can potentially
enable underexplored applications such as the real-world deployment of
traditional control methods based on mesh simulations. To deal with the
challenges posed by real-world 3D mesh reconstruction, we leverage a
professional volumetric capture system that allows complete 360{\deg}
reconstruction. PokeFlex consists of 18 deformable objects with varying
stiffness and shapes. Deformations are generated by dropping objects onto a
flat surface or by poking the objects with a robot arm. Interaction wrenches
and contact locations are also reported for the latter case. Using different
data modalities, we demonstrated a use case for our dataset training models
that, given the novelty of the multimodal nature of Pokeflex, constitute the
state-of-the-art in multi-object online template-based mesh reconstruction from
multimodal data, to the best of our knowledge. We refer the reader to our
website ( https://pokeflex-dataset.github.io/ ) for further demos and examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Tool
  Segmentation in Robot-Assisted Cardiovascular Catheterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot-assisted catheterization has garnered a good attention for its
potentials in treating cardiovascular diseases. However, advancing
surgeon-robot collaboration still requires further research, particularly on
task-specific automation. For instance, automated tool segmentation can assist
surgeons in visualizing and tracking of endovascular tools during cardiac
procedures. While learning-based models have demonstrated state-of-the-art
segmentation performances, generating ground-truth labels for fully-supervised
methods is both labor-intensive time consuming, and costly. In this study, we
propose a weakly-supervised learning method with multi-lateral pseudo labeling
for tool segmentation in cardiovascular angiogram datasets. The method utilizes
a modified U-Net architecture featuring one encoder and multiple laterally
branched decoders. The decoders generate diverse pseudo labels under different
perturbations, augmenting available partial labels. The pseudo labels are
self-generated using a mixed loss function with shared consistency across the
decoders. The weakly-supervised model was trained end-to-end and validated
using partially annotated angiogram data from three cardiovascular
catheterization procedures. Validation results show that the model could
perform closer to fully-supervised models. Also, the proposed weakly-supervised
multi-lateral method outperforms three well known methods used for
weakly-supervised learning, offering the highest segmentation performance
across the three angiogram datasets. Furthermore, numerous ablation studies
confirmed the model's consistent performance under different parameters.
Finally, the model was applied for tool segmentation in a robot-assisted
catheterization experiments. The model enhanced visualization with high
connectivity indices for guidewire and catheter, and a mean processing time of
35 ms per frame.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpatialVLA: Exploring Spatial Representations for Visual-Language-Action
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15830v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15830v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we claim that spatial understanding is the keypoint in robot
manipulation, and propose SpatialVLA to explore effective spatial
representations for the robot foundation model. Specifically, we introduce
Ego3D Position Encoding to inject 3D information into the input observations of
the visual-language-action model, and propose Adaptive Action Grids to
represent spatial robot movement actions with adaptive discretized action
grids, facilitating learning generalizable and transferrable spatial action
knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a
vision-language model with 1.1 Million real-world robot episodes, to learn a
generalist manipulation policy across multiple robot environments and tasks.
After pre-training, SpatialVLA is directly applied to perform numerous tasks in
a zero-shot manner. The superior results in both simulation and real-world
robots demonstrate its advantage of inferring complex robot motion trajectories
and its strong in-domain multi-task generalization ability. We further show the
proposed Adaptive Action Grids offer a new and effective way to fine-tune the
pre-trained SpatialVLA model for new simulation and real-world setups, where
the pre-learned action grids are re-discretized to capture robot-specific
spatial action movements of new setups. The superior results from extensive
evaluations demonstrate the exceptional in-distribution generalization and
out-of-distribution adaptation capability, highlighting the crucial benefit of
the proposed spatial-aware representations for generalist robot policy
learning. All the details and codes will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Front Hair Styling Robot System Using Path Planning for Root-Centric
  Strand Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soonhyo Kim, Naoaki Kanazawa, Shun Hasegawa, Kento Kawaharazuka, Kei Okada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hair styling is a crucial aspect of personal grooming, significantly
influenced by the appearance of front hair. While brushing is commonly used
both to detangle hair and for styling purposes, existing research primarily
focuses on robotic systems for detangling hair, with limited exploration into
robotic hair styling. This research presents a novel robotic system designed to
automatically adjust front hairstyles, with an emphasis on path planning for
root-centric strand adjustment. The system utilizes images to compare the
current hair state with the desired target state through an orientation map of
hair strands. By concentrating on the differences in hair orientation and
specifically targeting adjustments at the root of each strand, the system
performs detailed styling tasks. The path planning approach ensures effective
alignment of the hairstyle with the target, and a closed-loop mechanism refines
these adjustments to accurately evolve the hairstyle towards the desired
outcome. Experimental results demonstrate that the proposed system achieves a
high degree of similarity and consistency in front hair styling, showing
promising results for automated, precise hairstyle adjustments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/SICE SII2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collision Avoidance and Geofencing for Fixed-wing Aircraft with Control
  Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamas G. Molnar, Suresh K. Kannan, James Cunningham, Kyle Dunlap, Kerianne L. Hobbs, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety-critical failures often have fatal consequences in aerospace control.
Control systems on aircraft, therefore, must ensure the strict satisfaction of
safety constraints, preferably with formal guarantees of safe behavior. This
paper establishes the safety-critical control of fixed-wing aircraft in
collision avoidance and geofencing tasks. A control framework is developed
wherein a run-time assurance (RTA) system modulates the nominal flight
controller of the aircraft whenever necessary to prevent it from colliding with
other aircraft or crossing a boundary (geofence) in space. The RTA is
formulated as a safety filter using control barrier functions (CBFs) with
formal guarantees of safe behavior. CBFs are constructed and compared for a
nonlinear kinematic fixed-wing aircraft model. The proposed CBF-based
controllers showcase the capability of safely executing simultaneous collision
avoidance and geofencing, as demonstrated by simulations on the kinematic model
and a high-fidelity dynamical model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the IEEE Transactions on Control System Technology. 15
  pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mechanisms and Computational Design of Multi-Modal End-Effector with
  Force Sensing using Gated Networks <span class="chip">ICRA25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17524v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17524v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Tanaka, Alvin Zhu, Richard Lin, Ankur Mehta, Dennis Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In limbed robotics, end-effectors must serve dual functions, such as both
feet for locomotion and grippers for grasping, which presents design
challenges. This paper introduces a multi-modal end-effector capable of
transitioning between flat and line foot configurations while providing
grasping capabilities. MAGPIE integrates 8-axis force sensing using proposed
mechanisms with hall effect sensors, enabling both contact and tactile force
measurements. We present a computational design framework for our sensing
mechanism that accounts for noise and interference, allowing for desired
sensitivity and force ranges and generating ideal inverse models. The hardware
implementation of MAGPIE is validated through experiments, demonstrating its
capability as a foot and verifying the performance of the sensing mechanisms,
ideal models, and gated network-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceeding to 2025 IEEE International Conference on Robotics and
  Automation (ICRA25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe and Trustworthy Robot Pathfinding with BIM, MHA*, and NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mani Amani, Reza Akhavian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Construction robots have gained significant traction in recent years in
research and development. However, the application of industrial robots has
unique challenges. Dynamic environments, domain-specific tasks, and complex
localization and mapping are significant obstacles in their development. In
construction job sites, moving objects and complex machinery can make
pathfinding a difficult task due to the possibility of object collisions.
Existing methods such as simultaneous localization and mapping are viable
solutions to this problem, however, due to the precision and data quality
required by the sensors and the processing of the information, they can be very
computationally expensive. We propose using spatial and semantic information in
building information modeling (BIM) to develop domain-specific pathfinding
strategies. In this work, we integrate a multi-heuristic A* (MHA*) algorithm
using APFs from the BIM spatial information and process textual information
from the BIM using large language models (LLMs) to adjust the algorithm for
dynamic object avoidance. We show increased robot object proximity by 80% while
maintaining similar path lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Competency-Aware Planning for Probabilistically Safe Navigation Under
  Perception Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06111v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06111v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Pohland, Claire Tomlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception-based navigation systems are useful for unmanned ground vehicle
(UGV) navigation in complex terrains, where traditional depth-based navigation
schemes are insufficient. However, these data-driven methods are highly
dependent on their training data and can fail in surprising and dramatic ways
with little warning. To ensure the safety of the vehicle and the surrounding
environment, it is imperative that the navigation system is able to recognize
the predictive uncertainty of the perception model and respond safely and
effectively in the face of uncertainty. In an effort to enable safe navigation
under perception uncertainty, we develop a probabilistic and
reconstruction-based competency estimation (PaRCE) method to estimate the
model's level of familiarity with an input image as a whole and with specific
regions in the image. We find that the overall competency score can correctly
predict correctly classified, misclassified, and out-of-distribution (OOD)
samples. We also confirm that the regional competency maps can accurately
distinguish between familiar and unfamiliar regions across images. We then use
this competency information to develop a planning and control scheme that
enables effective navigation while maintaining a low probability of error. We
find that the competency-aware scheme greatly reduces the number of collisions
with unfamiliar obstacles, compared to a baseline controller with no competency
awareness. Furthermore, the regional competency information is very valuable in
enabling efficient navigation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-27T00:00:00Z">2025-01-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An FPGA-Based Neuro-Fuzzy Sensor for Personalized Driving Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Óscar Mata-Carballeira, Jon Gutiérrez-Zaballa, Inés del Campo, Victoria Martínez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced driving-assistance systems (ADAS) are intended to automatize driver
tasks, as well as improve driving and vehicle safety. This work proposes an
intelligent neuro-fuzzy sensor for driving style (DS) recognition, suitable for
ADAS enhancement. The development of the driving style intelligent sensor uses
naturalistic driving data from the SHRP2 study, which includes data from a CAN
bus, inertial measurement unit, and front radar. The system has been
successfully implemented using a field-programmable gate array (FPGA) device of
the Xilinx Zynq programmable system-on-chip (PSoC). It can mimic the typical
timing parameters of a group of drivers as well as tune these typical
parameters to model individual DSs. The neuro-fuzzy intelligent sensor provides
high-speed real-time active ADAS implementation and is able to personalize its
behavior into safe margins without driver intervention. In particular, the
personalization procedure of the time headway (THW) parameter for an ACC in
steady car following was developed, achieving a performance of 0.53
microseconds. This performance fulfilled the requirements of cutting-edge
active ADAS specifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal Article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Reconstruction of non-visible surfaces of objects from a Single Depth
  View -- Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafał Staszak, Piotr Michałek, Jakub Chudziński, Marek Kopicki, Dominik Belter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene and object reconstruction is an important problem in robotics, in
particular in planning collision-free trajectories or in object manipulation.
This paper compares two strategies for the reconstruction of nonvisible parts
of the object surface from a single RGB-D camera view. The first method, named
DeepSDF predicts the Signed Distance Transform to the object surface for a
given point in 3D space. The second method, named MirrorNet reconstructs the
occluded objects' parts by generating images from the other side of the
observed object. Experiments performed with objects from the ShapeNet dataset,
show that the view-dependent MirrorNet is faster and has smaller reconstruction
errors in most categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Underactuated dexterous robotic grasping with reconfigurable passive
  joints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Kopicki, Sainul Islam Ansary, Simone Tolomei, Franco Angelini, Manolo Garabini, Piotr Skrzypczyński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel reconfigurable passive joint (RP-joint), which has been
implemented and tested on an underactuated three-finger robotic gripper.
RP-joint has no actuation, but instead it is lightweight and compact. It can be
easily reconfigured by applying external forces and locked to perform complex
dexterous manipulation tasks, but only after tension is applied to the
connected tendon. Additionally, we present an approach that allows learning
dexterous grasps from single examples with underactuated grippers and
automatically configures the RP-joints for dexterous manipulation. This is
enhanced by integrating kinaesthetic contact optimization, which improves grasp
performance even further. The proposed RP-joint gripper and grasp planner have
been tested on over 370 grasps executed on 42 IKEA objects and on the YCB
object dataset, achieving grasping success rates of 80% and 87%, on IKEA and
YCB, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Taha Tariq, Congqing Wang, Yasir Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile robot path planning in complex environments remains a significant
challenge, especially in achieving efficient, safe and robust paths. The
traditional path planning techniques like DRL models typically trained for a
given configuration of the starting point and target positions, these models
only perform well when these conditions are satisfied. In this paper, we
proposed a novel path planning framework that embeds Large Language Models to
empower mobile robots with the capability of dynamically interpreting natural
language commands and autonomously generating efficient, collision-free
navigation paths. The proposed framework uses LLMs to translate high-level user
inputs into actionable waypoints while dynamically adjusting paths in response
to obstacles. We experimentally evaluated our proposed LLM-based approach
across three different environments of progressive complexity, showing the
robustness of our approach with llama3.1 model that outperformed other LLM
models in path planning time, waypoint generation success rate, and collision
avoidance. This underlines the promising contribution of LLMs for enhancing the
capability of mobile robots, especially when their operation involves complex
decisions in large and complex environments. Our framework has provided safer,
more reliable navigation systems and opened a new direction for the future
research. The source code of this work is publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, submitted in Journal Expert Systems with
  Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for
  Autonomous Driving with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuewen Mei, Tong Nie, Jian Sun, Ye Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring and improving the safety of autonomous driving systems (ADS) is
crucial for the deployment of highly automated vehicles, especially in
safety-critical events. To address the rarity issue, adversarial scenario
generation methods are developed, in which behaviors of traffic participants
are manipulated to induce safety-critical events. However, existing methods
still face two limitations. First, identification of the adversarial
participant directly impacts the effectiveness of the generation. However, the
complexity of real-world scenarios, with numerous participants and diverse
behaviors, makes identification challenging. Second, the potential of generated
safety-critical scenarios to continuously improve ADS performance remains
underexplored. To address these issues, we propose LLM-attacker: a closed-loop
adversarial scenario generation framework leveraging large language models
(LLMs). Specifically, multiple LLM agents are designed and coordinated to
identify optimal attackers. Then, the trajectories of the attackers are
optimized to generate adversarial scenarios. These scenarios are iteratively
refined based on the performance of ADS, forming a feedback loop to improve
ADS. Experimental results show that LLM-attacker can create more dangerous
scenarios than other methods, and the ADS trained with it achieves a collision
rate half that of training with normal scenarios. This indicates the ability of
LLM-attacker to test and enhance the safety and robustness of ADS. Video
demonstrations are provided at:
https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpatialVLA: Exploring Spatial Representations for Visual-Language-Action
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we claim that spatial understanding is the keypoint in robot
manipulation, and propose SpatialVLA to explore effective spatial
representations for the robot foundation model. Specifically, we introduce
Ego3D Position Encoding to inject 3D information into the input observations of
the visual-language-action model, and propose Adaptive Action Grids to
represent spatial robot movement actions with adaptive discretized action
grids, facilitating learning generalizable and transferrable spatial action
knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a
vision-language model with 1.1 Million real-world robot episodes, to learn a
generalist manipulation policy across multiple robot environments and tasks.
After pre-training, SpatialVLA is directly applied to perform numerous tasks in
a zero-shot manner. The superior results in both simulation and real-world
robots demonstrate its advantage of inferring complex robot motion trajectories
and its strong in-domain multi-task generalization ability. We further show the
proposed Adaptive Action Grids offer a new and effective way to fine-tune the
pre-trained SpatialVLA model for new simulation and real-world setups, where
the pre-learned action grids are re-discretized to capture robot-specific
spatial action movements of new setups. The superior results from extensive
evaluations demonstrate the exceptional in-distribution generalization and
out-of-distribution adaptation capability, highlighting the crucial benefit of
the proposed spatial-aware representations for generalist robot policy
learning. All the details and codes will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Horizon-based Asteroid Navigation With
  Observability-constrained Maneuvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Arjun Anibha, Kenshiro Oguri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asteroid exploration is a pertinent challenge due to the varying complexity
of their dynamical environments, shape and communication delays due to
distance. Thus, autonomous navigation methods are continually being developed
and improved in current research to enable their safe exploration. These
methods often involve using horizon-based Optical Navigation (OpNav) to
determine the spacecraft's location, which is reliant on the visibility of the
horizon. It is critical to ensure the reliability of this measurement such that
the spacecraft may maintain an accurate state estimate throughout its mission.
This paper presents an algorithm that generates control maneuvers for
spacecraft to follow trajectories that allow continuously usable optical
measurements to maintain system observability for safe navigation. This
algorithm improves upon existing asteroid navigation capabilities by allowing
the safe and robust autonomous targeting of various trajectories and orbits at
a wide range of distances within optical measurement range. It is adaptable to
different asteroid scenarios. Overall, the approach develops an
all-encompassing system that simulates the asteroid dynamics, synthetic image
generation, edge detection, horizon-based OpNav, filtering and
observability-enhancing control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 16 figures, preprint under journal review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error-State LQR Formulation for Quadrotor UAV Trajectory Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micah Reich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents an error-state Linear Quadratic Regulator (LQR)
formulation for robust trajectory tracking in quadrotor Unmanned Aerial
Vehicles (UAVs). The proposed approach leverages error-state dynamics and
employs exponential coordinates to represent orientation errors, enabling a
linearized system representation for real-time control. The control strategy
integrates an LQR-based full-state feedback controller for trajectory tracking,
combined with a cascaded bodyrate controller to handle actuator dynamics.
Detailed derivations of the error-state dynamics, the linearization process,
and the controller design are provided, highlighting the applicability of the
method for precise and stable quadrotor control in dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Mission Planning for Heterogeneous Multi-Robot Teams via
  LLM-constructed Hierarchical Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush Gupta, David Isele, Enna Sachdeva, Pin-Hao Huang, Behzad Dariush, Kwonjoon Lee, Sangjae Bae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel mission-planning strategy for heterogeneous multi-robot
teams, taking into account the specific constraints and capabilities of each
robot. Our approach employs hierarchical trees to systematically break down
complex missions into manageable sub-tasks. We develop specialized APIs and
tools, which are utilized by Large Language Models (LLMs) to efficiently
construct these hierarchical trees. Once the hierarchical tree is generated, it
is further decomposed to create optimized schedules for each robot, ensuring
adherence to their individual constraints and capabilities. We demonstrate the
effectiveness of our framework through detailed examples covering a wide range
of missions, showcasing its flexibility and scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Position Estimation in Tactile Internet-Enabled Remote Robotic
  Surgery Using MOESP-Based Kalman Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Hanif Lashari, Wafa Batayneh, Ashfaq Khokhar, Shakil Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the position of a patient's side robotic arm in real
time during remote surgery is a significant challenge, especially within
Tactile Internet (TI) environments. This paper presents a new and efficient
method for position estimation using a Kalman Filter (KF) combined with the
Multivariable Output-Error State Space (MOESP) method for system
identification. Unlike traditional approaches that require prior knowledge of
the system's dynamics, this study uses the JIGSAW dataset, a comprehensive
collection of robotic surgical data, along with input from the Master Tool
Manipulator (MTM) to derive the state-space model directly. The MOESP method
allows accurate modeling of the Patient Side Manipulator (PSM) dynamics without
prior system models, improving the KF's performance under simulated network
conditions, including delays, jitter, and packet loss. These conditions mimic
real-world challenges in Tactile Internet applications. The findings
demonstrate the KF's improved resilience and accuracy in state estimation,
achieving over 95 percent accuracy despite network-induced uncertainties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2406.04503</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular Framework for Uncertainty Prediction in Autonomous Vehicle
  Motion Forecasting within Complex Traffic Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Yuneil Yeo, Antonio R. Paiva, Jean Utke, Maria Laura Delle Monache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a modular modeling framework designed to enhance the capture and
validation of uncertainty in autonomous vehicle (AV) trajectory prediction.
Departing from traditional deterministic methods, our approach employs a
flexible, end-to-end differentiable probabilistic encoder-decoder architecture.
This modular design allows the encoder and decoder to be trained independently,
enabling seamless adaptation to diverse traffic scenarios without retraining
the entire system. Our key contributions include: (1) a probabilistic heatmap
predictor that generates context-aware occupancy grids for dynamic forecasting,
(2) a modular training approach that supports independent component training
and flexible adaptation, and (3) a structured validation scheme leveraging
uncertainty metrics to evaluate robustness under high-risk conditions. To
highlight the benefits of our framework, we benchmark it against an end-to-end
baseline, demonstrating faster convergence, improved stability, and
flexibility. Experimental results validate these advantages, showcasing the
capacity of the framework to efficiently handle complex scenarios while
ensuring reliable predictions and robust uncertainty representation. This
modular design offers significant practical utility and scalability for
real-world autonomous driving applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiFold: Bimanual Cloth Folding with Language Guidance <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriol Barbany, Adrià Colomé, Carme Torras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth folding is a complex task due to the inevitable self-occlusions of
clothes, their complicated dynamics, and the disparate materials, geometries,
and textures that garments can have. In this work, we learn folding actions
conditioned on text commands. Translating high-level, abstract instructions
into precise robotic actions requires sophisticated language understanding and
manipulation capabilities. To do that, we leverage a pre-trained
vision-language model and repurpose it to predict manipulation actions. Our
model, BiFold, can take context into account and achieves state-of-the-art
performance on an existing language-conditioned folding benchmark. Given the
lack of annotated bimanual folding data, we devise a procedure to automatically
parse actions of a simulated dataset and tag them with aligned text
instructions. BiFold attains the best performance on our dataset and can
transfer to new instructions, garments, and environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhysBench: Benchmarking and Enhancing Vision-Language Models for
  Physical World Understanding <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the physical world is a fundamental challenge in embodied AI,
critical for enabling agents to perform complex tasks and operate safely in
real-world environments. While Vision-Language Models (VLMs) have shown great
promise in reasoning and task planning for embodied agents, their ability to
comprehend physical phenomena remains extremely limited. To close this gap, we
introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'
physical world understanding capability across a diverse set of tasks.
PhysBench contains 100,000 entries of interleaved video-image-text data,
categorized into four major domains: physical object properties, physical
object relationships, physical scene understanding, and physics-based dynamics,
further divided into 19 subclasses and 8 distinct capability dimensions. Our
extensive experiments, conducted on 75 representative VLMs, reveal that while
these models excel in common-sense reasoning, they struggle with understanding
the physical world -- likely due to the absence of physical knowledge in their
training data and the lack of embedded physical priors. To tackle the
shortfall, we introduce PhysAgent, a novel framework that combines the
generalization strengths of VLMs with the specialized expertise of vision
models, significantly enhancing VLMs' physical understanding across a variety
of tasks, including an 18.4\% improvement on GPT-4o. Furthermore, our results
demonstrate that enhancing VLMs' physical world understanding capabilities can
help embodied agents such as MOKA. We believe that PhysBench and PhysAgent
offer valuable insights and contribute to bridging the gap between VLMs and
physical world understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://physbench.github.io/; Dataset:
  https://huggingface.co/datasets/USC-GVL/PhysBench;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile
  Manipulation Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05313v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05313v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Jaafar, Shreyas Sundara Raman, Yichen Wei, Sudarshan Harithas, Sofia Juliani, Anneke Wernerfelt, Benedict Quartey, Ifrah Idrees, Jason Xinyu Liu, Stefanie Tellex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently learning and executing long-horizon mobile manipulation (MoMa)
tasks is crucial for advancing robotics in household and workplace settings.
However, current MoMa models are data-inefficient, underscoring the need for
improved models that require realistic-sized benchmarks to evaluate their
efficiency, which do not exist. To address this, we introduce the LAMBDA
({\lambda}) benchmark (Long-horizon Actions for Mobile-manipulation
Benchmarking of Directed Activities), which evaluates the data efficiency of
models on language-conditioned, long-horizon, multi-room, multi-floor,
pick-and-place tasks using a dataset of manageable size, more feasible for
collection. The benchmark includes 571 human-collected demonstrations that
provide realism and diversity in simulated and real-world settings. Unlike
planner-generated data, these trajectories offer natural variability and
replay-verifiability, ensuring robust learning and evaluation. We benchmark
several models, including learning-based models and a neuro-symbolic modular
approach combining foundation models with task and motion planning.
Learning-based models show suboptimal success rates, even when leveraging
pretrained weights, underscoring significant data inefficiencies. However, the
neuro-symbolic approach performs significantly better while being more data
efficient. Findings highlight the need for more data-efficient learning-based
MoMa approaches. {\lambda} addresses this gap by serving as a key benchmark for
evaluating the data efficiency of those future models in handling household
robotics tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TeleP<span class="highlight-title">review</span>: A User-Friendly Teleoperation System with Virtual Arm
  Assistance for Enhanced Effectiveness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13548v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13548v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxiang Guo, Jiayu Luo, Zhenyu Wei, Yiwen Hou, Zhixuan Xu, Xiaoyi Lin, Chongkai Gao, Lin Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperation provides an effective way to collect robot data, which is
crucial for learning from demonstrations. In this field, teleoperation faces
several key challenges: user-friendliness for new users, safety assurance, and
transferability across different platforms. While collecting real robot
dexterous manipulation data by teleoperation to train robots has shown
impressive results on diverse tasks, due to the morphological differences
between human and robot hands, it is not only hard for new users to understand
the action mapping but also raises potential safety concerns during operation.
To address these limitations, we introduce TelePreview. This teleoperation
system offers real-time visual feedback on robot actions based on human user
inputs, with a total hardware cost of less than $1,000. TelePreview allows the
user to see a virtual robot that represents the outcome of the user's next
movement. By enabling flexible switching between command visualization and
actual execution, this system helps new users learn how to demonstrate quickly
and safely. We demonstrate that it outperforms other teleoperation systems
across five tasks, emphasize its ease of use, and highlight its straightforward
deployment across diverse robotic platforms. We release our code and a
deployment document on our website https://nus-lins-lab.github.io/telepreview/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual-Lidar Map Alignment for Infrastructure Inspections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake McLaughlin, Nicholas Charron, Sriram Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Routine and repetitive infrastructure inspections present safety, efficiency,
and consistency challenges as they are performed manually, often in challenging
or hazardous environments. They can also introduce subjectivity and errors into
the process, resulting in undesirable outcomes. Simultaneous localization and
mapping (SLAM) presents an opportunity to generate high-quality 3D maps that
can be used to extract accurate and objective inspection data. Yet, many SLAM
algorithms are limited in their ability to align 3D maps from repeated
inspections in GPS-denied settings automatically. This limitation hinders
practical long-term asset health assessments by requiring tedious manual
alignment for data association across scans from previous inspections. This
paper introduces a versatile map alignment algorithm leveraging both visual and
lidar data for improved place recognition robustness and presents an
infrastructure-focused dataset tailored for consecutive inspections. By
detaching map alignment from SLAM, our approach enhances infrastructure
inspection pipelines, supports monitoring asset degradation over time, and
invigorates SLAM research by permitting exploration beyond existing
multi-session SLAM algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, for associated code see
  https://github.com/jakemclaughlin6/vlma</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segmentation <span class="highlight-title">Dataset</span> for Reinforced Concrete Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Schmidt, Lazaros Nalpantidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a dataset of 14,805 RGB images with segmentation labels
for autonomous robotic inspection of reinforced concrete defects. Baselines for
the YOLOv8L-seg, DeepLabV3, and U-Net segmentation models are established.
Labelling inconsistencies are addressed statistically, and their influence on
model performance is analyzed. An error identification tool is employed to
examine the error modes of the models. The paper demonstrates that YOLOv8L-seg
performs best, achieving a validation mIOU score of up to 0.59. Label
inconsistencies were found to have a negligible effect on model performance,
while the inclusion of more data improved the performance. False negatives were
identified as the primary failure mode. The results highlight the importance of
data availability for the performance of deep learning-based models. The lack
of publicly available data is identified as a significant contributor to false
negatives. To address this, the paper advocates for an increased open-source
approach within the construction community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The ConRebSeg Dataset can be found under the following DOI:
  https://doi.org/10.11583/DTU.26213762 Corresponding code to download
  additional data and initialize the dataset under
  https://github.com/DTU-PAS/ConRebSeg This work is an accepted manuscript up
  for publication in the Elsevier journal "Automation in Construction"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SNN-Based Online Learning of Concepts and Action Laws in an Open World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christel Grimaud, Dominique Longin, Andreas Herzig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the architecture of a fully autonomous, bio-inspired cognitive
agent built around a spiking neural network (SNN) implementing the agent's
semantic memory. The agent explores its universe and learns concepts of
objects/situations and of its own actions in a one-shot manner. While
object/situation concepts are unary, action concepts are triples made up of an
initial situation, a motor activity, and an outcome. They embody the agent's
knowledge of its universe's actions laws. Both kinds of concepts have different
degrees of generality. To make decisions the agent queries its semantic memory
for the expected outcomes of envisaged actions and chooses the action to take
on the basis of these predictions. Our experiments show that the agent handles
new situations by appealing to previously learned general concepts and rapidly
modifies its concepts to adapt to environment changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collective Intelligence for 2D Push Manipulations with Mobile Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15136v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15136v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        So Kuroki, Tatsuya Matsushima, Jumpei Arima, Hiroki Furuta, Yutaka Matsuo, Shixiang Shane Gu, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While natural systems often present collective intelligence that allows them
to self-organize and adapt to changes, the equivalent is missing in most
artificial systems. We explore the possibility of such a system in the context
of cooperative 2D push manipulations using mobile robots. Although conventional
works demonstrate potential solutions for the problem in restricted settings,
they have computational and learning difficulties. More importantly, these
systems do not possess the ability to adapt when facing environmental changes.
In this work, we show that by distilling a planner derived from a
differentiable soft-body physics simulator into an attention-based neural
network, our multi-robot push manipulation system achieves better performance
than baselines. In addition, our system also generalizes to configurations not
seen during training and is able to adapt toward task completions when external
turbulence and environmental changes are applied. Supplementary videos can be
found on our project website: https://sites.google.com/view/ciom/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Robotics and Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenORM: Generalizable One-shot Rope Manipulation with Parameter-Aware
  Policy <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09872v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09872v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        So Kuroki, Jiaxian Guo, Tatsuya Matsushima, Takuya Okubo, Masato Kobayashi, Yuya Ikeda, Ryosuke Takanami, Paul Yoo, Yutaka Matsuo, Yusuke Iwasawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the inherent uncertainty in their deformability during motion,
previous methods in rope manipulation often require hundreds of real-world
demonstrations to train a manipulation policy for each rope, even for simple
tasks such as rope goal reaching, which hinder their applications in our
ever-changing world. To address this issue, we introduce GenORM, a framework
that allows the manipulation policy to handle different deformable ropes with a
single real-world demonstration. To achieve this, we augment the policy by
conditioning it on deformable rope parameters and training it with a diverse
range of simulated deformable ropes so that the policy can adjust actions based
on different rope parameters. At the time of inference, given a new rope,
GenORM estimates the deformable rope parameters by minimizing the disparity
between the grid density of point clouds of real-world demonstrations and
simulations. With the help of a differentiable physics simulator, we require
only a single real-world demonstration. Empirical validations on both simulated
and real-world rope manipulation setups clearly show that our method can
manipulate different ropes with a single demonstration and significantly
outperforms the baseline in both environments (62% improvement in in-domain
ropes, and 15% improvement in out-of-distribution ropes in simulation, 26%
improvement in real-world), demonstrating the effectiveness of our approach in
one-shot rope manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The extended version of this paper, GenDOM, was published in the 2024
  IEEE International Conference on Robotics and Automation (ICRA 2024),
  arXiv:2309.09051</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenDOM: Generalizable One-shot Deformable Object Manipulation with
  Parameter-Aware Policy <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09051v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09051v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        So Kuroki, Jiaxian Guo, Tatsuya Matsushima, Takuya Okubo, Masato Kobayashi, Yuya Ikeda, Ryosuke Takanami, Paul Yoo, Yutaka Matsuo, Yusuke Iwasawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the inherent uncertainty in their deformability during motion,
previous methods in deformable object manipulation, such as rope and cloth,
often required hundreds of real-world demonstrations to train a manipulation
policy for each object, which hinders their applications in our ever-changing
world. To address this issue, we introduce GenDOM, a framework that allows the
manipulation policy to handle different deformable objects with only a single
real-world demonstration. To achieve this, we augment the policy by
conditioning it on deformable object parameters and training it with a diverse
range of simulated deformable objects so that the policy can adjust actions
based on different object parameters. At the time of inference, given a new
object, GenDOM can estimate the deformable object parameters with only a single
real-world demonstration by minimizing the disparity between the grid density
of point clouds of real-world demonstrations and simulations in a
differentiable physics simulator. Empirical validations on both simulated and
real-world object manipulation setups clearly show that our method can
manipulate different objects with a single demonstration and significantly
outperforms the baseline in both environments (a 62% improvement for in-domain
ropes and a 15% improvement for out-of-distribution ropes in simulation, as
well as a 26% improvement for ropes and a 50% improvement for cloths in the
real world), demonstrating the effectiveness of our approach in one-shot
deformable object manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 2024 IEEE International Conference on Robotics and
  Automation (ICRA 2024). arXiv admin note: substantial text overlap with
  arXiv:2306.09872</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for
  Cooperative Push Manipulation by Mobile Robots <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02008v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02008v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        So Kuroki, Mai Nishimura, Tadashi Kozuno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the complex interactions between agents, learning multi-agent control
policy often requires a prohibited amount of data. This paper aims to enable
multi-agent systems to effectively utilize past memories to adapt to novel
collaborative tasks in a data-efficient fashion. We propose the Multi-Agent
Coordination Skill Database, a repository for storing a collection of
coordinated behaviors associated with key vectors distinctive to them. Our
Transformer-based skill encoder effectively captures spatio-temporal
interactions that contribute to coordination and provides a unique skill
representation for each coordinated behavior. By leveraging only a small number
of demonstrations of the target task, the database enables us to train the
policy using a dataset augmented with the retrieved demonstrations.
Experimental evaluations demonstrate that our method achieves a significantly
higher success rate in push manipulation tasks compared with baseline methods
like few-shot imitation learning. Furthermore, we validate the effectiveness of
our retrieve-and-learn framework in a real environment using a team of wheeled
robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 2024 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QuIP: Experimental design for expensive simulators with many Qualitative
  factors via Integer Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Chun Liu, Simon Mak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need to explore and/or optimize expensive simulators with many
qualitative factors arises in broad scientific and engineering problems. Our
motivating application lies in path planning - the exploration of feasible
paths for navigation, which plays an important role in robotics, surgical
planning and assembly planning. Here, the feasibility of a path is evaluated
via expensive virtual experiments, and its parameter space is typically
discrete and high-dimensional. A carefully selected experimental design is thus
essential for timely decision-making. We propose here a novel framework, called
QuIP, for experimental design of Qualitative factors via Integer Programming
under a Gaussian process surrogate model with an exchangeable covariance
function. For initial design, we show that its asymptotic D-optimal design can
be formulated as a variant of the well-known assignment problem in operations
research, which can be efficiently solved to global optimality using
state-of-the-art integer programming solvers. For sequential design
(specifically, for active learning or black-box optimization), we show that its
design criterion can similarly be formulated as an assignment problem, thus
enabling efficient and reliable optimization with existing solvers. We then
demonstrate the effectiveness of QuIP over existing methods in a suite of path
planning experiments and an application to rover trajectory optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GCBF+: A Neural Graph Control Barrier Function Framework for Distributed
  Safe Multi-Agent Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyuan Zhang, Oswin So, Kunal Garg, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed, scalable, and safe control of large-scale multi-agent systems is
a challenging problem. In this paper, we design a distributed framework for
safe multi-agent control in large-scale environments with obstacles, where a
large number of agents are required to maintain safety using only local
information and reach their goal locations. We introduce a new class of
certificates, termed graph control barrier function (GCBF), which are based on
the well-established control barrier function theory for safety guarantees and
utilize a graph structure for scalable and generalizable distributed control of
MAS. We develop a novel theoretical framework to prove the safety of an
arbitrary-sized MAS with a single GCBF. We propose a new training framework
GCBF+ that uses graph neural networks to parameterize a candidate GCBF and a
distributed control policy. The proposed framework is distributed and is
capable of taking point clouds from LiDAR, instead of actual state information,
for real-world robotic applications. We illustrate the efficacy of the proposed
method through various hardware experiments on a swarm of drones with
objectives ranging from exchanging positions to docking on a moving target
without collision. Additionally, we perform extensive numerical experiments,
where the number and density of agents, as well as the number of obstacles,
increase. Empirical results show that in complex environments with agents with
nonlinear dynamics (e.g., Crazyflie drones), GCBF+ outperforms the hand-crafted
CBF-based method with the best performance by up to 20% for relatively
small-scale MAS with up to 256 agents, and leading reinforcement learning (RL)
methods by up to 40% for MAS with 1024 agents. Furthermore, the proposed method
does not compromise on the performance, in terms of goal reaching, for
achieving high safety rates, which is a common trade-off in RL-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 15 figures; Accepted by IEEE Transactions on Robotics
  (T-RO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Planning Domain Inference for Task and Motion Planning <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinbang Huang, Allen Tao, Rozilyn Marco, Miroslav Bogdanovic, Jonathan Kelly, Florian Shkurti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task and motion planning (TAMP) frameworks address long and complex planning
problems by integrating high-level task planners with low-level motion
planners. However, existing TAMP methods rely heavily on the manual design of
planning domains that specify the preconditions and postconditions of all
high-level actions. This paper proposes a method to automate planning domain
inference from a handful of test-time trajectory demonstrations, reducing the
reliance on human design. Our approach incorporates a deep learning-based
estimator that predicts the appropriate components of a domain for a new task
and a search algorithm that refines this prediction, reducing the size and
ensuring the utility of the inferred domain. Our method is able to generate new
domains from minimal demonstrations at test time, enabling robots to handle
complex tasks more efficiently. We demonstrate that our approach outperforms
behavior cloning baselines, which directly imitate planner behavior, in terms
of planning performance and generalization across a variety of tasks.
Additionally, our method reduces computational costs and data amount
requirements at test time for inferring new planning domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 International Conference on Robotics and
  Automation(ICRA) 8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Structural-RNN for Robot Crowd Navigation with Deep
  Reinforcement Learning <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.04820v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.04820v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuijing Liu, Peixin Chang, Weihang Liang, Neeloy Chakraborty, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe and efficient navigation through human crowds is an essential capability
for mobile robots. Previous work on robot crowd navigation assumes that the
dynamics of all agents are known and well-defined. In addition, the performance
of previous methods deteriorates in partially observable environments and
environments with dense crowds. To tackle these problems, we propose
decentralized structural-Recurrent Neural Network (DS-RNN), a novel network
that reasons about spatial and temporal relationships for robot decision making
in crowd navigation. We train our network with model-free deep reinforcement
learning without any expert supervision. We demonstrate that our model
outperforms previous methods in challenging crowd navigation scenarios. We
successfully transfer the policy learned in the simulator to a real-world
TurtleBot 2i. For more information, please visit the project website at
https://sites.google.com/view/crowdnav-ds-rnn/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper in IEEE International Conference on
  Robotics and Automation (ICRA), 2021</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-26T00:00:00Z">2025-01-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirIO: Learning Inertial Odometry with Enhanced IMU Feature
  Observability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Qiu, Can Xu, Yutian Chen, Shibo Zhao, Junyi Geng, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial odometry (IO) using only Inertial Measurement Units (IMUs) offers a
lightweight and cost-effective solution for Unmanned Aerial Vehicle (UAV)
applications, yet existing learning-based IO models often fail to generalize to
UAVs due to the highly dynamic and non-linear-flight patterns that differ from
pedestrian motion. In this work, we identify that the conventional practice of
transforming raw IMU data to global coordinates undermines the observability of
critical kinematic information in UAVs. By preserving the body-frame
representation, our method achieves substantial performance improvements, with
a 66.7% average increase in accuracy across three datasets. Furthermore,
explicitly encoding attitude information into the motion network results in an
additional 23.8% improvement over prior results. Combined with a data-driven
IMU correction model (AirIMU) and an uncertainty-aware Extended Kalman Filter
(EKF), our approach ensures robust state estimation under aggressive UAV
maneuvers without relying on external sensors or control inputs. Notably, our
method also demonstrates strong generalizability to unseen data not included in
the training set, underscoring its potential for real-world UAV applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your Learned Constraint is Secretly a Backward Reachable Tube 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Qadri, Gokul Swamy, Jonathan Francis, Michael Kaess, Andrea Bajcsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse Constraint Learning (ICL) is the problem of inferring constraints
from safe (i.e., constraint-satisfying) demonstrations. The hope is that these
inferred constraints can then be used downstream to search for safe policies
for new tasks and, potentially, under different dynamics. Our paper explores
the question of what mathematical entity ICL recovers. Somewhat surprisingly,
we show that both in theory and in practice, ICL recovers the set of states
where failure is inevitable, rather than the set of states where failure has
already happened. In the language of safe control, this means we recover a
backwards reachable tube (BRT) rather than a failure set. In contrast to the
failure set, the BRT depends on the dynamics of the data collection system. We
discuss the implications of the dynamics-conditionedness of the recovered
constraint on both the sample-efficiency of policy search and the
transferability of learned constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-Based Planning for Autonomous Driving with Flexible Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Zheng, Ruiming Liang, Kexin Zheng, Jinliang Zheng, Liyuan Mao, Jianxiong Li, Weihao Gu, Rui Ai, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving human-like driving behaviors in complex open-world environments is
a critical challenge in autonomous driving. Contemporary learning-based
planning approaches such as imitation learning methods often struggle to
balance competing objectives and lack of safety assurance,due to limited
adaptability and inadequacy in learning complex multi-modal behaviors commonly
exhibited in human planning, not to mention their strong reliance on the
fallback strategy with predefined rules. We propose a novel transformer-based
Diffusion Planner for closed-loop planning, which can effectively model
multi-modal driving behavior and ensure trajectory quality without any
rule-based refinement. Our model supports joint modeling of both prediction and
planning tasks under the same architecture, enabling cooperative behaviors
between vehicles. Moreover, by learning the gradient of the trajectory score
function and employing a flexible classifier guidance mechanism, Diffusion
Planner effectively achieves safe and adaptable planning behaviors. Evaluations
on the large-scale real-world autonomous planning benchmark nuPlan and our
newly collected 200-hour delivery-vehicle driving dataset demonstrate that
Diffusion Planner achieves state-of-the-art closed-loop performance with robust
transferability in diverse driving styles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Potential of iMarkers: Invisible Fiducial Markers for
  Advanced Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Tourani, Deniz Isinsu Avsar, Hriday Bavle, Jose Luis Sanchez-Lopez, Jan Lagerwall, Holger Voos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fiducial markers are widely used in various robotics tasks, facilitating
enhanced navigation, object recognition, and scene understanding. Despite their
advantages for robots and Augmented Reality (AR) applications, they often
disrupt the visual aesthetics of environments because they are visible to
humans, making them unsuitable for non-intrusive use cases. To address this
gap, this paper presents "iMarkers"-innovative, unobtrusive fiducial markers
detectable exclusively by robots equipped with specialized sensors. These
markers offer high flexibility in production, allowing customization of their
visibility range and encoding algorithms to suit various demands. The paper
also introduces the hardware designs and software algorithms developed for
detecting iMarkers, highlighting their adaptability and robustness in the
detection and recognition stages. Various evaluations have demonstrated the
effectiveness of iMarkers compared to conventional (printed) and blended
fiducial markers and confirmed their applicability in diverse robotics
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAVbot: An Autonomous Target Tracking Micro-Robot with Frequency
  Actuation Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Hao, Ashwin Lele, Yan Fang, Arijit Raychowdhury, Azadeh Ansari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic autonomy at centimeter scale requires compact and
miniaturization-friendly actuation integrated with sensing and neural network
processing assembly within a tiny form factor. Applications of such systems
have witnessed significant advancements in recent years in fields such as
healthcare, manufacturing, and post-disaster rescue. The system design at this
scale puts stringent constraints on power consumption for both the sensory
front-end and actuation back-end and the weight of the electronic assembly for
robust operation. In this paper, we introduce FAVbot, the first autonomous
mobile micro-robotic system integrated with a novel actuation mechanism and
convolutional neural network (CNN) based computer vision - all integrated
within a compact 3-cm form factor. The novel actuation mechanism utilizes
mechanical resonance phenomenon to achieve frequency-controlled steering with a
single piezoelectric actuator. Experimental results demonstrate the
effectiveness of FAVbot's frequency-controlled actuation, which offers a
diverse selection of resonance modes with different motion characteristics. The
actuation system is complemented with the vision front-end where a camera along
with a microcontroller supports object detection for closed-loop control and
autonomous target tracking. This enables adaptive navigation in dynamic
environments. This work contributes to the evolving landscape of neural
network-enabled micro-robotic systems showing the smallest autonomous robot
built using controllable multi-directional single-actuator mechanism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under consideration for journal publication. Authors
  reserve the right to transfer copyright without notice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Sim2Real Gap: Vision Encoder <span class="highlight-title">Pre-Train</span>ing for Visuomotor
  Policy Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Biruduganti, Yash Yardi, Lars Ankile
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation offers a scalable and efficient alternative to real-world data
collection for learning visuomotor robotic policies. However, the
simulation-to-reality, or "Sim2Real" distribution shift -- introduced by
employing simulation-trained policies in real-world environments -- frequently
prevents successful policy transfer. This study explores the potential of using
large-scale pre-training of vision encoders to address the Sim2Real gap. We
examine a diverse collection of encoders, evaluating their ability to (1)
extract features necessary for robot control while (2) remaining invariant to
task-irrelevant environmental variations. We quantitatively measure the
encoder's feature extraction capabilities through linear probing and its domain
invariance by computing distances between simulation and real-world embedding
centroids. Additional qualitative insights are provided through t-SNE plots and
GradCAM saliency maps. Findings suggest that encoders pre-trained on
manipulation-specific datasets generally outperform those trained on generic
datasets in bridging the Sim2Real gap.
https://github.com/yyardi/Bridging-the-Sim2Real-Gap
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures, view GitHub for all appendix figures from the
  study</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Assessment of Lidar Odometry Frameworks: A Case Study at the
  Australian Botanic Garden Mount Annan 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Mourad Ouazghire, Julie Stephany Berrio, Mao Shan, Stewart Worrall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles are being tested in diverse environments worldwide.
However, a notable gap exists in evaluating datasets representing natural,
unstructured environments such as forests or gardens. To address this, we
present a study on localisation at the Australian Botanic Garden Mount Annan.
This area encompasses open grassy areas, paved pathways, and densely vegetated
sections with trees and other objects. The dataset was recorded using a
128-beam LiDAR sensor and GPS and IMU readings to track the ego-vehicle. This
paper evaluates the performance of two state-of-the-art LiDARinertial odometry
frameworks, COIN-LIO and LIO-SAM, on this dataset. We analyse trajectory
estimates in both horizontal and vertical dimensions and assess relative
translation and yaw errors over varying distances. Our findings reveal that
while both frameworks perform adequately in the vertical plane, COINLIO
demonstrates superior accuracy in the horizontal plane, particularly over
extended trajectories. In contrast, LIO-SAM shows increased drift and yaw
errors over longer distances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2024 Australasian Conference on Robotics and Automation (ACRA
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal and Force-Matched Imitation Learning with a See-Through
  Visuotactile Sensor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01248v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01248v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor Ablett, Oliver Limoyo, Adam Sigal, Affan Jilani, Jonathan Kelly, Kaleem Siddiqi, Francois Hogan, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contact-rich tasks continue to present many challenges for robotic
manipulation. In this work, we leverage a multimodal visuotactile sensor within
the framework of imitation learning (IL) to perform contact-rich tasks that
involve relative motion (e.g., slipping and sliding) between the end-effector
and the manipulated object. We introduce two algorithmic contributions, tactile
force matching and learned mode switching, as complimentary methods for
improving IL. Tactile force matching enhances kinesthetic teaching by reading
approximate forces during the demonstration and generating an adapted robot
trajectory that recreates the recorded forces. Learned mode switching uses IL
to couple visual and tactile sensor modes with the learned motion policy,
simplifying the transition from reaching to contacting. We perform robotic
manipulation experiments on four door-opening tasks with a variety of
observation and algorithm configurations to study the utility of multimodal
visuotactile sensing and our proposed improvements. Our results show that the
inclusion of force matching raises average policy success rates by 62.5%,
visuotactile mode switching by 30.3%, and visuotactile data as a policy input
by 42.5%, emphasizing the value of see-through tactile sensing for IL, both for
data collection to allow force matching, and for policy execution to enable
accurate task feedback. Project site: https://papers.starslab.ca/sts-il/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sensor-Based Distributionally Robust Control for Safe Robot Navigation
  in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehan Long, Yinzhuang Yi, Zhirui Dai, Sylvia Herbert, Jorge Cortés, Nikolay Atanasov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel method for mobile robot navigation in dynamic, unknown
environments, leveraging onboard sensing and distributionally robust
optimization to impose probabilistic safety constraints. Our method introduces
a distributionally robust control barrier function (DR-CBF) that directly
integrates noisy sensor measurements and state estimates to define safety
constraints. This approach is applicable to a wide range of control-affine
dynamics, generalizable to robots with complex geometries, and capable of
operating at real-time control frequencies. Coupled with a control Lyapunov
function (CLF) for path following, the proposed CLF-DR-CBF control synthesis
method achieves safe, robust, and efficient navigation in challenging
environments. We demonstrate the effectiveness and robustness of our approach
for safe autonomous navigation under uncertainty in simulations and real-world
experiments with differential-drive robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://existentialrobotics.org/DRO_Safe_Navigation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Model Predictive Covariance Steering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Augustinos D. Saravanos, Isin M. Balci, Efstathios Bakolas, Evangelos A. Theodorou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Distributed Model Predictive Covariance Steering (DiMPCS)
for multi-agent control under stochastic uncertainty. The scope of our approach
is to blend covariance steering theory, distributed optimization and model
predictive control (MPC) into a single framework that is safe, scalable and
decentralized. Initially, we pose a problem formulation that uses the
Wasserstein distance to steer the state distributions of a multi-agent system
to desired targets, and probabilistic constraints to ensure safety. We then
transform this problem into a finite-dimensional optimization one by utilizing
a disturbance feedback policy parametrization for covariance steering and a
tractable approximation of the safety constraints. To solve the latter problem,
we derive a decentralized consensus-based algorithm using the Alternating
Direction Method of Multipliers. This method is then extended to a receding
horizon form, which yields the proposed DiMPCS algorithm. Simulation
experiments on a variety of multi-robot tasks with up to hundreds of robots
demonstrate the effectiveness of DiMPCS. The superior scalability and
performance of the proposed method is also highlighted through a comparison
against related stochastic MPC approaches. Finally, hardware results on a
multi-robot platform also verify the applicability of DiMPCS on real systems. A
video with all results is available in https://youtu.be/tzWqOzuj2kQ.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-25T00:00:00Z">2025-01-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Agile Transportation of Cable-Suspended Payload via Multiple
  Aerial Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Wang, Junjie Wang, Xiaobin Zhou, Tiankai Yang, Chao Xu, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transporting a heavy payload using multiple aerial robots (MARs) is an
efficient manner to extend the load capacity of a single aerial robot. However,
existing schemes for the multiple aerial robots transportation system (MARTS)
still lack the capability to generate a collision-free and dynamically feasible
trajectory in real-time and further track an agile trajectory especially when
there are no sensors available to measure the states of payload and cable.
Therefore, they are limited to low-agility transportation in simple
environments. To bridge the gap, we propose complete planning and control
schemes for the MARTS, achieving safe and agile aerial transportation (SAAT) of
a cable-suspended payload in complex environments. Flatness maps for the aerial
robot considering the complete kinematical constraint and the dynamical
coupling between each aerial robot and payload are derived. To improve the
responsiveness for the generation of the safe, dynamically feasible, and agile
trajectory in complex environments, a real-time spatio-temporal trajectory
planning scheme is proposed for the MARTS. Besides, we break away from the
reliance on the state measurement for both the payload and cable, as well as
the closed-loop control for the payload, and propose a fully distributed
control scheme to track the agile trajectory that is robust against imprecise
payload mass and non-point mass payload. The proposed schemes are extensively
validated through benchmark comparisons, ablation studies, and simulations.
Finally, extensive real-world experiments are conducted on a MARTS integrated
by three aerial robots with onboard computers and sensors. The result validates
the efficiency and robustness of our proposed schemes for SAAT in complex
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 14 figures, submitted to IEEE Transactions on Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Robotic Manipulation with Language-guided Instruction and
  Formal Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Tang, Zihan Ye, Yuping Yan, Ziqi Zheng, Ting Gao, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation is often challenging due to the long-horizon tasks and
the complex object relationships. A common solution is to develop a task and
motion planning framework that integrates planning for high-level task and
low-level motion. Recently, inspired by the powerful reasoning ability of Large
Language Models (LLMs), LLM-based planning approaches have achieved remarkable
progress. However, these methods still heavily rely on expert-specific
knowledge, often generating invalid plans for unseen and unfamiliar tasks. To
address this issue, we propose an innovative language-guided symbolic task
planning (LM-SymOpt) framework with optimization. It is the first expert-free
planning framework since we combine the world knowledge from LLMs with formal
reasoning, resulting in improved generalization capability to new tasks.
Specifically, differ to most existing work, our LM-SymOpt employs LLMs to
translate natural language instructions into symbolic representations, thereby
representing actions as high-level symbols and reducing the search space for
planning. Next, after evaluating the action probability of completing the task
using LLMs, a weighted random sampling method is introduced to generate
candidate plans. Their feasibility is assessed through symbolic reasoning and
their cost efficiency is then evaluated using trajectory optimization for
selecting the optimal planning. Our experimental results show that LM-SymOpt
outperforms existing LLM-based planning approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Conscious Service Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sven Behnke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning's success in perception, natural language processing, etc.
inspires hopes for advancements in autonomous robotics. However, real-world
robotics face challenges like variability, high-dimensional state spaces,
non-linear dependencies, and partial observability. A key issue is
non-stationarity of robots, environments, and tasks, leading to performance
drops with out-of-distribution data. Unlike current machine learning models,
humans adapt quickly to changes and new tasks due to a cognitive architecture
that enables systematic generalization and meta-cognition. Human brain's System
1 handles routine tasks unconsciously, while System 2 manages complex tasks
consciously, facilitating flexible problem-solving and self-monitoring. For
robots to achieve human-like learning and reasoning, they need to integrate
causal models, working memory, planning, and metacognitive processing. By
incorporating human cognition insights, the next generation of service robots
will handle novel situations and monitor themselves to avoid risks and mitigate
errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In: Science for a Better Tomorrow: Curious 2024 Insights Actions,
  Springer 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Forward Invariant Sets from Neural Network-Based Control
  Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goli Vaisi, James Ferlez, Yasser Shoukry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Neural Networks (NNs) to serve as Barrier Functions (BFs) is a
popular way to improve the safety of autonomous dynamical systems. Despite
significant practical success, these methods are not generally guaranteed to
produce true BFs in a provable sense, which undermines their intended use as
safety certificates. In this paper, we consider the problem of formally
certifying a learned NN as a BF with respect to state avoidance for an
autonomous system: viz. computing a region of the state space on which the
candidate NN is provably a BF. In particular, we propose a sound algorithm that
efficiently produces such a certificate set for a shallow NN. Our algorithm
combines two novel approaches: it first uses NN reachability tools to identify
a subset of states for which the output of the NN does not increase along
system trajectories; then, it uses a novel enumeration algorithm for hyperplane
arrangements to find the intersection of the NN's zero-sub-level set with the
first set of states. In this way, our algorithm soundly finds a subset of
states on which the NN is certified as a BF. We further demonstrate the
effectiveness of our algorithm at certifying for real-world NNs as BFs in two
case studies. We complemented these with scalability experiments that
demonstrate the efficiency of our algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact-resistant, autonomous robots inspired by tensegrity architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William R. Johnson III, Xiaonan Huang, Shiyang Lu, Kun Wang, Joran W. Booth, Kostas Bekris, Rebecca Kramer-Bottiglio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future robots will navigate perilous, remote environments with resilience and
autonomy. Researchers have proposed building robots with compliant bodies to
enhance robustness, but this approach often sacrifices the autonomous
capabilities expected of rigid robots. Inspired by tensegrity architecture, we
introduce a tensegrity robot -- a hybrid robot made from rigid struts and
elastic tendons -- that demonstrates the advantages of compliance and the
autonomy necessary for task performance. This robot boasts impact resistance
and autonomy in a field environment and additional advances in the state of the
art, including surviving harsh impacts from drops (at least 5.7 m), accurately
reconstructing its shape and orientation using on-board sensors, achieving high
locomotion speeds (18 bar lengths per minute), and climbing the steepest
incline of any tensegrity robot (28 degrees). We characterize the robot's
locomotion on unstructured terrain, showcase its autonomous capabilities in
navigation tasks, and demonstrate its robustness by rolling it off a cliff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding via Gaze: Gaze-based Task Decomposition for Imitation
  Learning of Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Takizawa, Yoshiyuki Ohmura, Yasuo Kuniyoshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In imitation learning for robotic manipulation, decomposing object
manipulation tasks into multiple semantic actions is essential. This
decomposition enables the reuse of learned skills in varying contexts and the
combination of acquired skills to perform novel tasks, rather than merely
replicating demonstrated motions. Gaze, an evolutionary tool for understanding
ongoing events, plays a critical role in human object manipulation, where it
strongly correlates with motion planning. In this study, we propose a simple
yet robust task decomposition method based on gaze transitions. We hypothesize
that an imitation agent's gaze control, fixating on specific landmarks and
transitioning between them, naturally segments demonstrated manipulations into
sub-tasks. Notably, our method achieves consistent task decomposition across
all demonstrations, which is desirable in contexts such as machine learning.
Using teleoperation, a common modality in imitation learning for robotic
manipulation, we collected demonstration data for various tasks, applied our
segmentation method, and evaluated the characteristics and consistency of the
resulting sub-tasks. Furthermore, through extensive testing across a wide range
of hyperparameter variations, we demonstrated that the proposed method
possesses the robustness necessary for application to different robotic
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Atomic Skill Library Construction Method for Data-Efficient Embodied
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjiang Li, Bo Peng, Chang Li, Ning Qiao, Qi Zheng, Lei Sun, Yusen Qin, Bangguo Li, Yifeng Luan, Yibing Zhan, Mingang Sun, Tong Xu, Lusong Li, Hui Shen, Xiaodong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied manipulation is a fundamental ability in the realm of embodied
artificial intelligence. Although current embodied manipulation models show
certain generalizations in specific settings, they struggle in new environments
and tasks due to the complexity and diversity of real-world scenarios. The
traditional end-to-end data collection and training manner leads to significant
data demands, which we call ``data explosion''. To address the issue, we
introduce a three-wheeled data-driven method to build an atomic skill library.
We divide tasks into subtasks using the Vision-Language Planning (VLP). Then,
atomic skill definitions are formed by abstracting the subtasks. Finally, an
atomic skill library is constructed via data collection and
Vision-Language-Action (VLA) fine-tuning. As the atomic skill library expands
dynamically with the three-wheel update strategy, the range of tasks it can
cover grows naturally. In this way, our method shifts focus from end-to-end
tasks to atomic skills, significantly reducing data costs while maintaining
high performance and enabling efficient adaptation to new tasks. Extensive
experiments in real-world settings demonstrate the effectiveness and efficiency
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extensive Exploration in Complex Traffic Scenarios using Hierarchical
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Zhang, Ekim Yurtsever, Keith A. Redmill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing an automated driving system capable of navigating complex traffic
environments remains a formidable challenge. Unlike rule-based or supervised
learning-based methods, Deep Reinforcement Learning (DRL) based controllers
eliminate the need for domain-specific knowledge and datasets, thus providing
adaptability to various scenarios. Nonetheless, a common limitation of existing
studies on DRL-based controllers is their focus on driving scenarios with
simple traffic patterns, which hinders their capability to effectively handle
complex driving environments with delayed, long-term rewards, thus compromising
the generalizability of their findings. In response to these limitations, our
research introduces a pioneering hierarchical framework that efficiently
decomposes intricate decision-making problems into manageable and interpretable
subtasks. We adopt a two step training process that trains the high-level
controller and low-level controller separately. The high-level controller
exhibits an enhanced exploration potential with long-term delayed rewards, and
the low-level controller provides longitudinal and lateral control ability
using short-term instantaneous rewards. Through simulation experiments, we
demonstrate the superiority of our hierarchical controller in managing complex
highway driving situations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal DLT-based Solutions for the Perspective-n-Point 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sébastien Henry, John A. Christian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a modified normalized direct linear transform (DLT) algorithm for
solving the perspective-n-point (PnP) problem with much better behavior than
the conventional DLT. The modification consists of analytically weighting the
different measurements in the linear system with a negligible increase in
computational load. Our approach exhibits clear improvements -- in both
performance and runtime -- when compared to popular methods such as EPnP, CPnP,
RPnP, and OPnP. Our new non-iterative solution approaches that of the true
optimal found via Gauss-Newton optimization, but at a fraction of the
computational cost. Our optimal DLT (oDLT) implementation, as well as the
experiments, are released in open source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon
  Visuomotor Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Break Yang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a low-cost legged mobile manipulation system that solves
long-horizon real-world tasks, trained by reinforcement learning purely in
simulation. This system is made possible by 1) a hierarchical design of a
high-level policy for visual-mobile manipulation following instructions and a
low-level policy for quadruped movement and limb control, 2) a progressive
exploration and learning approach that leverages privileged task decomposition
information to train the teacher policy for long-horizon tasks, which will
guide an imitation-based student policy for efficient training of the
high-level visuomotor policy, and 3) a suite of techniques for minimizing
sim-to-real gaps.
  In contrast to previous approaches that use high-end equipment, our system
demonstrates effective performance with more accessible hardware -
specifically, a Unitree Go1 quadruped, a WidowX250S arm, and a single
wrist-mounted RGB camera - despite the increased challenges of sim-to-real
transfer. When fully trained in simulation, a single policy autonomously solves
long-horizon tasks such as search, move, grasp, and drop-into, achieving nearly
80% success. This performance is comparable to that of expert human
teleoperation on the same tasks but significantly more efficient, operating at
about 1.5x the speed. The sim-to-real transfer is fluid across diverse indoor
and outdoor scenes under varying lighting conditions. Finally, we discuss the
key techniques that enable the entire pipeline, including efficient RL training
and sim-to-real, to work effectively for legged mobile manipulation, and
present their ablation results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imperative Learning: A <span class="highlight-title">Self-supervised</span> Neuro-Symbolic Learning Framework
  for Robot Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16087v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16087v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Wang, Kaiyi Ji, Junyi Geng, Zhongqiang Ren, Taimeng Fu, Fan Yang, Yifan Guo, Haonan He, Xiangyu Chen, Zitong Zhan, Qiwei Du, Shaoshu Su, Bowen Li, Yuheng Qiu, Yi Du, Qihang Li, Yifan Yang, Xiao Lin, Zhipeng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven methods such as reinforcement and imitation learning have
achieved remarkable success in robot autonomy. However, their data-centric
nature still hinders them from generalizing well to ever-changing environments.
Moreover, collecting large datasets for robotic tasks is often impractical and
expensive. To overcome these challenges, we introduce a new self-supervised
neuro-symbolic (NeSy) computational framework, imperative learning (IL), for
robot autonomy, leveraging the generalization abilities of symbolic reasoning.
The framework of IL consists of three primary components: a neural module, a
reasoning engine, and a memory system. We formulate IL as a special bilevel
optimization (BLO), which enables reciprocal learning over the three modules.
This overcomes the label-intensive obstacles associated with data-driven
approaches and takes advantage of symbolic reasoning concerning logical
reasoning, physical principles, geometric analysis, etc. We discuss several
optimization techniques for IL and verify their effectiveness in five distinct
robot autonomy tasks including path planning, rule induction, optimal control,
visual odometry, and multi-robot routing. Through various experiments, we show
that IL can significantly enhance robot autonomy capabilities and we anticipate
that it will catalyze further research across diverse domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Spacecraft Trajectory Optimization via <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuji Takubo, Tommaso Guffanti, Daniele Gammelli, Marco Pavone, Simone D'Amico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future multi-spacecraft missions require robust autonomous trajectory
optimization capabilities to ensure safe and efficient rendezvous operations.
This capability hinges on solving non-convex optimal control problems in
real-time, although traditional iterative methods such as sequential convex
programming impose significant computational challenges. To mitigate this
burden, the Autonomous Rendezvous Transformer (ART) introduced a generative
model trained to provide near-optimal initial guesses. This approach provides
convergence to better local optima (e.g., fuel optimality), improves
feasibility rates, and results in faster convergence speed of optimization
algorithms through warm-starting. This work extends the capabilities of ART to
address robust chance-constrained optimal control problems. Specifically, ART
is applied to challenging rendezvous scenarios in Low Earth Orbit (LEO),
ensuring fault-tolerant behavior under uncertainty. Through extensive
experimentation, the proposed warm-starting strategy is shown to consistently
produce high-quality reference trajectories, achieving up to 30\% cost
improvement and 50\% reduction in infeasible cases compared to conventional
methods, demonstrating robust performance across multiple state
representations. Additionally, a post hoc evaluation framework is proposed to
assess the quality of generated trajectories and mitigate runtime failures,
marking an initial step toward the reliable deployment of AI-driven solutions
in safety-critical autonomous systems such as spacecraft.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE Aerospace Conference 2025. 13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Fault Tolerance Strategy for Abrupt Reachability Constraint
  Changes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henghua Shen, Qixin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a system's constraints change abruptly, the system's reachability safety
does no longer sustain. Thus, the system can reach a forbidden/dangerous value.
Conventional remedy practically involves online controller redesign (OCR) to
re-establish the reachability's compliance with the new constraints, which,
however, is usually too slow. There is a need for an online strategy capable of
managing runtime changes in reachability constraints. However, to the best of
the authors' knowledge, this topic has not been addressed in the existing
literature. In this paper, we propose a fast fault tolerance strategy to
recover the system's reachability safety in runtime. Instead of redesigning the
system's controller, we propose to change the system's reference state to
modify the system's reachability to comply with the new constraints. We frame
the reference state search as an optimization problem and employ the
Karush-Kuhn-Tucker (KKT) method as well as the Interior Point Method (IPM)
based Newton's method (as a fallback for the KKT method) for fast solution
derivation. The optimization also allows more future fault tolerance. Numerical
simulations demonstrate that our method outperforms the conventional OCR method
in terms of computational efficiency and success rate. Specifically, the
results show that the proposed method finds a solution $10^{2}$ (with the IPM
based Newton's method) $\sim 10^{4}$ (with the KKT method) times faster than
the OCR method. Additionally, the improvement rate of the success rate of our
method over the OCR method is $40.81\%$ without considering the deadline of run
time. The success rate remains at $49.44\%$ for the proposed method, while it
becomes $0\%$ for the OCR method when a deadline of $1.5 \; seconds$ is
imposed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Scaling Laws in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Sartor, Neil Thompson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural scaling laws have driven significant advancements in machine learning,
particularly in domains like language modeling and computer vision. However,
the exploration of neural scaling laws within robotics has remained relatively
underexplored, despite the growing adoption of foundation models in this field.
This paper represents the first comprehensive study to quantify neural scaling
laws for Robot Foundation Models (RFMs) and Large Language Models (LLMs) in
robotics tasks. Through a meta-analysis of 327 research papers, we investigate
how data size, model size, and compute resources influence downstream
performance across a diverse set of robotic tasks. Consistent with previous
scaling law research, our results reveal that the performance of robotic models
improves with increased resources, following a power-law relationship.
Promisingly, the improvement in robotic task performance scales notably faster
than language tasks. This suggests that, while performance on downstream
robotic tasks today is often moderate-to-poor, increased data and compute are
likely to signficantly improve performance in the future. Also consistent with
previous scaling law research, we also observe the emergence of new robot
capabilities as models scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Strange Attractor Model of Bipedal Locomotion and its Consequences
  on Motor Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1802.03498v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1802.03498v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Tiseo, Ming Jeat Foo, Kalyana C Veluvolu, Arturo Forner-Cordero, Wei Tech Ang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite decades of study, many unknowns exist about the mechanisms governing
human locomotion. Current models and motor control theories can only partially
capture the phenomenon. This may be a major cause of the reduced efficacy of
lower limb rehabilitation therapies. Recently, it has been proposed that human
locomotion can be planned in the task-space by taking advantage of the
gravitational pull acting on the Centre of Mass (CoM) by modelling the
attractor dynamics. The model proposed represents the CoM transversal
trajectory as a harmonic oscillator propagating on the attractor manifold.
However, the vertical trajectory of the CoM, controlled through ankle
strategies, has not been accurately captured yet. Research Questions: Is it
possible to improve the model accuracy by introducing a mathematical model of
the ankle strategies by coordinating the heel-strike and toe-off strategies
with the CoM movement? Our solution consists of closed-form equations that plan
human-like trajectories for the CoM, the foot swing, and the ankle strategies.
We have tested our model by extracting the biomechanics data and postural
during locomotion from the motion capture trajectories of 12 healthy subjects
at 3 self-selected speeds to generate a virtual subject using our model. Our
virtual subject has been based on the average of the collected data. The model
output shows our virtual subject has walking trajectories that have their
features consistent with our motion capture data. Additionally, it emerged from
the data analysis that our model regulates the stance phase of the foot as
humans do. The model proves that locomotion can be modelled as an attractor
dynamics, proving the existence of a nonlinear map that our nervous system
learns. It can support a deeper investigation of locomotion motor control,
potentially improving locomotion rehabilitation and assistive technologies.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-24T00:00:00Z">2025-01-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">27</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Predictive Approach for Enhancing Accuracy in Remote Robotic Surgery
  Using Informer Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Hanif Lashari, Shakil Ahmed, Wafa Batayneh, Ashfaq Khokhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise and real-time estimation of the robotic arm's position on the
patient's side is essential for the success of remote robotic surgery in
Tactile Internet (TI) environments. This paper presents a prediction model
based on the Transformer-based Informer framework for accurate and efficient
position estimation. Additionally, it combines a Four-State Hidden Markov Model
(4-State HMM) to simulate realistic packet loss scenarios. The proposed
approach addresses challenges such as network delays, jitter, and packet loss
to ensure reliable and precise operation in remote surgical applications. The
method integrates the optimization problem into the Informer model by embedding
constraints such as energy efficiency, smoothness, and robustness into its
training process using a differentiable optimization layer. The Informer
framework uses features such as ProbSparse attention, attention distilling, and
a generative-style decoder to focus on position-critical features while
maintaining a low computational complexity of O(L log L). The method is
evaluated using the JIGSAWS dataset, achieving a prediction accuracy of over 90
percent under various network scenarios. A comparison with models such as TCN,
RNN, and LSTM demonstrates the Informer framework's superior performance in
handling position prediction and meeting real-time requirements, making it
suitable for Tactile Internet-enabled robotic surgery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian-Process-based Adaptive Tracking Control with Dynamic Active
  Learning for Autonomous Ground Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristóf Floch, Tamás Péni, Roland Tóth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proposes an active-learning-based adaptive trajectory tracking
control method for autonomous ground vehicles to compensate for modeling errors
and unmodeled dynamics. The nominal vehicle model is decoupled into lateral and
longitudinal subsystems, which are augmented with online Gaussian Processes
(GPs), using measurement data. The estimated mean functions of the GPs are used
to construct a feedback compensator, which, together with an LPV state feedback
controller designed for the nominal system, gives the adaptive control
structure. To assist exploration of the dynamics, the paper proposes a new,
dynamic active learning method to collect the most informative samples to
accelerate the training process. To analyze the performance of the overall
learning tool-chain provided controller, a novel iterative,
counterexample-based algorithm is proposed for calculating the induced L2 gain
between the reference trajectory and the tracking error. The analysis can be
executed for a set of possible realizations of the to-be-controlled system,
giving robust performance certificate of the learning method under variation of
the vehicle dynamics. The efficiency of the proposed control approach is shown
on a high-fidelity physics simulator and in real experiments using a 1/10 scale
F1TENTH electric car.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Control Systems Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuIP: Experimental design for expensive simulators with many Qualitative
  factors via Integer Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Chun Liu, Simon Mak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need to explore and/or optimize expensive simulators with many
qualitative factors arises in broad scientific and engineering problems. Our
motivating application lies in path planning - the exploration of feasible
paths for navigation, which plays an important role in robotics, surgical
planning and assembly planning. Here, the feasibility of a path is evaluated
via expensive virtual experiments, and its parameter space is typically
discrete and high-dimensional. A carefully selected experimental design is thus
essential for timely decision-making. We propose here a novel framework, called
QuIP, for experimental design of Qualitative factors via Integer Programming
under a Gaussian process surrogate model with an exchangeable covariance
function. For initial design, we show that its asymptotic D-optimal design can
be formulated as a variant of the well-known assignment problem in operations
research, which can be efficiently solved to global optimality using
state-of-the-art integer programming solvers. For sequential design
(specifically, for active learning or black-box optimization), we show that its
design criterion can similarly be formulated as an assignment problem, thus
enabling efficient and reliable optimization with existing solvers. We then
demonstrate the effectiveness of QuIP over existing methods in a suite of path
planning experiments and an application to rover trajectory optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 6 figures, submitted to JCGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Localization via Semantic Structures in Autonomous Photovoltaic
  Power Plant Inspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Kozák, Karel Košnar, Jan Chudoba, Miroslav Kulich, Libor Přeučil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspection systems utilizing unmanned aerial vehicles (UAVs) equipped with
thermal cameras are increasingly popular for the maintenance of photovoltaic
(PV) power plants. However, automation of the inspection task is a challenging
problem as it requires precise navigation to capture images from optimal
distances and viewing angles.
  This paper presents a novel localization pipeline that directly integrates PV
module detection with UAV navigation, allowing precise positioning during
inspection. Detections are used to identify the power plant structures in the
image and associate these with the power plant model. We define visually
recognizable anchor points for the initial association and use object tracking
to discern global associations. We present three distinct methods for visual
segmentation of PV modules based on traditional computer vision, deep learning,
and their fusion, and we evaluate their performance in relation to the proposed
localization pipeline.
  The presented methods were verified and evaluated using custom aerial
inspection data sets, demonstrating their robustness and applicability for
real-time navigation. Additionally, we evaluate the influence of the power
plant model's precision on the localization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Grasping Precision for Industrial Pick-and-Place Tasks
  Through a Novel Visual Servoing Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khairidine Benali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of robotic arm manipulators into industrial manufacturing
lines has become common, thanks to their efficiency and effectiveness in
executing specific tasks. With advancements in camera technology, visual
sensors and perception systems have been incorporated to address more complex
operations. This study introduces a novel visual serving control system
designed for robotic operations in challenging environments, where accurate
object pose estimation is hindered by factors such as vibrations, tool path
deviations, and machining marks. To overcome these obstacles, our solution
focuses on enhancing the accuracy of picking and placing tasks, ensuring
reliable performance across various scenarios. This is accomplished by a novel
visual servoing method based on the integration of two complementary
methodologies: a technique for object localization and a separate approach for
precise control through visual feedback, leveraging their strengths to address
the challenges posed by the industrial context and thereby improving overall
grasping accuracy. Our method employ feedback from perception sensors to adjust
the control loop efficiently, enabling the robotic system to adeptly pick and
place objects. We have introduced a controller capable of seamlessly managing
the detection and manipulation of various shapes and types of objects within an
industrial context, addressing numerous challenges that arise in such
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustified Time-optimal Point-to-point Motion Planning and Control
  under Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhao Zhang, Jan Swevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to formulate time-optimal point-to-point
motion planning and control under uncertainty. The approach defines a
robustified two-stage Optimal Control Problem (OCP), in which stage 1, with a
fixed time grid, is seamlessly stitched with stage 2, which features a variable
time grid. Stage 1 optimizes not only the nominal trajectory, but also feedback
gains and corresponding state covariances, which robustify constraints in both
stages. The outcome is a minimized uncertainty in stage 1 and a minimized total
motion time for stage 2, both contributing to the time optimality and safety of
the total motion. A timely replanning strategy is employed to handle changes in
constraints and maintain feasibility, while a tailored iterative algorithm is
proposed for efficient, real-time OCP execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ABPT: Amended Backpropagation through Time with Partially Differentiable
  Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanxing Li, Fangyu Sun, Tianbao Zhang, Danping Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using the exact gradients of the rewards to directly optimize policy
parameters via backpropagation-through-time (BPTT) enables high training
performance for quadrotor tasks. However, designing a fully differentiable
reward architecture is often challenging. Partially differentiable rewards will
result in biased gradient propagation that degrades training performance. To
overcome this limitation, we propose Amended Backpropagation-through-Time
(ABPT), a novel approach that mitigates gradient bias while preserving the
training efficiency of BPTT. ABPT combines 0-step and N-step returns,
effectively reducing the bias by leveraging value gradients from the learned
Q-value function. Additionally, it adopts entropy regularization and state
initialization mechanisms to encourage exploration during training. We evaluate
ABPT on four representative quadrotor flight tasks. Experimental results
demonstrate that ABPT converges significantly faster and achieves higher
ultimate rewards than existing learning algorithms, particularly in tasks
involving partially differentiable rewards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking global optimization techniques for unmanned aerial vehicle
  path planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mhd Ali Shehadeh, Jakub Kudela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Unmanned Aerial Vehicle (UAV) path planning problem is a complex
optimization problem in the field of robotics. In this paper, we investigate
the possible utilization of this problem in benchmarking global optimization
methods. We devise a problem instance generator and pick 56 representative
instances, which we compare to established benchmarking suits through
Exploratory Landscape Analysis to show their uniqueness. For the computational
comparison, we select twelve well-performing global optimization techniques
from both subfields of stochastic algorithms (evolutionary computation methods)
and deterministic algorithms (Dividing RECTangles, or DIRECT-type methods). The
experiments were conducted in settings with varying dimensionality and
computational budgets. The results were analyzed through several criteria
(number of best-found solutions, mean relative error, Friedman ranks) and
utilized established statistical tests. The best-ranking methods for the UAV
problems were almost universally the top-performing evolutionary techniques
from recent competitions on numerical optimization at the Institute of
Electrical and Electronics Engineers Congress on Evolutionary Computation.
Lastly, we discussed the variable dimension characteristics of the studied UAV
problems that remain still largely under-investigated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-Based Vehicle Detection and Tracking for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcello Cellina, Matteo Corno, Sergio Matteo Savaresi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous racing provides a controlled environment for testing the software
and hardware of autonomous vehicles operating at their performance limits.
Competitive interactions between multiple autonomous racecars however introduce
challenging and potentially dangerous scenarios. Accurate and consistent
vehicle detection and tracking is crucial for overtaking maneuvers, and
low-latency sensor processing is essential to respond quickly to hazardous
situations. This paper presents the LiDAR-based perception algorithms deployed
on Team PoliMOVE's autonomous racecar, which won multiple competitions in the
Indy Autonomous Challenge series. Our Vehicle Detection and Tracking pipeline
is composed of a novel fast Point Cloud Segmentation technique and a specific
Vehicle Pose Estimation methodology, together with a variable-step Multi-Target
Tracking algorithm. Experimental results demonstrate the algorithm's
performance, robustness, computational efficiency, and suitability for
autonomous racing applications, enabling fully autonomous overtaking maneuvers
at velocities exceeding 275 km/h.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual-Lidar Map Alignment for Infrastructure Inspections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake McLaughlin, Nicholas Charron, Sriram Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Routine and repetitive infrastructure inspections present safety, efficiency,
and consistency challenges as they are performed manually, often in challenging
or hazardous environments. They can also introduce subjectivity and errors into
the process, resulting in undesirable outcomes. Simultaneous localization and
mapping (SLAM) presents an opportunity to generate high-quality 3D maps that
can be used to extract accurate and objective inspection data. Yet, many SLAM
algorithms are limited in their ability to align 3D maps from repeated
inspections in GPS-denied settings automatically. This limitation hinders
practical long-term asset health assessments by requiring tedious manual
alignment for data association across scans from previous inspections. This
paper introduces a versatile map alignment algorithm leveraging both visual and
lidar data for improved place recognition robustness and presents an
infrastructure-focused dataset tailored for consecutive inspections. By
detaching map alignment from SLAM, our approach enhances infrastructure
inspection pipelines, supports monitoring asset degradation over time, and
invigorates SLAM research by permitting exploration beyond existing
multi-session SLAM algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, for associated code see
  https://github.com/jakemclaughlin6/vlma</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARL-OT: Multi-Agent Reinforcement Learning Guided Online Fuzzing to
  Detect Safety Violation in Autonomous Driving Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linfeng Liang, Xi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Driving Systems (ADSs) are safety-critical, as real-world safety
violations can result in significant losses. Rigorous testing is essential
before deployment, with simulation testing playing a key role. However, ADSs
are typically complex, consisting of multiple modules such as perception and
planning, or well-trained end-to-end autonomous driving systems. Offline
methods, such as the Genetic Algorithm (GA), can only generate predefined
trajectories for dynamics, which struggle to cause safety violations for ADSs
rapidly and efficiently in different scenarios due to their evolutionary
nature. Online methods, such as single-agent reinforcement learning (RL), can
quickly adjust the dynamics' trajectory online to adapt to different scenarios,
but they struggle to capture complex corner cases of ADS arising from the
intricate interplay among multiple vehicles. Multi-agent reinforcement learning
(MARL) has a strong ability in cooperative tasks. On the other hand, it faces
its own challenges, particularly with convergence. This paper introduces
MARL-OT, a scalable framework that leverages MARL to detect safety violations
of ADS resulting from surrounding vehicles' cooperation. MARL-OT employs MARL
for high-level guidance, triggering various dangerous scenarios for the
rule-based online fuzzer to explore potential safety violations of ADS, thereby
generating dynamic, realistic safety violation scenarios. Our approach improves
the detected safety violation rate by up to 136.2% compared to the
state-of-the-art (SOTA) testing technique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning more with the same effort: how randomization improves the
  robustness of a robotic deep reinforcement learning agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucía Güitta-López, Jaime Boal, Álvaro J. López-López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The industrial application of Deep Reinforcement Learning (DRL) is frequently
slowed down because of the inability to generate the experience required to
train the models. Collecting data often involves considerable time and economic
effort that is unaffordable in most cases. Fortunately, devices like robots can
be trained with synthetic experience thanks to virtual environments. With this
approach, the sample efficiency problems of artificial agents are mitigated,
but another issue arises: the need for efficiently transferring the synthetic
experience into the real world (sim-to-real).
  This paper analyzes the robustness of a state-of-the-art sim-to-real
technique known as progressive neural networks (PNNs) and studies how adding
diversity to the synthetic experience can complement it. To better understand
the drivers that lead to a lack of robustness, the robotic agent is still
tested in a virtual environment to ensure total control on the divergence
between the simulated and real models.
  The results show that a PNN-like agent exhibits a substantial decrease in its
robustness at the beginning of the real training phase. Randomizing certain
variables during simulation-based training significantly mitigates this issue.
On average, the increase in the model's accuracy is around 25% when diversity
is introduced in the training process. This improvement can be translated into
a decrease in the required real experience for the same final robustness
performance. Notwithstanding, adding real experience to agents should still be
beneficial regardless of the quality of the virtual experience fed into the
agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article was accepted and published in Applied Intelligence
  (10.1007/s10489-022-04227-3)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SKIL: Semantic Keypoint Imitation Learning for Generalizable
  Data-efficient Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Wang, Jiacheng You, Yihang Hu, Jiongye Li, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world tasks such as garment manipulation and table rearrangement demand
robots to perform generalizable, highly precise, and long-horizon actions.
Although imitation learning has proven to be an effective approach for teaching
robots new skills, large amounts of expert demonstration data are still
indispensible for these complex tasks, resulting in high sample complexity and
costly data collection. To address this, we propose Semantic Keypoint Imitation
Learning (SKIL), a framework which automatically obtain semantic keypoints with
help of vision foundation models, and forms the descriptor of semantic
keypoints that enables effecient imitation learning of complex robotic tasks
with significantly lower sample complexity. In real world experiments, SKIL
doubles the performance of baseline methods in tasks such as picking a cup or
mouse, while demonstrating exceptional robustness to variations in objects,
environmental changes, and distractors. For long-horizon tasks like hanging a
towel on a rack where previous methods fail completely, SKIL achieves a mean
success rate of 70\% with as few as 30 demonstrations. Furthermore, SKIL
naturally supports cross-embodiment learning due to its semantic keypoints
abstraction, our experiments demonstrate that even human videos bring
considerable improvement to the learning performance. All these results
demonstrate the great success of SKIL in achieving data-efficint generalizable
robotic learning. Visualizations and code are available at:
https://skil-robotics.github.io/SKIL-robotics/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dream to Fly: Model-Based Reinforcement Learning for Vision-Based Drone
  Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel Romero, Ashwin Shenai, Ismail Geles, Elie Aljalbout, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous drone racing has risen as a challenging robotic benchmark for
testing the limits of learning, perception, planning, and control. Expert human
pilots are able to agilely fly a drone through a race track by mapping the
real-time feed from a single onboard camera directly to control commands.
Recent works in autonomous drone racing attempting direct pixel-to-commands
control policies (without explicit state estimation) have relied on either
intermediate representations that simplify the observation space or performed
extensive bootstrapping using Imitation Learning (IL). This paper introduces an
approach that learns policies from scratch, allowing a quadrotor to
autonomously navigate a race track by directly mapping raw onboard camera
pixels to control commands, just as human pilots do. By leveraging model-based
reinforcement learning~(RL) - specifically DreamerV3 - we train visuomotor
policies capable of agile flight through a race track using only raw pixel
observations. While model-free RL methods such as PPO struggle to learn under
these conditions, DreamerV3 efficiently acquires complex visuomotor behaviors.
Moreover, because our policies learn directly from pixel inputs, the
perception-aware reward term employed in previous RL approaches to guide the
training process is no longer needed. Our experiments demonstrate in both
simulation and real-world flight how the proposed approach can be deployed on
agile quadrotors. This approach advances the frontier of vision-based
autonomous flight and shows that model-based RL is a promising direction for
real-world robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and
  3D Reconstruction from Noisy Video <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohao Xu, Tianyi Zhang, Shibo Zhao, Xiang Li, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Sebastian Scherer, Xiaonan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to redefine robust ego-motion estimation and photorealistic 3D
reconstruction by addressing a critical limitation: the reliance on noise-free
data in existing models. While such sanitized conditions simplify evaluation,
they fail to capture the unpredictable, noisy complexities of real-world
environments. Dynamic motion, sensor imperfections, and synchronization
perturbations lead to sharp performance declines when these models are deployed
in practice, revealing an urgent need for frameworks that embrace and excel
under real-world noise. To bridge this gap, we tackle three core challenges:
scalable data generation, comprehensive benchmarking, and model robustness
enhancement. First, we introduce a scalable noisy data synthesis pipeline that
generates diverse datasets simulating complex motion, sensor imperfections, and
synchronization errors. Second, we leverage this pipeline to create
Robust-Ego3D, a benchmark rigorously designed to expose noise-induced
performance degradation, highlighting the limitations of current learning-based
methods in ego-motion accuracy and 3D reconstruction quality. Third, we propose
Correspondence-guided Gaussian Splatting (CorrGS), a novel test-time adaptation
method that progressively refines an internal clean 3D representation by
aligning noisy observations with rendered RGB-D frames from clean 3D map,
enhancing geometric alignment and appearance restoration through visual
correspondence. Extensive experiments on synthetic and real-world data
demonstrate that CorrGS consistently outperforms prior state-of-the-art
methods, particularly in scenarios involving rapid motion and dynamic
illumination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025; 92 Pages; Project Repo:
  https://github.com/Xiaohao-Xu/SLAM-under-Perturbation. arXiv admin note:
  substantial text overlap with arXiv:2406.16850</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Robotic Precision in Construction: A Modular Factor
  Graph-Based Framework to Deflection and Backlash Compensation Using
  High-Accuracy Accelerometers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Kindle, Michael Loetscher, Andrea Alessandretti, Cesar Cadena, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate positioning is crucial in the construction industry, where labor
shortages highlight the need for automation. Robotic systems with long
kinematic chains are required to reach complex workspaces, including floors,
walls, and ceilings. These requirements significantly impact positioning
accuracy due to effects such as deflection and backlash in various parts along
the kinematic chain. In this work, we introduce a novel approach that
integrates deflection and backlash compensation models with high-accuracy
accelerometers, significantly enhancing position accuracy. Our method employs a
modular framework based on a factor graph formulation to estimate the state of
the kinematic chain, leveraging acceleration measurements to inform the model.
Extensive testing on publicly released datasets, reflecting real-world
construction disturbances, demonstrates the advantages of our approach. The
proposed method reduces the $95\%$ error threshold in the xy-plane by $50\%$
compared to the state-of-the-art Virtual Joint Method, and by $31\%$ when
incorporating base tilt compensation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, Accepted on November 2024 at IEEE Robotics and
  Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point-LN: A Lightweight Framework for Efficient Point Cloud
  Classification Using Non-Parametric Positional Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marzieh Mohammadi, Amir Salarpour, Pedram MohajerAnsari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Point-LN, a novel lightweight framework engineered for efficient
3D point cloud classification. Point-LN integrates essential non-parametric
components-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),
and non-learnable positional encoding-with a streamlined learnable classifier
that significantly enhances classification accuracy while maintaining a minimal
parameter footprint. This hybrid architecture ensures low computational costs
and rapid inference speeds, making Point-LN ideal for real-time and
resource-constrained applications. Comprehensive evaluations on benchmark
datasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN
achieves competitive performance compared to state-of-the-art methods, all
while offering exceptional efficiency. These results establish Point-LN as a
robust and scalable solution for diverse point cloud classification tasks,
highlighting its potential for widespread adoption in various computer vision
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at the 29th
  International Computer Conference, Computer Society of Iran (CSICC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from
  Video Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayi Zhou, Ruixiang Wang, Yunxin Tai, Yueci Deng, Guiliang Liu, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bimanual robotic manipulation is a long-standing challenge of embodied
intelligence due to its characteristics of dual-arm spatial-temporal
coordination and high-dimensional action spaces. Previous studies rely on
pre-defined action taxonomies or direct teleoperation to alleviate or
circumvent these issues, often making them lack simplicity, versatility and
scalability. Differently, we believe that the most effective and efficient way
for teaching bimanual manipulation is learning from human demonstrated videos,
where rich features such as spatial-temporal positions, dynamic postures,
interaction states and dexterous transitions are available almost for free. In
this work, we propose the YOTO (You Only Teach Once), which can extract and
then inject patterns of bimanual actions from as few as a single binocular
observation of hand movements, and teach dual robot arms various complex tasks.
Furthermore, based on keyframes-based motion trajectories, we devise a subtle
solution for rapidly generating training demonstrations with diverse variations
of manipulated objects and their locations. These data can then be used to
learn a customized bimanual diffusion policy (BiDP) across diverse scenes. In
experiments, YOTO achieves impressive performance in mimicking 5 intricate
long-horizon bimanual tasks, possesses strong generalization under different
visual and spatial conditions, and outperforms existing visuomotor imitation
learning methods in accuracy and efficiency. Our project link is
https://hnuzhy.github.io/projects/YOTO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaccoonBot: An Autonomous Wire-Traversing Solar-Tracking Robot for
  Persistent Environmental Monitoring <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Efrain Mendez-Flores, Agaton Pourshahidi, Magnus Egerstedt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental monitoring is used to characterize the health and relationship
between organisms and their environments. In forest ecosystems, robots can
serve as platforms to acquire such data, even in hard-to-reach places where
wire-traversing platforms are particularly promising due to their efficient
displacement. This paper presents the RaccoonBot, which is a novel autonomous
wire-traversing robot for persistent environmental monitoring, featuring a
fail-safe mechanical design with a self-locking mechanism in case of electrical
shortage. The robot also features energy-aware mobility through a novel Solar
tracking algorithm, that allows the robot to find a position on the wire to
have direct contact with solar power to increase the energy harvested.
Experimental results validate the electro-mechanical features of the
RaccoonBot, showing that it is able to handle wire perturbations, different
inclinations, and achieving energy autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print submitted to the 2025 IEEE International Conference on
  Robotics & Automation (ICRA 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Yu, Timothy Chen, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting offers expressive scene reconstruction, modeling a
broad range of visual, geometric, and semantic information. However, efficient
real-time map reconstruction with data streamed from multiple robots and
devices remains a challenge. To that end, we propose HAMMER, a server-based
collaborative Gaussian Splatting method that leverages widely available ROS
communication infrastructure to generate 3D, metric-semantic maps from
asynchronous robot data-streams with no prior knowledge of initial robot
positions and varying on-device pose estimators. HAMMER consists of (i) a frame
alignment module that transforms local SLAM poses and image data into a global
frame and requires no prior relative pose knowledge, and (ii) an online module
for training semantic 3DGS maps from streaming data. HAMMER handles mixed
perception modes, adjusts automatically for variations in image pre-processing
among different devices, and distills CLIP semantic codes into the 3D scene for
open-vocabulary language queries. In our real-world experiments, HAMMER creates
higher-fidelity maps (2x) compared to competing baselines and is useful for
downstream tasks, such as semantic goal-conditioned navigation (e.g., ``go to
the couch"). Accompanying content available at hammer-project.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Force-Based Robotic Imitation Learning: A Two-Phase Approach for
  Construction Assembly Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengxu You, Yang Ye, Tianyu Zhou, Jing Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The drive for efficiency and safety in construction has boosted the role of
robotics and automation. However, complex tasks like welding and pipe insertion
pose challenges due to their need for precise adaptive force control, which
complicates robotic training. This paper proposes a two-phase system to improve
robot learning, integrating human-derived force feedback. The first phase
captures real-time data from operators using a robot arm linked with a virtual
simulator via ROS-Sharp. In the second phase, this feedback is converted into
robotic motion instructions, using a generative approach to incorporate force
feedback into the learning process. This method's effectiveness is demonstrated
through improved task completion times and success rates. The framework
simulates realistic force-based interactions, enhancing the training data's
quality for precise robotic manipulation in construction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Binding Foundation Model for Material Property Recognition via
  Tactile Sequence Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengxu You, Tianyu Zhou, Jing Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots engaged in complex manipulation tasks require robust material property
recognition to ensure adaptability and precision. Traditionally, visual data
has been the primary source for object perception; however, it often proves
insufficient in scenarios where visibility is obstructed or detailed
observation is needed. This gap highlights the necessity of tactile sensing as
a complementary or primary input for material recognition. Tactile data becomes
particularly essential in contact-rich, small-scale manipulations where subtle
deformations and surface interactions cannot be accurately captured by vision
alone. This letter presents a novel approach leveraging a temporal binding
foundation model for tactile sequence understanding to enhance material
property recognition. By processing tactile sensor data with a temporal focus,
the proposed system captures the sequential nature of tactile interactions,
similar to human fingertip perception. Additionally, this letter demonstrates
that, through tailored and specific design, the foundation model can more
effectively capture temporal information embedded in tactile sequences,
advancing material property understanding. Experimental results validate the
model's capability to capture these temporal patterns, confirming its utility
for material property recognition in visually restricted scenarios. This work
underscores the necessity of embedding advanced tactile data processing
frameworks within robotic systems to achieve truly embodied and responsive
manipulation capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative
  Framework for Imitation Learning from Observation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Abhijit Diwan, Julen Urain, Jens Kober, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new imitation learning framework based on
energy-based generative models capable of learning complex, physics-dependent,
robot motion policies through state-only expert motion trajectories. Our
algorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR),
constructs several perturbed versions of the expert's motion data distribution
and learns smooth, and well-defined representations of the data distribution's
energy function using denoising score matching. We propose to use these learnt
energy functions as reward functions to learn imitation policies via
reinforcement learning. We also present a strategy to gradually switch between
the learnt energy functions, ensuring that the learnt rewards are always
well-defined in the manifold of policy-generated samples. We evaluate our
algorithm on complex humanoid tasks such as locomotion and martial arts and
compare it with state-only adversarial imitation learning algorithms like
Adversarial Motion Priors (AMP). Our framework sidesteps the optimisation
challenges of adversarial imitation learning techniques and produces results
comparable to AMP in several quantitative metrics across multiple imitation
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at the International Conference on
  Learning Representations (ICLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From One to the Power of Many: Invariance to Multi-LiDAR Perception from
  Single-Sensor <span class="highlight-title">Dataset</span>s <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Uecker, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, LiDAR segmentation methods for autonomous vehicles, powered by deep
neural networks, have experienced steep growth in performance on classic
benchmarks, such as nuScenes and SemanticKITTI. However, there are still large
gaps in performance when deploying models trained on such single-sensor setups
to modern vehicles with multiple high-resolution LiDAR sensors. In this work,
we introduce a new metric for feature-level invariance which can serve as a
proxy to measure cross-domain generalization without requiring labeled data.
Additionally, we propose two application-specific data augmentations, which
facilitate better transfer to multi-sensor LiDAR setups, when trained on
single-sensor datasets. We provide experimental evidence on both simulated and
real data, that our proposed augmentations improve invariance across LiDAR
setups, leading to improved generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the ML4AD Workshop @ AAAI Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon
  Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06605v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06605v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Jing Huo, Yangtao Chen, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient control in long-horizon robotic manipulation is challenging due to
complex representation and policy learning requirements. Model-based visual
reinforcement learning (RL) has shown great potential in addressing these
challenges but still faces notable limitations, particularly in handling sparse
rewards and complex visual features in long-horizon environments. To address
these limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for
long-horizon tasks and further introduce RoboHorizon, an LLM-assisted
multi-view world model tailored for long-horizon robotic manipulation. In
RoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage
sub-tasks based on task language instructions, enabling robots to better
recognize long-horizon tasks. Keyframe discovery is then integrated into the
multi-view masked autoencoder (MAE) architecture to enhance the robot's ability
to sense critical task sequences, strengthening its multi-stage perception of
long-horizon processes. Leveraging these dense rewards and multi-view
representations, a robotic world model is constructed to efficiently plan
long-horizon tasks, enabling the robot to reliably act through RL algorithms.
Experiments on two representative benchmarks, RLBench and FurnitureBench, show
that RoboHorizon outperforms state-of-the-art visual model-based RL methods,
achieving a 23.35% improvement in task success rates on RLBench's 4
short-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from
RLBench and 3 furniture assembly tasks from FurnitureBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S3PT: Scene Semantics and Structure Guided Clustering to Boost
  <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing for Autonomous Driving <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K. Wozniak, Hariprasath Govindarajan, Marvin Klingner, Camille Maurice, B Ravi Kiran, Senthil Yogamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-supervised clustering-based pre-training techniques like DINO and
Cribo have shown impressive results for downstream detection and segmentation
tasks. However, real-world applications such as autonomous driving face
challenges with imbalanced object class and size distributions and complex
scene geometries. In this paper, we propose S3PT a novel scene semantics and
structure guided clustering to provide more scene-consistent objectives for
self-supervised training. Specifically, our contributions are threefold: First,
we incorporate semantic distribution consistent clustering to encourage better
representation of rare classes such as motorcycles or animals. Second, we
introduce object diversity consistent spatial clustering, to handle imbalanced
and diverse object sizes, ranging from large background areas to small objects
such as pedestrians and traffic signs. Third, we propose a depth-guided spatial
clustering to regularize learning based on geometric information of the scene,
thus further refining region separation on the feature level. Our learned
representations significantly improve performance in downstream semantic
segmentation and 3D object detection tasks on the nuScenes, nuImages, and
Cityscapes datasets and show promising domain translation properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for WACV 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deadlock-free, Safe, and Decentralized Multi-Robot Navigation in Social
  Mini-Games via Discrete-Time Control Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10966v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10966v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Chandra, Vrushabh Zinage, Efstathios Bakolas, Peter Stone, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach to ensure safe and deadlock-free navigation for
decentralized multi-robot systems operating in constrained environments,
including doorways and intersections. Although many solutions have been
proposed that ensure safety and resolve deadlocks, optimally preventing
deadlocks in a minimally invasive and decentralized fashion remains an open
problem. We first formalize the objective as a non-cooperative,
non-communicative, partially observable multi-robot navigation problem in
constrained spaces with multiple conflicting agents, which we term as social
mini-games. Formally, we solve a discrete-time optimal receding horizon control
problem leveraging control barrier functions for safe long-horizon planning.
Our approach to ensuring liveness rests on the insight that \textit{there
exists barrier certificates that allow each robot to preemptively perturb their
state in a minimally-invasive fashion onto liveness sets i.e. states where
robots are deadlock-free}. We evaluate our approach in simulation as well on
physical robots using F$1/10$ robots, a Clearpath Jackal, as well as a Boston
Dynamics Spot in a doorway, hallway, and corridor intersection scenario.
Compared to both fully decentralized and centralized approaches with and
without deadlock resolution capabilities, we demonstrate that our approach
results in safer, more efficient, and smoother navigation, based on a
comprehensive set of metrics including success rate, collision rate, stop time,
change in velocity, path deviation, time-to-goal, and flow rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>major update since last revision</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-23T00:00:00Z">2025-01-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">33</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, Matt Feiszli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view 3D reconstruction remains a core challenge in computer vision,
particularly in applications requiring accurate and scalable representations
across diverse perspectives. Current leading methods such as DUSt3R employ a
fundamentally pairwise approach, processing images in pairs and necessitating
costly global alignment procedures to reconstruct from multiple views. In this
work, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view
generalization to DUSt3R that achieves efficient and scalable 3D reconstruction
by processing many views in parallel. Fast3R's Transformer-based architecture
forwards N images in a single forward pass, bypassing the need for iterative
alignment. Through extensive experiments on camera pose estimation and 3D
reconstruction, Fast3R demonstrates state-of-the-art performance, with
significant improvements in inference speed and reduced error accumulation.
These results establish Fast3R as a robust alternative for multi-view
applications, offering enhanced scalability without compromising reconstruction
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://fast3r-3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Preference Optimization for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAST-LIVO2 on Resource-Constrained Platforms: LiDAR-Inertial-Visual
  Odometry with Efficient Memory and Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyang Zhou, Chunran Zheng, Ziming Wang, Fangcheng Zhu, Yixi Cai, Fu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a lightweight LiDAR-inertial-visual odometry system
optimized for resource-constrained platforms. It integrates a
degeneration-aware adaptive visual frame selector into error-state iterated
Kalman filter (ESIKF) with sequential updates, improving computation efficiency
significantly while maintaining a similar level of robustness. Additionally, a
memory-efficient mapping structure combining a locally unified visual-LiDAR map
and a long-term visual map achieves a good trade-off between performance and
memory usage. Extensive experiments on x86 and ARM platforms demonstrate the
system's robustness and efficiency. On the Hilti dataset, our system achieves a
33% reduction in per-frame runtime and 47% lower memory usage compared to
FAST-LIVO2, with only a 3 cm increase in RMSE. Despite this slight accuracy
trade-off, our system remains competitive, outperforming state-of-the-art
(SOTA) LIO methods such as FAST-LIO2 and most existing LIVO systems. These
results validate the system's capability for scalable deployment on
resource-constrained edge computing platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First Lessons Learned of an Artificial Intelligence Robotic System for
  Autonomous Coarse Waste Recycling Using Multispectral Imaging-Based Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Lange, Ajish Babu, Philipp Meyer, Matthis Keppner, Tim Tiedemann, Martin Wittmaier, Sebastian Wolff, Thomas Vögele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current disposal facilities for coarse-grained waste perform manual sorting
of materials with heavy machinery. Large quantities of recyclable materials are
lost to coarse waste, so more effective sorting processes must be developed to
recover them. Two key aspects to automate the sorting process are object
detection with material classification in mixed piles of waste, and autonomous
control of hydraulic machinery. Because most objects in those accumulations of
waste are damaged or destroyed, object detection alone is not feasible in the
majority of cases. To address these challenges, we propose a classification of
materials with multispectral images of ultraviolet (UV), visual (VIS), near
infrared (NIR), and short-wave infrared (SWIR) spectrums. Solution for
autonomous control of hydraulic heavy machines for sorting of bulky waste is
being investigated using cost-effective cameras and artificial
intelligence-based controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of Sardinia 2023, 19th International
  Symposium on Waste Management, Resource Recovery and Sustainable Landfilling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Logic Guided Safe Navigation for Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Parameshwaran, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety verification for autonomous vehicles (AVs) and ground robots is
crucial for ensuring reliable operation given their uncertain environments.
Formal language tools provide a robust and sound method to verify safety rules
for such complex cyber-physical systems. In this paper, we propose a hybrid
approach that combines the strengths of formal verification languages like
Linear Temporal Logic (LTL) and Signal Temporal Logic (STL) to generate safe
trajectories and optimal control inputs for autonomous vehicle navigation. We
implement a symbolic path planning approach using LTL to generate a formally
safe reference trajectory. A mixed integer linear programming (MILP) solver is
then used on this reference trajectory to solve for the control inputs while
satisfying the state, control and safety constraints described by STL. We test
our proposed solution on two environments and compare the results with popular
path planning algorithms. In contrast to conventional path planning algorithms,
our formally safe solution excels in handling complex specification scenarios
while ensuring both safety and comparable computation times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, Modelling Estimation and Controls Conference-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-World Validation of a Physics-Based Ship Motion Prediction
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Mathioudakis, Christos Papandreou, Theodoros Stouraitis, Vicky Margari, Antonios Nikitakis, Stavros Paschalakis, Konstantinos Kyriakopoulos, Kostas J. Spyrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The maritime industry aims towards a sustainable future, which requires
significant improvements in operational efficiency. Current approaches focus on
minimising fuel consumption and emissions through greater autonomy. Efficient
and safe autonomous navigation requires high-fidelity ship motion models
applicable to real-world conditions. Although physics-based ship motion models
can predict ships' motion with sub-second resolution, their validation in
real-world conditions is rarely found in the literature. This study presents a
physics-based 3D dynamics motion model that is tailored to a container-ship,
and compares its predictions against real-world voyages. The model integrates
vessel motion over time and accounts for its hydrodynamic behavior under
different environmental conditions. The model's predictions are evaluated
against real vessel data both visually and using multiple distance measures.
Both methodologies demonstrate that the model's predictions align closely with
the real-world trajectories of the container-ship.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Only Crash Once v2: Perceptually Consistent Strong Features for
  One-Stage Domain Adaptive Detection of Space Terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Chase Jr, Christopher Wilson, Karthik Dantu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The in-situ detection of planetary, lunar, and small-body surface terrain is
crucial for autonomous spacecraft applications, where learning-based computer
vision methods are increasingly employed to enable intelligence without prior
information or human intervention. However, many of these methods remain
computationally expensive for spacecraft processors and prevent real-time
operation. Training of such algorithms is additionally complex due to the
scarcity of labeled data and reliance on supervised learning approaches.
Unsupervised Domain Adaptation (UDA) offers a promising solution by
facilitating model training with disparate data sources such as simulations or
synthetic scenes, although UDA is difficult to apply to celestial environments
where challenging feature spaces are paramount. To alleviate such issues, You
Only Crash Once (YOCOv1) has studied the integration of Visual Similarity-based
Alignment (VSA) into lightweight one-stage object detection architectures to
improve space terrain UDA. Although proven effective, the approach faces
notable limitations, including performance degradations in multi-class and
high-altitude scenarios. Building upon the foundation of YOCOv1, we propose
novel additions to the VSA scheme that enhance terrain detection capabilities
under UDA, and our approach is evaluated across both simulated and real-world
data. Our second YOCO rendition, YOCOv2, is capable of achieving
state-of-the-art UDA performance on surface terrain detection, where we
showcase improvements upwards of 31% compared with YOCOv1 and terrestrial
state-of-the-art. We demonstrate the practical utility of YOCOv2 with
spacecraft flight hardware performance benchmarking and qualitative evaluation
of NASA mission data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Road to Learning Explainable Inverse Kinematic Models: Graph Neural
  Networks as Inductive Bias for Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pravin Pandey, Julia Reuter, Christoph Steup, Sanaz Mostaghim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shows how a Graph Neural Network (GNN) can be used to learn an
Inverse Kinematics (IK) based on an automatically generated dataset. The
generated Inverse Kinematics is generalized to a family of manipulators with
the same Degree of Freedom (DOF), but varying link length configurations. The
results indicate a position error of less than 1.0 cm for 3 DOF and 4.5 cm for
5 DOF, and orientation error of 2$^\circ$ for 3 DOF and 8.2$^\circ$ for 6 DOF,
which allows the deployment to certain real world-problems. However,
out-of-domain errors and lack of extrapolation can be observed in the resulting
GNN. An extensive analysis of these errors indicates potential for enhancement
in the future. Consequently, the generated GNNs are tailored to be used in
future work as an inductive bias to generate analytical equations through
symbolic regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Shaping of Multi-Particle Aggregates based on Action Trees and
  VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoi-Yin Lee, Peng Zhou, Anqing Duan, Chenguang Yang, David Navarro-Alarcon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the problem of manipulating multi-particle
aggregates using a bimanual robotic system. Our approach enables the autonomous
transport of dispersed particles through a series of shaping and pushing
actions using robotically-controlled tools. Achieving this advanced
manipulation capability presents two key challenges: high-level task planning
and trajectory execution. For task planning, we leverage Vision Language Models
(VLMs) to enable primitive actions such as tool affordance grasping and
non-prehensile particle pushing. For trajectory execution, we represent the
evolving particle aggregate's contour using truncated Fourier series, providing
efficient parametrization of its closed shape. We adaptively compute trajectory
waypoints based on group cohesion and the geometric centroid of the aggregate,
accounting for its spatial distribution and collective motion. Through
real-world experiments, we demonstrate the effectiveness of our methodology in
actively shaping and manipulating multi-particle aggregates while maintaining
high system cohesion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Informed Multi-Agent Trajectory Prediction at Signalized
  Intersections for Infrastructure-to-Everything 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huilin Yin, Yangwenhui Xu, Jiaxiang Li, Hao Zhang, Gerhard Rigoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent trajectory prediction at signalized intersections is crucial for
developing efficient intelligent transportation systems and safe autonomous
driving systems. Due to the complexity of intersection scenarios and the
limitations of single-vehicle perception, the performance of vehicle-centric
prediction methods has reached a plateau. Furthermore, most works underutilize
critical intersection information, including traffic signals, and behavior
patterns induced by road structures. Therefore, we propose a multi-agent
trajectory prediction framework at signalized intersections dedicated to
Infrastructure-to-Everything (I2XTraj). Our framework leverages dynamic graph
attention to integrate knowledge from traffic signals and driving behaviors. A
continuous signal-informed mechanism is proposed to adaptively process
real-time traffic signals from infrastructure devices. Additionally, leveraging
the prior knowledge of the intersection topology, we propose a driving strategy
awareness mechanism to model the joint distribution of goal intentions and
maneuvers. To the best of our knowledge, I2XTraj represents the first
multi-agent trajectory prediction framework explicitly designed for
infrastructure deployment, supplying subscribable prediction services to all
vehicles at intersections. I2XTraj demonstrates state-of-the-art performance on
both the Vehicle-to-Infrastructure dataset V2X-Seq and the aerial-view dataset
SinD for signalized intersections. Quantitative evaluations show that our
approach outperforms existing methods by more than 30% in both multi-agent and
single-agent scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijia Liu, Ancheng Hou, Xiao Yu, Xiang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Signal Temporal Logic (STL) is a powerful specification language for
describing complex temporal behaviors of continuous signals, making it
well-suited for high-level robotic task descriptions. However, generating
executable plans for STL tasks is challenging, as it requires consideration of
the coupling between the task specification and the system dynamics. Existing
approaches either follow a model-based setting that explicitly requires
knowledge of the system dynamics or adopt a task-oriented data-driven approach
to learn plans for specific tasks. In this work, we investigate the problem of
generating executable STL plans for systems whose dynamics are unknown a
priori. We propose a new planning framework that uses only task-agnostic data
during the offline training stage, enabling zero-shot generalization to new STL
tasks. Our framework is hierarchical, involving: (i) decomposing the STL task
into a set of progress and time constraints, (ii) searching for time-aware
waypoints guided by task-agnostic data, and (iii) generating trajectories using
a pre-trained safe diffusion model. Simulation results demonstrate the
effectiveness of our method indeed in achieving zero-shot generalization to
various STL tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion estimation from video footage with LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samer Attrah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion estimation in general is a field that has been studied for a long
time, and several approaches exist using machine learning. in this paper, we
present an LSTM model, that processes the blend-shapes produced by the library
MediaPipe, for a face detected in a live stream of a camera, to estimate the
main emotion from the facial expressions, this model is trained on the FER2013
dataset and delivers a result of 71% accuracy and 62% f1-score which meets the
accuracy benchmark of the FER2013 dataset, with significantly reduced
computation costs. https://github.com/
Samir-atra/Emotion_estimation_from_video_footage_with_LSTM_ML_algorithm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 32 references, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeomGS: LiDAR-Guided Geometry-Aware Gaussian Splatting for Robot
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewon Lee, Mangyu Kong, Minseong Park, Euntai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping and localization are crucial problems in robotics and autonomous
driving. Recent advances in 3D Gaussian Splatting (3DGS) have enabled precise
3D mapping and scene understanding by rendering photo-realistic images.
However, existing 3DGS methods often struggle to accurately reconstruct a 3D
map that reflects the actual scale and geometry of the real world, which
degrades localization performance. To address these limitations, we propose a
novel 3DGS method called Geometry-Aware Gaussian Splatting (GeomGS). This
method fully integrates LiDAR data into 3D Gaussian primitives via a
probabilistic approach, as opposed to approaches that only use LiDAR as initial
points or introduce simple constraints for Gaussian points. To this end, we
introduce a Geometric Confidence Score (GCS), which identifies the structural
reliability of each Gaussian point. The GCS is optimized simultaneously with
Gaussians under probabilistic distance constraints to construct a precise
structure. Furthermore, we propose a novel localization method that fully
utilizes both the geometric and photometric properties of GeomGS. Our GeomGS
demonstrates state-of-the-art geometric and localization performance across
several benchmarks, while also improving photometric performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3PT: A <span class="highlight-title">Transformer</span> for Multimodal, Multi-Party Social Signal Prediction
  with Person-aware Blockwise Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Tang, Abrar Anwar, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding social signals in multi-party conversations is important for
human-robot interaction and artificial social intelligence. Multi-party
interactions include social signals like body pose, head pose, speech, and
context-specific activities like acquiring and taking bites of food when
dining. Incorporating all the multimodal signals in a multi-party interaction
is difficult, and past work tends to build task-specific models for predicting
social signals. In this work, we address the challenge of predicting multimodal
social signals in multi-party settings in a single model. We introduce M3PT, a
causal transformer architecture with modality and temporal blockwise attention
masking which allows for the simultaneous processing of multiple social cues
across multiple participants and their temporal interactions. This approach
better captures social dynamics over time by considering longer horizons of
social signals between individuals. We train and evaluate our unified model on
the Human-Human Commensality Dataset (HHCD), and demonstrate that using
multiple modalities improves bite timing and speaking status prediction. Source
code: https://github.com/AbrarAnwar/masked-social-signals/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VIGS SLAM: IMU-based Large-Scale 3D Gaussian Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuhyeon Pak, Euntai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, map representations based on radiance fields such as 3D Gaussian
Splatting and NeRF, which excellent for realistic depiction, have attracted
considerable attention, leading to attempts to combine them with SLAM. While
these approaches can build highly realistic maps, large-scale SLAM still
remains a challenge because they require a large number of Gaussian images for
mapping and adjacent images as keyframes for tracking. We propose a novel 3D
Gaussian Splatting SLAM method, VIGS SLAM, that utilizes sensor fusion of RGB-D
and IMU sensors for large-scale indoor environments. To reduce the
computational load of 3DGS-based tracking, we adopt an ICP-based tracking
framework that combines IMU preintegration to provide a good initial guess for
accurate pose estimation. Our proposed method is the first to propose that
Gaussian Splatting-based SLAM can be effectively performed in large-scale
environments by integrating IMU sensor measurements. This proposal not only
enhances the performance of Gaussian Splatting SLAM beyond room-scale scenarios
but also achieves SLAM performance comparable to state-of-the-art methods in
large-scale indoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CuriousBot: Interactive Mobile Exploration via Actionable 3D Relational
  Object Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Leonor Fermoselle, Tarik Kelestemur, Jiuguang Wang, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile exploration is a longstanding challenge in robotics, yet current
methods primarily focus on active perception instead of active interaction,
limiting the robot's ability to interact with and fully explore its
environment. Existing robotic exploration approaches via active interaction are
often restricted to tabletop scenes, neglecting the unique challenges posed by
mobile exploration, such as large exploration spaces, complex action spaces,
and diverse object relations. In this work, we introduce a 3D relational object
graph that encodes diverse object relations and enables exploration through
active interaction. We develop a system based on this representation and
evaluate it across diverse scenes. Our qualitative and quantitative results
demonstrate the system's effectiveness and generalization capabilities,
outperforming methods that rely solely on vision-language models (VLMs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://curiousbot.theaiinstitute.com/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Perceived Danger (PD) Scale: Development and Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaclyn Molan, Laura Saad, Eileen Roesler, J. Malcolm McCurry, Nathaniel Gyory, J. Gregory Trafton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are currently no psychometrically valid tools to measure the perceived
danger of robots. To fill this gap, we provided a definition of perceived
danger and developed and validated a 12-item bifactor scale through four
studies. An exploratory factor analysis revealed four subdimensions of
perceived danger: affective states, physical vulnerability, ominousness, and
cognitive readiness. A confirmatory factor analysis confirmed the bifactor
model. We then compared the perceived danger scale to the Godspeed perceived
safety scale and found that the perceived danger scale is a better predictor of
empirical data. We also validated the scale in an in-person setting and found
that the perceived danger scale is sensitive to robot speed manipulations,
consistent with previous empirical findings. Results across experiments suggest
that the perceived danger scale is reliable, valid, and an adequate predictor
of both perceived safety and perceived danger in human-robot interaction
contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, to be published in the Proceedings of the 2025
  ACM/IEEE International Conference on Human-Robot Interaction (HRI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Persian Lip Reading in Surena-V Humanoid Robot for
  Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Farshian Abbasi, Aghil Yousefi-Koma, Soheil Dehghani Firouzabadi, Parisa Rashidi, Alireza Naeini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lip reading is vital for robots in social settings, improving their ability
to understand human communication. This skill allows them to communicate more
easily in crowded environments, especially in caregiving and customer service
roles. Generating a Persian Lip-reading dataset, this study integrates Persian
lip-reading technology into the Surena-V humanoid robot to improve its speech
recognition capabilities. Two complementary methods are explored, an indirect
method using facial landmark tracking and a direct method leveraging
convolutional neural networks (CNNs) and long short-term memory (LSTM)
networks. The indirect method focuses on tracking key facial landmarks,
especially around the lips, to infer movements, while the direct method
processes raw video data for action and speech recognition. The best-performing
model, LSTM, achieved 89\% accuracy and has been successfully implemented into
the Surena-V robot for real-time human-robot interaction. The study highlights
the effectiveness of these methods, particularly in environments where verbal
communication is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSAOT: Cooperative Multi-Agent System for Active Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hy Nguyen, Bao Pham, Hung Du, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Tracking is essential for many computer vision applications, such as
autonomous navigation, surveillance, and robotics. Unlike Passive Object
Tracking (POT), which relies on static camera viewpoints to detect and track
objects across consecutive frames, Active Object Tracking (AOT) requires a
controller agent to actively adjust its viewpoint to maintain visual contact
with a moving target in complex environments. Existing AOT solutions are
predominantly single-agent-based, which struggle in dynamic and complex
scenarios due to limited information gathering and processing capabilities,
often resulting in suboptimal decision-making. Alleviating these limitations
necessitates the development of a multi-agent system where different agents
perform distinct roles and collaborate to enhance learning and robustness in
dynamic and complex environments. Although some multi-agent approaches exist
for AOT, they typically rely on external auxiliary agents, which require
additional devices, making them costly. In contrast, we introduce the
Collaborative System for Active Object Tracking (CSAOT), a method that
leverages multi-agent deep reinforcement learning (MADRL) and a Mixture of
Experts (MoE) framework to enable multiple agents to operate on a single
device, thereby improving tracking performance and reducing costs. Our approach
enhances robustness against occlusions and rapid motion while optimizing camera
movements to extend tracking duration. We validated the effectiveness of CSAOT
on various interactive maps with dynamic and stationary obstacles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCRL4OR: Multimodal Contrastive Representation Learning for Off-Road
  Environmental Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yang, Zhang Zhang, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most studies on environmental perception for autonomous vehicles (AVs) focus
on urban traffic environments, where the objects/stuff to be perceived are
mainly from man-made scenes and scalable datasets with dense annotations can be
used to train supervised learning models. By contrast, it is hard to densely
annotate a large-scale off-road driving dataset manually due to the inherently
unstructured nature of off-road environments. In this paper, we propose a
Multimodal Contrastive Representation Learning approach for Off-Road
environmental perception, namely MCRL4OR. This approach aims to jointly learn
three encoders for processing visual images, locomotion states, and control
actions by aligning the locomotion states with the fused features of visual
images and control actions within a contrastive learning framework. The
causation behind this alignment strategy is that the inertial locomotion state
is the result of taking a certain control action under the current
landform/terrain condition perceived by visual sensors. In experiments, we
pre-train the MCRL4OR with a large-scale off-road driving dataset and adopt the
learned multimodal representations for various downstream perception tasks in
off-road driving scenarios. The superior performance in downstream tasks
demonstrates the advantages of the pre-trained multimodal representations. The
codes can be found in \url{https://github.com/1uciusy/MCRL4OR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github repository: https://github.com/1uciusy/MCRL4OR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Evaluation of a Transformable CubeSat for Nonholonomic Attitude
  Reorientation Using a Drop Tower 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Kubo, Tsubasa Ando, Hirona Kawahara, Shu Miyata, Naoya Uchiyama, Kazutoshi Ito, Yoshiki Sugawara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a design for a drop tower test to evaluate a numerical
model for a structurally reconfigurable spacecraft with actuatable joints,
referred to as a transformable spacecraft. A mock-up robot for a 3U-sized
transformable spacecraft is designed to fit in a limited time and space of the
microgravity environment available in the drop tower. The robot performs agile
reorientation, referred to as nonholonomic attitude control, by actuating
joints in a particular manner. To adapt to the very short duration of
microgravity in the drop tower test, a successive joint actuation maneuver is
optimized to maximize the amount of attitude reorientation within the time
constraint. The robot records the angular velocity history of all four bodies,
and the data is analyzed to evaluate the accuracy of the numerical model. We
confirm that the constructed numerical model sufficiently replicates the
robot's motion and show that the post-experiment model corrections further
improve the accuracy of the numerical simulations. Finally, the difference
between this drop tower test and the actual orbit demonstration is discussed to
show the prospect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards spiking analog hardware implementation of a trajectory
  interpolation mechanism for smooth closed-loop control of a spiking robot arm <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Casanueva-Morato, Chenxi Wu, Giacomo Indiveri, Juan P. Dominguez-Morales, Alejandro Linares-Barranco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic engineering aims to incorporate the computational principles
found in animal brains, into modern technological systems. Following this
approach, in this work we propose a closed-loop neuromorphic control system for
an event-based robotic arm. The proposed system consists of a shifted
Winner-Take-All spiking network for interpolating a reference trajectory and a
spiking comparator network responsible for controlling the flow continuity of
the trajectory, which is fed back to the actual position of the robot. The
comparator model is based on a differential position comparison neural network,
which governs the execution of the next trajectory points to close the control
loop between both components of the system. To evaluate the system, we
implemented and deployed the model on a mixed-signal analog-digital
neuromorphic platform, the DYNAP-SE2, to facilitate integration and
communication with the ED-Scorbot robotic arm platform. Experimental results on
one joint of the robot validate the use of this architecture and pave the way
for future neuro-inspired control of the entire robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, conference, ISCAS 2025, accepted for publication,
  Spiking Neural Network</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entanglement Definitions for Tethered Robots: Exploration and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianpietro Battocletti, Dimitris Boskos, Domagoj Tolić, Ivana Palunko, Bart De Schutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we consider the problem of tether entanglement for tethered
mobile robots. One of the main risks of using a tethered connection between a
mobile robot and an anchor point is that the tether may get entangled with the
obstacles present in the environment or with itself. To avoid these situations,
a non-entanglement constraint can be considered in the motion planning problem
for tethered robots. This constraint is typically expressed as a set of
specific tether configurations that must be avoided. However, the literature
lacks a generally accepted definition of entanglement, with existing
definitions being limited and partial in the sense that they only focus on
specific instances of entanglement. In practice, this means that the existing
definitions do not effectively cover all instances of tether entanglement. Our
goal in this article is to bridge this gap and to provide new definitions of
entanglement, which, together with the existing ones, can be effectively used
to qualify the entanglement state of a tethered robot in diverse situations.
The new definitions find application in motion planning for tethered robots,
where they can be used to obtain more safe and robust entanglement-free
trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures. Published on IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrojanRobot: Physical-World Backdoor Attacks Against VLM-based Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11683v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11683v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianlong Wang, Hewen Pan, Hangtao Zhang, Minghui Li, Shengshan Hu, Ziqi Zhou, Lulu Xue, Peijin Guo, Yichen Wang, Wei Wan, Aishan Liu, Leo Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation in the physical world is increasingly empowered by
\textit{large language models} (LLMs) and \textit{vision-language models}
(VLMs), leveraging their understanding and perception capabilities. Recently,
various attacks against such robotic policies have been proposed, with backdoor
attacks drawing considerable attention for their high stealth and strong
persistence capabilities. However, existing backdoor efforts are limited to
simulators and suffer from physical-world realization. To address this, we
propose \textit{TrojanRobot}, a highly stealthy and broadly effective robotic
backdoor attack in the physical world. Specifically, we introduce a
module-poisoning approach by embedding a backdoor module into the modular
robotic policy, enabling backdoor control over the policy's visual perception
module thereby backdooring the entire robotic policy. Our vanilla
implementation leverages a backdoor-finetuned VLM to serve as the backdoor
module. To enhance its generalization in physical environments, we propose a
prime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing
three types of prime attacks, \ie, \textit{permutation}, \textit{stagnation},
and \textit{intentional} attacks, thus achieving finer-grained backdoors.
Extensive experiments on the UR3e manipulator with 18 task instructions using
robotic policies based on four VLMs demonstrate the broad effectiveness and
physical-world stealth of TrojanRobot. Our attack's video demonstrations are
available via a github link \url{https://trojanrobot.github.io}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOB-Net: Limb-modularized Uncertainty Torque Learning of Humanoids for
  Sensorless External Torque Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daegyu Lim, Myeong-Ju Kim, Junhyeok Cha, Jaeheung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Momentum observer (MOB) can estimate external joint torque without requiring
additional sensors, such as force/torque or joint torque sensors. However, the
estimation performance of MOB deteriorates due to the model uncertainty which
encompasses the modeling errors and the joint friction. Moreover, the
estimation error is significant when MOB is applied to high-dimensional
floating-base humanoids, which prevents the estimated external joint torque
from being used for force control or collision detection in the real humanoid
robot. In this paper, the pure external joint torque estimation method named
MOB-Net, is proposed for humanoids. MOB-Net learns the model uncertainty torque
and calibrates the estimated signal of MOB. The external joint torque can be
estimated in the generalized coordinate including whole-body and virtual joints
of the floating-base robot with only internal sensors (an IMU on the pelvis and
encoders in the joints). Our method substantially reduces the estimation errors
of MOB, and the robust performance of MOB-Net for the unseen data is validated
through extensive simulations, real robot experiments, and ablation studies.
Finally, various collision handling scenarios are presented using the estimated
external joint torque from MOB-Net: contact wrench feedback control for
locomotion, collision detection, and collision reaction for safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published to IJRR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid
  Prototyping in Virtual Reality Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09600v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09600v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Augusto Pinheiro de Sousa, Heiko Hamann, Oliver Deussen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLAM is a foundational technique with broad applications in robotics and
AR/VR. SLAM simulations evaluate new concepts, but testing on
resource-constrained devices, such as VR HMDs, faces challenges: high
computational cost and restricted sensor data access. This work proposes a
sparse framework using mesh geometry projections as features, which improves
efficiency and circumvents direct sensor data access, advancing SLAM research
as we demonstrate in VR and through numerical evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ENPT XR at IEEE VR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design Optimizer for Soft Growing Robot Manipulators in
  Three-Dimensional Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00368v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00368v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Astar, Ozan Nurcan, Erk Demirel, Emir Ozen, Ozan Kutlar, Fabio Stroppa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft growing robots are novel devices that mimic plant-like growth for
navigation in cluttered or dangerous environments. Their ability to adapt to
surroundings, combined with advancements in actuation and manufacturing
technologies, allows them to perform specialized manipulation tasks. This work
presents an approach for design optimization of soft growing robots;
specifically, the three-dimensional extension of the optimizer designed for
planar manipulators. This tool is intended to be used by engineers and robot
enthusiasts before manufacturing their robot: it suggests the optimal size of
the robot for solving a specific task. The design process models a
multi-objective optimization problem to refine a soft manipulator's kinematic
chain. Thanks to the novel Rank Partitioning algorithm integrated into
Evolutionary Computation (EC) algorithms, this method achieves high precision
in reaching targets and is efficient in resource usage. Results show
significantly high performance in solving three-dimensional tasks, whereas
comparative experiments indicate that the optimizer features robust output when
tested with different EC algorithms, particularly genetic algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Graphical Inverse Kinematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08812v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08812v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Filip Marić, Matthew Giamou, Petra Alexson, Ivan Petrović, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly and reliably finding accurate inverse kinematics (IK) solutions
remains a challenging problem for many robot manipulators. Existing numerical
solvers are broadly applicable but typically only produce a single solution and
rely on local search techniques to minimize nonconvex objective functions. More
recent learning-based approaches that approximate the entire feasible set of
solutions have shown promise as a means to generate multiple fast and accurate
IK results in parallel. However, existing learning-based techniques have a
significant drawback: each robot of interest requires a specialized model that
must be trained from scratch. To address this key shortcoming, we propose a
novel distance-geometric robot representation coupled with a graph structure
that allows us to leverage the sample efficiency of Euclidean equivariant
functions and the generalizability of graph neural networks (GNNs). Our
approach is generative graphical inverse kinematics (GGIK), the first learned
IK solver able to accurately and efficiently produce a large number of diverse
solutions in parallel while also displaying the ability to generalize -- a
single learned model can be used to produce IK solutions for a variety of
different robots. When compared to several other learned IK methods, GGIK
provides more accurate solutions with the same amount of data. GGIK can
generalize reasonably well to robot manipulators unseen during training.
Additionally, GGIK can learn a constrained distribution that encodes joint
limits and scales efficiently to larger robots and a high number of sampled
solutions. Finally, GGIK can be used to complement local IK solvers by
providing reliable initializations for a local optimization process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaWM: Adaptive World Model based Planning for Autonomous Driving <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Wang, Xin Ye, Feng Tao, Chenbin Pan, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, Junshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World model based reinforcement learning (RL) has emerged as a promising
approach for autonomous driving, which learns a latent dynamics model and uses
it to train a planning policy. To speed up the learning process, the
pretrain-finetune paradigm is often used, where online RL is initialized by a
pretrained model and a policy learned offline. However, naively performing such
initialization in RL may result in dramatic performance degradation during the
online interactions in the new task. To tackle this challenge, we first analyze
the performance degradation and identify two primary root causes therein: the
mismatch of the planning policy and the mismatch of the dynamics model, due to
distribution shift. We further analyze the effects of these factors on
performance degradation during finetuning, and our findings reveal that the
choice of finetuning strategies plays a pivotal role in mitigating these
effects. We then introduce AdaWM, an Adaptive World Model based planning
method, featuring two key steps: (a) mismatch identification, which quantifies
the mismatches and informs the finetuning strategy, and (b) alignment-driven
finetuning, which selectively updates either the policy or the model as needed
using efficient low-rank updates. Extensive experiments on the challenging
CARLA driving tasks demonstrate that AdaWM significantly improves the
finetuning process, resulting in more robust and efficient performance in
autonomous driving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Optimality-Aware Stochastic LiDAR Bundle Adjustment with
  Progressive Spatial Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianping Li, Thien-Minh Nguyen, Muqing Cao, Shenghai Yuan, Tzu-Yi Hung, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale LiDAR Bundle Adjustment (LBA) to refine sensor orientation and
point cloud accuracy simultaneously to build the navigation map is a
fundamental task in logistics and robotics. Unlike pose-graph-based methods
that rely solely on pairwise relationships between LiDAR frames, LBA leverages
raw LiDAR correspondences to achieve more precise results, especially when
initial pose estimates are unreliable for low-cost sensors. However, existing
LBA methods face challenges such as simplistic planar correspondences,
extensive observations, and dense normal matrices in the least-squares problem,
which limit robustness, efficiency, and scalability. To address these issues,
we propose a Graph Optimality-aware Stochastic Optimization scheme with
Progressive Spatial Smoothing, namely PSS-GOSO, to achieve \textit{robust},
\textit{efficient}, and \textit{scalable} LBA. The Progressive Spatial
Smoothing (PSS) module extracts \textit{robust} LiDAR feature association
exploiting the prior structure information obtained by the polynomial smooth
kernel. The Graph Optimality-aware Stochastic Optimization (GOSO) module first
sparsifies the graph according to optimality for an \textit{efficient}
optimization. GOSO then utilizes stochastic clustering and graph
marginalization to solve the large-scale state estimation problem for a
\textit{scalable} LBA. We validate PSS-GOSO across diverse scenes captured by
various platforms, demonstrating its superior performance compared to existing
methods. Moreover, the resulting point cloud maps are used for automatic
last-mile delivery in large-scale complex scenes. The project page can be found
at: \url{https://kafeiyin00.github.io/PSS-GOSO/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and
  Chain-of-Thought for Embodied Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, Helong Huang, Guangjian Tian, Weichao Qiu, Xingyue Quan, Jianye Hao, Yuzheng Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning is an essential problem in embodied AI research. Efforts to
enhance spatial reasoning abilities through supplementary spatial data and
fine-tuning have proven limited and ineffective when addressing complex
embodied tasks, largely due to their dependence on language-based outputs.
While some approaches have introduced a point-based action space to mitigate
this issue, they fall short in managing more intricate tasks within complex
environments. This deficiency arises from their failure to fully exploit the
inherent thinking and reasoning capabilities that are fundamental strengths of
Vision-Language Models (VLMs). To address these limitations, we propose a novel
approach named SpatialCoT, specifically designed to bolster the spatial
reasoning capabilities of VLMs. Our approach comprises two stages: spatial
coordinate bi-directional alignment, which aligns vision-language inputs with
spatial coordinates, and chain-of-thought spatial grounding, which harnesses
the reasoning capabilities of language models for advanced spatial reasoning.
We evaluate SpatialCoT on challenging navigation and manipulation tasks, both
in simulation and real-world settings. Experimental results demonstrate that
our method significantly outperforms previous state-of-the-art approaches in
both tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POLAR-Sim: Augmenting NASA's POLAR <span class="highlight-title">Dataset</span> for Data-Driven Lunar
  Perception and Rover Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Hsun Chen, Peter Negrut, Thomas Liang, Nevindu Batagoda, Harry Zhang, Dan Negrut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NASA's POLAR dataset contains approximately 2,600 pairs of high dynamic range
stereo photos captured across 13 varied terrain scenarios, including areas with
sparse or dense rock distributions, craters, and rocks of different sizes. The
purpose of these photos is to spur development in robotics, AI-based
perception, and autonomous navigation. Acknowledging a scarcity of lunar images
from around the lunar poles, NASA Ames produced on Earth but in controlled
conditions images that resemble rover operating conditions from these regions
of the Moon. We report on the outcomes of an effort aimed at accomplishing two
tasks. In Task 1, we provided bounding boxes and semantic segmentation
information for all the images in NASA's POLAR dataset. This effort resulted in
23,000 labels and semantic segmentation annotations pertaining to rocks,
shadows, and craters. In Task 2, we generated the digital twins of the 13
scenarios that have been used to produce all the photos in the POLAR dataset.
Specifically, for each of these scenarios, we produced individual meshes,
texture information, and material properties associated with the ground and the
rocks in each scenario. This allows anyone with a camera model to synthesize
images associated with any of the 13 scenarios of the POLAR dataset.
Effectively, one can generate as many semantically labeled synthetic images as
desired -- with different locations and exposure values in the scene, for
different positions of the sun, with or without the presence of active
illumination, etc. The benefit of this work is twofold. Using outcomes of Task
1, one can train and/or test perception algorithms that deal with Moon images.
For Task 2, one can produce as much data as desired to train and test AI
algorithms that are anticipated to work in lunar conditions. All the outcomes
of this work are available in a public repository for unfettered use and
distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design and Implementation of an Efficient Onboard Computer System for
  CanSat Atmosphere Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Gadekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advancements in technology, the smaller versions of satellites have
gained momentum in the space industry for earth monitoring and
communication-based applications. The rise of CanSat technology has
significantly impacted the space industry by providing a cost-effective
solution for space exploration. CanSat is a simulation model of a real
satellite and plays a crucial role in collecting and transmitting atmospheric
data. This paper discusses the design of an Onboard Computer System forCanSat,
used to study various environmental parameters by monitoring the concentrations
of gases in the atmosphere. The Onboard Computer System uses GPS,
accelerometer, altitude, temperature, pressure, gyroscope, magnetometer, UV
radiation, and air quality sensors for atmospheric sensing. A highly efficient
and low-power ESP32 microcontroller and a transceiver module are used to
acquire data, facilitate seamless communication and transmit the collected data
to the ground station.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-22T00:00:00Z">2025-01-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaWM: Adaptive World Model based Planning for Autonomous Driving <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Wang, Xin Ye, Feng Tao, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, Junshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World model based reinforcement learning (RL) has emerged as a promising
approach for autonomous driving, which learns a latent dynamics model and uses
it to train a planning policy. To speed up the learning process, the
pretrain-finetune paradigm is often used, where online RL is initialized by a
pretrained model and a policy learned offline. However, naively performing such
initialization in RL may result in dramatic performance degradation during the
online interactions in the new task. To tackle this challenge, we first analyze
the performance degradation and identify two primary root causes therein: the
mismatch of the planning policy and the mismatch of the dynamics model, due to
distribution shift. We further analyze the effects of these factors on
performance degradation during finetuning, and our findings reveal that the
choice of finetuning strategies plays a pivotal role in mitigating these
effects. We then introduce AdaWM, an Adaptive World Model based planning
method, featuring two key steps: (a) mismatch identification, which quantifies
the mismatches and informs the finetuning strategy, and (b) alignment-driven
finetuning, which selectively updates either the policy or the model as needed
using efficient low-rank updates. Extensive experiments on the challenging
CARLA driving tasks demonstrate that AdaWM significantly improves the
finetuning process, resulting in more robust and efficient performance in
autonomous driving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drone Carrier: An Integrated Unmanned Surface Vehicle for Autonomous
  Inspection and Intervention in GNSS-Denied Maritime Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Dong, Muhayyu Ud Din, Francesco Lagala, Hailiang Kuang, Jianjun Sun, Siyuan Yang, Irfan Hussain, Shaoming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative drone carrier concept that is applied in
maritime port security or offshore rescue. This system works with a
heterogeneous system consisting of multiple Unmanned Aerial Vehicles (UAVs) and
Unmanned Surface Vehicles (USVs) to perform inspection and intervention tasks
in GNSS-denied or interrupted environments. The carrier, an electric catamaran
measuring 4m by 7m, features a 4m by 6m deck supporting automated takeoff and
landing for four DJI M300 drones, along with a 10kg-payload manipulator
operable in up to level 3 sea conditions. Utilizing an offshore gimbal camera
for navigation, the carrier can autonomously navigate, approach and dock with
non-cooperative vessels, guided by an onboard camera, LiDAR, and Doppler
Velocity Log (DVL) over a 3 km$^2$ area. UAVs equipped with onboard
Ultra-Wideband (UWB) technology execute mapping, detection, and manipulation
tasks using a versatile gripper designed for wet, saline conditions.
Additionally, two UAVs can coordinate to transport large objects to the
manipulator or interact directly with them. These procedures are fully
automated and were successfully demonstrated at the Mohammed Bin Zayed
International Robotic Competition (MBZIRC2024), where the drone carrier
equipped with four UAVS and one manipulator, automatically accomplished the
intervention tasks in sea-level-3 (wave height 1.25m) based on the rough target
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSGSL: A Probabilistic Framework Integrating Semantic Scene
  Understanding and Gas Sensing for Gas Source Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pepe Ojeda, Javier Monroy, Javier Gonzalez-Jimenez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic scene understanding allows a robotic agent to reason about problems
in complex ways, using information from multiple and varied sensors to make
deductions about a particular matter. As a result, this form of intelligent
robotics is capable of performing more complex tasks and achieving more precise
results than simpler approaches based on single data sources. However, these
improved capabilities come at the cost of higher complexity, both computational
and in terms of design. Due to the increased design complexity, formal
approaches for exploiting semantic understanding become necessary.
  We present here a probabilistic formulation for integrating semantic
knowledge into the process of gas source localization (GSL). The problem of GSL
poses many unsolved challenges, and proposed solutions need to contend with the
constraining limitations of sensing hardware. By exploiting semantic scene
understanding, we can leverage other sources of information, such as vision, to
improve the estimation of the source location. We show how our formulation can
be applied to pre-existing GSL algorithms and the effect that including
semantic data has on the produced estimations of the location of the source.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Int2Planner: An Intention-based Multi-modal Motion Planner for
  Integrated Prediction and Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolei Chen, Junchi Yan, Wenlong Liao, Tao He, Pai Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion planning is a critical module in autonomous driving, with the primary
challenge of uncertainty caused by interactions with other participants. As
most previous methods treat prediction and planning as separate tasks, it is
difficult to model these interactions. Furthermore, since the route path
navigates ego vehicles to a predefined destination, it provides relatively
stable intentions for ego vehicles and helps constrain uncertainty. On this
basis, we construct Int2Planner, an \textbf{Int}ention-based
\textbf{Int}egrated motion \textbf{Planner} achieves multi-modal planning and
prediction. Instead of static intention points, Int2Planner utilizes route
intention points for ego vehicles and generates corresponding planning
trajectories for each intention point to facilitate multi-modal planning. The
experiments on the private dataset and the public nuPlan benchmark show the
effectiveness of route intention points, and Int2Planner achieves
state-of-the-art performance. We also deploy it in real-world vehicles and have
conducted autonomous driving for hundreds of kilometers in urban areas. It
further verifies that Int2Planner can continuously interact with the traffic
environment. Code will be avaliable at https://github.com/cxlz/Int2Planner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grid-based Submap Joining: An Efficient Algorithm for Simultaneously
  Optimizing Global Occupancy Map and Local Submap Frames <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyu Wang, Liang Zhao, Shoudong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing robot poses and the map simultaneously has been shown to provide
more accurate SLAM results. However, for non-feature based SLAM approaches,
directly optimizing all the robot poses and the whole map will greatly increase
the computational cost, making SLAM problems difficult to solve in large-scale
environments. To solve the 2D non-feature based SLAM problem in large-scale
environments more accurately and efficiently, we propose the grid-based submap
joining method. Specifically, we first formulate the 2D grid-based submap
joining problem as a non-linear least squares (NLLS) form to optimize the
global occupancy map and local submap frames simultaneously. We then prove that
in solving the NLLS problem using Gauss-Newton (GN) method, the increments of
the poses in each iteration are independent of the occupancy values of the
global occupancy map. Based on this property, we propose a poseonly GN
algorithm equivalent to full GN method to solve the NLLS problem. The proposed
submap joining algorithm is very efficient due to the independent property and
the pose-only solution. Evaluations using simulations and publicly available
practical 2D laser datasets confirm the outperformance of our proposed method
compared to the state-of-the-art methods in terms of efficiency and accuracy,
as well as the ability to solve the grid-based SLAM problem in very large-scale
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnyNav: Visual Neuro-Symbolic Friction Learning for Off-road Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taimeng Fu, Zitong Zhan, Zhipeng Zhao, Shaoshu Su, Xiao Lin, Ehsan Tarkesh Esfahani, Karthik Dantu, Souma Chowdhury, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-road navigation is essential for a wide range of applications in field
robotics such as planetary exploration and disaster response. However, it
remains an unresolved challenge due to the unstructured environments and
inherent complexity of terrain-vehicle interactions. Traditional physics-based
methods struggle to accurately model the nonlinear dynamics of these
interactions, while data-driven approaches often suffer from overfitting to
specific motion patterns, vehicle sizes, and types, limiting their
generalizability. To overcome these challenges, we introduce a vision-based
friction estimation framework grounded in neuro-symbolic principles,
integrating neural networks for visual perception with symbolic reasoning for
physical modeling. This enables significantly improved generalization abilities
through explicit physical reasoning incorporating the predicted friction.
Additionally, we develop a physics-informed planner that leverages the learned
friction coefficient to generate physically feasible and efficient paths, along
with corresponding speed profiles. We refer to our approach as AnyNav and
evaluate it in both simulation and real-world experiments, demonstrating its
utility and robustness across various off-road scenarios and multiple types of
four-wheeled vehicles. These results mark an important step toward developing
neuro-symbolic spatial intelligence to reason about complex, unstructured
environments and enable autonomous off-road navigation in challenging
scenarios. Video demonstrations are available at https://sairlab.org/anynav/,
where the source code will also be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 3-Step Optimization Framework with Hybrid Models for a Humanoid
  Robot's Jump Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Qi, Zhangguo Yu, Xuechao Chen, Yaliang Liu, Chuanku Yi, Chencheng Dong, Fei Meng, Qiang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High dynamic jump motions are challenging tasks for humanoid robots to
achieve environment adaptation and obstacle crossing. The trajectory
optimization is a practical method to achieve high-dynamic and explosive
jumping. This paper proposes a 3-step trajectory optimization framework for
generating a jump motion for a humanoid robot. To improve iteration speed and
achieve ideal performance, the framework comprises three sub-optimizations. The
first optimization incorporates momentum, inertia, and center of pressure
(CoP), treating the robot as a static reaction momentum pendulum (SRMP) model
to generate corresponding trajectories. The second optimization maps these
trajectories to joint space using effective Quadratic Programming (QP) solvers.
Finally, the third optimization generates whole-body joint trajectories
utilizing trajectories generated by previous parts. With the combined
consideration of momentum and inertia, the robot achieves agile forward jump
motions. A simulation and experiments (Fig. \ref{Fig First page fig}) of
forward jump with a distance of 1.0 m and 0.5 m height are presented in this
paper, validating the applicability of the proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "See You Later, Alligator": Impacts of Robot Small Talk on Task,
  Rapport, and Interaction Dynamics in Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaitlynn Taylor Pineda, Ethan Brown, Chien-Ming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small talk can foster rapport building in human-human teamwork; yet how
non-anthropomorphic robots, such as collaborative manipulators commonly used in
industry, may capitalize on these social communications remains unclear. This
work investigates how robot-initiated small talk influences task performance,
rapport, and interaction dynamics in human-robot collaboration. We developed an
autonomous robot system that assists a human in an assembly task while
initiating and engaging in small talk. A user study ($N = 58$) was conducted in
which participants worked with either a functional robot, which engaged in only
task-oriented speech, or a social robot, which also initiated small talk. Our
study found that participants in the social condition reported significantly
higher levels of rapport with the robot. Moreover, all participants in the
social condition responded to the robot's small talk attempts; 59% initiated
questions to the robot, and 73% engaged in lingering conversations after
requesting the final task item. Although active working times were similar
across conditions, participants in the social condition recorded longer task
durations than those in the functional condition. We discuss the design and
implications of robot small talk in shaping human-robot collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, preprint for HRI25, the 20th edition of the
  IEEE/ACM International Conference on Human-Robot Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Efficient Robot Action Planning in the Presence of Unconcerned
  Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Amiri, Mehdi Hosseinzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a robot action planning scheme that provides an efficient
and probabilistically safe plan for a robot interacting with an unconcerned
human -- someone who is either unaware of the robot's presence or unwilling to
engage in ensuring safety. The proposed scheme is predictive, meaning that the
robot is required to predict human actions over a finite future horizon; such
predictions are often inaccurate in real-world scenarios. One possible approach
to reduce the uncertainties is to provide the robot with the capability of
reasoning about the human's awareness of potential dangers. This paper
discusses that by using a binary variable, so-called danger awareness
coefficient, it is possible to differentiate between concerned and unconcerned
humans, and provides a learning algorithm to determine this coefficient by
observing human actions. Moreover, this paper argues how humans rely on
predictions of other agents' future actions (including those of robots in
human-robot interaction) in their decision-making. It also shows that ignoring
this aspect in predicting human's future actions can significantly degrade the
efficiency of the interaction, causing agents to deviate from their optimal
paths. The proposed robot action planning scheme is verified and validated via
extensive simulation and experimental studies on a LoCoBot WidowX-250.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polyhedral Collision Detection via Vertex Enumeration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Cinar, Yue Zhao, Forrest Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collision detection is a critical functionality for robotics. The degree to
which objects collide cannot be represented as a continuously differentiable
function for any shapes other than spheres. This paper proposes a framework for
handling collision detection between polyhedral shapes. We frame the signed
distance between two polyhedral bodies as the optimal value of a convex
optimization, and consider constraining the signed distance in a bilevel
optimization problem. To avoid relying on specialized bilevel solvers, our
method exploits the fact that the signed distance is the minimal point of a
convex region related to the two bodies. Our method enumerates the values
obtained at all extreme points of this region and lists them as constraints in
the higher-level problem. We compare our formulation to existing methods in
terms of reliability and speed when solved using the same mixed complementarity
problem solver. We demonstrate that our approach more reliably solves difficult
collision detection problems with multiple obstacles than other methods, and is
faster than existing methods in some cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Map Prediction and Generative Entropy for Multi-Agent Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Spinos, Bradley Woosley, Justin Rokisky, Christopher Korpela, John G. Rogers III, Brian A. Bittner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, autonomous reconnaissance applications have acted on explicit
sets of historical observations. Aided by recent breakthroughs in generative
technologies, this work enables robot teams to act beyond what is currently
known about the environment by inferring a distribution of reasonable
interpretations of the scene. We developed a map predictor that inpaints the
unknown space in a multi-agent 2D occupancy map during an exploration mission.
From a comparison of several inpainting methods, we found that a fine-tuned
latent diffusion inpainting model could provide rich and coherent
interpretations of simulated urban environments with relatively little
computation time. By iteratively inferring interpretations of the scene
throughout an exploration run, we are able to identify areas that exhibit high
uncertainty in the prediction, which we formalize with the concept of
generative entropy. We prioritize tasks in regions of high generative entropy,
hypothesizing that this will expedite convergence on an accurate predicted map
of the scene. In our study we juxtapose this new paradigm of task ranking with
the state of the art, which ranks regions to explore by those which maximize
expected information recovery. We compare both of these methods in a simulated
urban environment with three vehicles. Our results demonstrate that by using
our new task ranking method, we can predict a correct scene significantly
faster than with a traditional information-guided method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat
  Using Leader-Follower Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhui Pang, Jinglin He, Noureldin Mohamed Abdelaal Ahmed Mohamed, Changqing Lin, Zhihui Zhang, Xiaoshuai Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-UAV air combat is a complex task involving multiple autonomous UAVs, an
evolving field in both aerospace and artificial intelligence. This paper aims
to enhance adversarial performance through collaborative strategies. Previous
approaches predominantly discretize the action space into predefined actions,
limiting UAV maneuverability and complex strategy implementation. Others
simplify the problem to 1v1 combat, neglecting the cooperative dynamics among
multiple UAVs. To address the high-dimensional challenges inherent in
six-degree-of-freedom space and improve cooperation, we propose a hierarchical
framework utilizing the Leader-Follower Multi-Agent Proximal Policy
Optimization (LFMAPPO) strategy. Specifically, the framework is structured into
three levels. The top level conducts a macro-level assessment of the
environment and guides execution policy. The middle level determines the angle
of the desired action. The bottom level generates precise action commands for
the high-dimensional action space. Moreover, we optimize the state-value
functions by assigning distinct roles with the leader-follower strategy to
train the top-level policy, followers estimate the leader's utility, promoting
effective cooperation among agents. Additionally, the incorporation of a target
selector, aligned with the UAVs' posture, assesses the threat level of targets.
Finally, simulation experiments validate the effectiveness of our proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Spatio-temporal Graph Network Allowing Incomplete Trajectory Input for
  Pedestrian Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncen Long, Gianluca Bardaro, Simone Mentasti, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian trajectory prediction is important in the research of mobile robot
navigation in environments with pedestrians. Most pedestrian trajectory
prediction algorithms require the input historical trajectories to be complete.
If a pedestrian is unobservable in any frame in the past, then its historical
trajectory become incomplete, the algorithm will not predict its future
trajectory. To address this limitation, we propose the STGN-IT, a
spatio-temporal graph network allowing incomplete trajectory input, which can
predict the future trajectories of pedestrians with incomplete historical
trajectories. STGN-IT uses the spatio-temporal graph with an additional
encoding method to represent the historical trajectories and observation states
of pedestrians. Moreover, STGN-IT introduces static obstacles in the
environment that may affect the future trajectories as nodes to further improve
the prediction accuracy. A clustering algorithm is also applied in the
construction of spatio-temporal graphs. Experiments on public datasets show
that STGN-IT outperforms state of the art algorithms on these metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Ergodic Search with Kernel Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Muchen Sun, Ayush Gaggar, Peter Trautman, Todd Murphey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ergodic search enables optimal exploration of an information distribution
while guaranteeing the asymptotic coverage of the search space. However,
current methods typically have exponential computation complexity in the search
space dimension and are restricted to Euclidean space. We introduce a
computationally efficient ergodic search method. Our contributions are
two-fold. First, we develop a kernel-based ergodic metric and generalize it
from Euclidean space to Lie groups. We formally prove the proposed metric is
consistent with the standard ergodic metric while guaranteeing linear
complexity in the search space dimension. Secondly, we derive the first-order
optimality condition of the kernel ergodic metric for nonlinear systems, which
enables efficient trajectory optimization. Comprehensive numerical benchmarks
show that the proposed method is at least two orders of magnitude faster than
the state-of-the-art algorithm. Finally, we demonstrate the proposed algorithm
with a peg-in-hole insertion task. We formulate the problem as a coverage task
in the space of SE(3) and use a 30-second-long human demonstration as the prior
distribution for ergodic coverage. Ergodicity guarantees the asymptotic
solution of the peg-in-hole problem so long as the solution resides within the
prior information distribution, which is seen in the 100% success rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Robotics (T-RO). 20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data-driven Contact Estimation Method for Wheeled-Biped Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ü. Bora Gökbakan, Frederike Dümbgen, Stéphane Caron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contact estimation is a key ability for limbed robots, where making and
breaking contacts has a direct impact on state estimation and balance control.
Existing approaches typically rely on gate-cycle priors or designated contact
sensors. We design a contact estimator that is suitable for the emerging
wheeled-biped robot types that do not have these features. To this end, we
propose a Bayes filter in which update steps are learned from real-robot torque
measurements while prediction steps rely on inertial measurements. We evaluate
this approach in extensive real-robot and simulation experiments. Our method
achieves better performance while being considerably more sample efficient than
a comparable deep-learning baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and
  Chain-of-Thought for Embodied Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, Helong Huang, Guangjian Tian, Weichao Qiu, Xingyue Quan, Jianye Hao, Yuzheng Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning is an essential problem in embodied AI research. Efforts to
enhance spatial reasoning abilities through supplementary spatial data and
fine-tuning have proven limited and ineffective when addressing complex
embodied tasks, largely due to their dependence on language-based outputs.
While some approaches have introduced a point-based action space to mitigate
this issue, they fall short in managing more intricate tasks within complex
environments. This deficiency arises from their failure to fully exploit the
inherent thinking and reasoning capabilities that are fundamental strengths of
Vision-Language Models (VLMs). To address these limitations, we propose a novel
approach named SpatialCoT, specifically designed to bolster the spatial
reasoning capabilities of VLMs. Our approach comprises two stages: spatial
coordinate bi-directional alignment, which aligns vision-language inputs with
spatial coordinates, and chain-of-thought spatial grounding, which harnesses
the reasoning capabilities of language models for advanced spatial reasoning.
We evaluate SpatialCoT on challenging navigation and manipulation tasks, both
in simulation and real-world settings. Experimental results demonstrate that
our method significantly outperforms previous state-of-the-art approaches in
both tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design Optimizer for Soft Growing Robot Manipulators in
  Three-Dimensional Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Astar, Ozan Nurcan, Erk Demirel, Emir Ozen, Ozan Kutlar, Fabio Stroppa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft growing robots are novel devices that mimic plant-like growth for
navigation in cluttered or dangerous environments. Their ability to adapt to
surroundings, combined with advancements in actuation and manufacturing
technologies, allows them to perform specialized manipulation tasks. This work
presents an approach for design optimization of soft growing robots;
specifically, the three-dimensional extension of the optimizer designed for
planar manipulators. This tool is intended to be used by engineers and robot
enthusiasts before manufacturing their robot: it suggests the optimal size of
the robot for solving a specific task. The design process models a
multi-objective optimization problem to refine a soft manipulator's kinematic
chain. Thanks to the novel Rank Partitioning algorithm integrated into
Evolutionary Computation (EC) algorithms, this method achieves high precision
in reaching targets and is efficient in resource usage. Results show
significantly high performance in solving three-dimensional tasks, whereas
comparative experiments indicate that the optimizer features robust output when
tested with different EC algorithms, particularly genetic algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TeleP<span class="highlight-title">review</span>: A User-Friendly Teleoperation System with Virtual Arm
  Assistance for Enhanced Effectiveness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxiang Guo, Jiayu Luo, Zhenyu Wei, Yiwen Hou, Zhixuan Xu, Xiaoyi Lin, Chongkai Gao, Lin Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperation provides an effective way to collect robot data, which is
crucial for learning from demonstrations. In this field, teleoperation faces
several key challenges: user-friendliness for new users, safety assurance, and
transferability across different platforms. While collecting real robot
dexterous manipulation data by teleoperation to train robots has shown
impressive results on diverse tasks, due to the morphological differences
between human and robot hands, it is not only hard for new users to understand
the action mapping but also raises potential safety concerns during operation.
To address these limitations, we introduce TelePreview. This teleoperation
system offers real-time visual feedback on robot actions based on human user
inputs, with a total hardware cost of less than $1,000. TelePreview allows the
user to see a virtual robot that represents the outcome of the user's next
movement. By enabling flexible switching between command visualization and
actual execution, this system helps new users learn how to demonstrate quickly
and safely. We demonstrate that it outperforms other teleoperation systems
across five tasks, emphasize its ease of use, and highlight its straightforward
deployment across diverse robotic platforms. We release our code and a
deployment document on our website https://telepreview.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon
  Visuomotor Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Break Yang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a low-cost legged mobile manipulation system that solves
long-horizon real-world tasks, trained by reinforcement learning purely in
simulation. This system is made possible by 1) a hierarchical design of a
high-level policy for visual-mobile manipulation following instructions and a
low-level policy for quadruped movement and limb control, 2) a progressive
exploration and learning approach that leverages privileged task decomposition
information to train the teacher policy for long-horizon tasks, which will
guide an imitation-based student policy for efficient training of the
high-level visuomotor policy, and 3) a suite of techniques for minimizing
sim-to-real gaps.
  In contrast to previous approaches that use high-end equipment, our system
demonstrates effective performance with more accessible hardware -
specifically, a Unitree Go1 quadruped, a WidowX250S arm, and a single
wrist-mounted RGB camera - despite the increased challenges of sim-to-real
transfer. When fully trained in simulation, a single policy autonomously solves
long-horizon tasks such as search, move, grasp, and drop-into, achieving nearly
80% success. This performance is comparable to that of expert human
teleoperation on the same tasks but operates in a more efficient way, at 1.5
times the speed of human expert. The sim-to-real transfer is fluid across
diverse indoor and outdoor scenes under varying lighting conditions. Finally,
we discuss the key techniques that enable the entire pipeline, including
efficient RL training and sim-to-real, to work effectively for legged mobile
manipulation, and present their ablation results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline-to-online Reinforcement Learning for Image-based Grasping with
  Scarce Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Chan, Anson Leung, James Bergstra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline-to-online reinforcement learning (O2O RL) aims to obtain a
continually improving policy as it interacts with the environment, while
ensuring the initial policy behaviour is satisficing. This satisficing
behaviour is necessary for robotic manipulation where random exploration can be
costly due to catastrophic failures and time. O2O RL is especially compelling
when we can only obtain a scarce amount of (potentially suboptimal)
demonstrations$\unicode{x2014}$a scenario where behavioural cloning (BC) is
known to suffer from distribution shift. Previous works have outlined the
challenges in applying O2O RL algorithms under the image-based environments. In
this work, we propose a novel O2O RL algorithm that can learn in a real-life
image-based robotic vacuum grasping task with a small number of demonstrations
where BC fails majority of the time. The proposed algorithm replaces the target
network in off-policy actor-critic algorithms with a regularization technique
inspired by neural tangent kernel. We demonstrate that the proposed algorithm
can reach above 90\% success rate in under two hours of interaction time, with
only 50 human demonstrations, while BC and existing commonly-used RL algorithms
fail to achieve similar performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CoRL Workshop on Mastering Robot Manipulation in a World of
  Abundant Data 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Novice to Skilled: RL-based Shared Autonomy Communicating with
  Pilots in UAV Multi-Task Missions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kal Backman, Dana Kulić, Hoam Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task missions for unmanned aerial vehicles (UAVs) involving inspection
and landing tasks are challenging for novice pilots due to the difficulties
associated with depth perception and the control interface. We propose a shared
autonomy system, alongside supplementary information displays, to assist pilots
to successfully complete multi-task missions without any pilot training. Our
approach comprises of three modules: (1) a perception module that encodes
visual information onto a latent representation, (2) a policy module that
augments pilot's actions, and (3) an information augmentation module that
provides additional information to the pilot. The policy module is trained in
simulation with simulated users and transferred to the real world without
modification in a user study (n=29), alongside alternative supplementary
information schemes including learnt red/green light feedback cues and an
augmented reality display. The pilot's intent is unknown to the policy module
and is inferred from the pilot's input and UAV's states. The assistant
increased task success rate for the landing and inspection tasks from [16.67% &
54.29%] respectively to [95.59% & 96.22%]. With the assistant, inexperienced
pilots achieved similar performance to experienced pilots. Red/green light
feedback cues reduced the required time by 19.53% and trajectory length by
17.86% for the inspection task, where participants rated it as their preferred
condition due to the intuitive interface and providing reassurance. This work
demonstrates that simple user models can train shared autonomy systems in
simulation, and transfer to physical tasks to estimate user intent and provide
effective assistance and information to the pilot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 11 figures, 6 tables. Accepted to ACM Transactions on
  Human-Robot Interaction (THRI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word
  GRNG for AI Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zephan M. Enciso, Boyang Cheng, Likai Pei, Jianbo Liu, Steven Davis, Michael Niemier, Ningyuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty estimation is an indispensable capability for AI-enabled,
safety-critical applications, e.g. autonomous vehicles or medical diagnosis.
Bayesian neural networks (BNNs) use Bayesian statistics to provide both
classification predictions and uncertainty estimation, but they suffer from
high computational overhead associated with random number generation and
repeated sample iterations. Furthermore, BNNs are not immediately amenable to
acceleration through compute-in-memory architectures due to the frequent memory
writes necessary after each RNG operation. To address these challenges, we
present an ASIC that integrates 360 fJ/Sample Gaussian RNG directly into the
SRAM memory words. This integration reduces RNG overhead and enables
fully-parallel compute-in-memory operations for BNNs. The prototype chip
achieves 5.12 GSa/s RNG throughput and 102 GOp/s neural network throughput
while occupying 0.45 mm2, bringing AI uncertainty estimation to edge
computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-21T00:00:00Z">2025-01-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">31</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Feedback Motion Planning using Probably Approximately
  Correct Nonlinear Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Gonzales, Adam Polevoy, Marin Kobilarov, Joseph Moore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For many tasks, multi-robot teams often provide greater efficiency,
robustness, and resiliency. However, multi-robot collaboration in real-world
scenarios poses a number of major challenges, especially when dynamic robots
must balance competing objectives like formation control and obstacle avoidance
in the presence of stochastic dynamics and sensor uncertainty. In this paper,
we propose a distributed, multi-agent receding-horizon feedback motion planning
approach using Probably Approximately Correct Nonlinear Model Predictive
Control (PAC-NMPC) that is able to reason about both model and measurement
uncertainty to achieve robust multi-agent formation control while navigating
cluttered obstacle fields and avoiding inter-robot collisions. Our approach
relies not only on the underlying PAC-NMPC algorithm but also on a terminal
cost-function derived from gyroscopic obstacle avoidance. Through numerical
simulation, we show that our distributed approach performs on par with a
centralized formulation, that it offers improved performance in the case of
significant measurement noise, and that it can scale to more complex dynamical
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving robot understanding using conversational AI: demonstration and
  feasibility study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikhar Kumar, Yael Edan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations constitute an important aspect of successful human robot
interactions and can enhance robot understanding. To improve the understanding
of the robot, we have developed four levels of explanation (LOE) based on two
questions: what needs to be explained, and why the robot has made a particular
decision. The understandable robot requires a communicative action when there
is disparity between the human s mental model of the robot and the robots state
of mind. This communicative action was generated by utilizing a conversational
AI platform to generate explanations. An adaptive dialog was implemented for
transition from one LOE to another. Here, we demonstrate the adaptive dialog in
a collaborative task with errors and provide results of a feasibility study
with users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40th Anniversary, IEEE International Conference on Robotics and
  Automation,2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced
  Human-Robot Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Schreiter, Jens V. Rüppel, Rishi Hazra, Andrey Rudenko, Martin Magnusson, Achim J. Lilienthal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve natural and intuitive interaction with people, HRI frameworks
combine a wide array of methods for human perception, intention communication,
human-aware navigation and collaborative action. In practice, when encountering
unpredictable behavior of people or unexpected states of the environment, these
frameworks may lack the ability to dynamically recognize such states, adapt and
recover to resume the interaction. Large Language Models (LLMs), owing to their
advanced reasoning capabilities and context retention, present a promising
solution for enhancing robot adaptability. This potential, however, may not
directly translate to improved interaction metrics. This paper considers a
representative interaction with an industrial robot involving approach,
instruction, and object manipulation, implemented in two conditions: (1) fully
scripted and (2) including LLM-enhanced responses. We use gaze tracking and
questionnaires to measure the participants' task efficiency, engagement, and
robot perception. The results indicate higher subjective ratings for the LLM
condition, but objective metrics show that the scripted condition performs
comparably, particularly in efficiency and focus during simple tasks. We also
note that the scripted condition may have an edge over LLM-enhanced responses
in terms of response latency and energy consumption, especially for trivial and
repetitive interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Late-Breaking Report to the 2025, 20th ACM/IEEE
  International Conference on Human-Robot Interaction (HRI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards autonomous photogrammetric forest inventory using a lightweight
  under-canopy robotic drone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Väinö Karjalainen, Niko Koivumäki, Teemu Hakala, Jesse Muhojoki, Eric Hyyppä, Anand George, Juha Suomalainen, Eija Honkavaara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drones are increasingly used in forestry to capture high-resolution remote
sensing data. While operations above the forest canopy are already highly
automated, flying inside forests remains challenging, primarily relying on
manual piloting. Inside dense forests, reliance on the Global Navigation
Satellite System (GNSS) for localization is not feasible. Additionally, the
drone must autonomously adjust its flight path to avoid collisions. Recently,
advancements in robotics have enabled autonomous drone flights in GNSS-denied
obstacle-rich areas. In this article, a step towards autonomous forest data
collection is taken by building a prototype of a robotic under-canopy drone
utilizing state-of-the-art open-source methods and validating its performance
for data collection inside forests. The autonomous flight capability was
evaluated through multiple test flights in two boreal forest test sites. The
tree parameter estimation capability was studied by conducting diameter at
breast height (DBH) estimation using onboard stereo camera data and
photogrammetric methods. The prototype conducted flights in selected
challenging forest environments, and the experiments showed excellent
performance in forest reconstruction with a miniaturized stereoscopic
photogrammetric system. The stem detection algorithm managed to identify 79.31
% of the stems. The DBH estimation had a root mean square error (RMSE) of 3.33
cm (12.79 %) and a bias of 1.01 cm (3.87 %) across all trees. For trees with a
DBH less than 30 cm, the RMSE was 1.16 cm (5.74 %), and the bias was 0.13 cm
(0.64 %). When considering the overall performance in terms of DBH accuracy,
autonomy, and forest complexity, the proposed approach was superior compared to
methods proposed in the scientific literature. Results provided valuable
insights into autonomous forest reconstruction using drones, and several
further development topics were proposed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 13 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Cost 3D printed, Biocompatible Ionic Polymer Membranes for Soft
  Actuators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Trümpler, Ryo Kanno, Niu David, Anja Huch, Pham Huy Nguyen, Maksims Jurinovs, Gustav Nyström, Sergejs Gaidukovs, Mirko Kovac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ionic polymer actuators, in essence, consist of ion exchange polymers
sandwiched between layers of electrodes. They have recently gained recognition
as promising candidates for soft actuators due to their lightweight nature,
noise-free operation, and low-driving voltages. However, the materials
traditionally utilized to develop them are often not human/environmentally
friendly. Thus, to address this issue, researchers have been focusing on
developing biocompatible versions of this actuator. Despite this, such
actuators still face challenges in achieving high performance, in payload
capacity, bending capabilities, and response time. In this paper, we present a
biocompatible ionic polymer actuator whose membrane is fully 3D printed
utilizing a direct ink writing method. The structure of the printed membranes
consists of biodegradable ionic fluid encapsulated within layers of activated
carbon polymers. From the microscopic observations of its structure, we
confirmed that the ionic polymer is well encapsulated. The actuators can
achieve a bending performance of up to 124$^\circ$ (curvature of 0.82
$\text{cm}^{-1}$), which, to our knowledge, is the highest curvature attained
by any bending ionic polymer actuator to date. It can operate comfortably up to
a 2 Hz driving frequency and can achieve blocked forces of up to 0.76 mN. Our
results showcase a promising, high-performing biocompatible ionic polymer
actuator, whose membrane can be easily manufactured in a single step using a
standard FDM 3D printer. This approach paves the way for creating customized
designs for functional soft robotic applications, including human-interactive
devices, in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures, Accepted in IEEE International Conference on Soft
  Robotics 2025 (Robosoft)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Hop for a Single-Legged Robot with Parallel Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Zhang, Xiangyu Chu, Yanlin Chen, Yunxi Tang, Linzhu Yue, Yun-Hui Liu, Kwok Wai Samuel Au
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents the application of reinforcement learning to improve the
performance of a highly dynamic hopping system with a parallel mechanism.
Unlike serial mechanisms, parallel mechanisms can not be accurately simulated
due to the complexity of their kinematic constraints and closed-loop
structures. Besides, learning to hop suffers from prolonged aerial phase and
the sparse nature of the rewards. To address them, we propose a learning
framework to encode long-history feedback to account for the under-actuation
brought by the prolonged aerial phase. In the proposed framework, we also
introduce a simplified serial configuration for the parallel design to avoid
directly simulating parallel structure during the training. A torque-level
conversion is designed to deal with the parallel-serial conversion to handle
the sim-to-real issue. Simulation and hardware experiments have been conducted
to validate this framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Robot Swarm Through a Virtual Tube with Flow-Adaptive
  Distribution Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongwei Zhang, Shuli Lv, Kairong Liu, Quanyi Liang, Quan Quan, Zhikun She
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of robot swarm technology and its diverse
applications, navigating robot swarms through complex environments has emerged
as a critical research direction. To ensure safe navigation and avoid potential
collisions with obstacles, the concept of virtual tubes has been introduced to
define safe and navigable regions. However, current control methods in virtual
tubes face the congestion issues, particularly in narrow virtual tubes with low
throughput. To address these challenges, we first originally introduce the
concepts of virtual tube area and flow capacity, and develop an new evolution
model for the spatial density function. Next, we propose a novel control method
that combines a modified artificial potential field (APF) for swarm navigation
and density feedback control for distribution regulation, under which a
saturated velocity command is designed. Then, we generate a global velocity
field that not only ensures collision-free navigation through the virtual tube,
but also achieves locally input-to-state stability (LISS) for density tracking
errors, both of which are rigorously proven. Finally, numerical simulations and
realistic applications validate the effectiveness and advantages of the
proposed method in managing robot swarms within narrow virtual tubes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nocturnal eye inspired liquid to gas phase change soft actuator with
  Laser-Induced-Graphene: enhanced environmental light harvesting and
  photothermal conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maina Sogabe, Youhyun Kim, Kenji Kawashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic systems' mobility is constrained by power sources and wiring. While
pneumatic actuators remain tethered to air supplies, we developed a new
actuator utilizing light energy. Inspired by nocturnal animals' eyes, we
designed a bilayer soft actuator incorporating Laser-Induced Graphene (LIG) on
the inner surface of a silicone layer. This design maintains silicone's
transparency and flexibility while achieving 54% faster response time compared
to conventional actuators through enhanced photothermal conversion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23pages, 8 figures, journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Morris, Yiduo Wang, Mikolaj Kliniewski, Viorela Ila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Visual Simultaneous Localization and Mapping (vSLAM) systems
focus solely on static scene structures, overlooking dynamic elements in the
environment. Although effective for accurate visual odometry in complex
scenarios, these methods discard crucial information about moving objects. By
incorporating this information into a Dynamic SLAM framework, the motion of
dynamic entities can be estimated, enhancing navigation whilst ensuring
accurate localization. However, the fundamental formulation of Dynamic SLAM
remains an open challenge, with no consensus on the optimal approach for
accurate motion estimation within a SLAM pipeline. Therefore, we developed
DynoSAM, an open-source framework for Dynamic SLAM that enables the efficient
implementation, testing, and comparison of various Dynamic SLAM optimization
formulations. DynoSAM integrates static and dynamic measurements into a unified
optimization problem solved using factor graphs, simultaneously estimating
camera poses, static scene, object motion or poses, and object structures. We
evaluate DynoSAM across diverse simulated and real-world datasets, achieving
state-of-the-art motion estimation in indoor and outdoor environments, with
substantial improvements over existing systems. Additionally, we demonstrate
DynoSAM utility in downstream applications, including 3D reconstruction of
dynamic scenes and trajectory prediction, thereby showcasing potential for
advancing dynamic object-aware SLAM systems. DynoSAM is open-sourced at
https://github.com/ACFR-RPG/DynOSAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures. Submitted to T-RO Visual SLAM SI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connection-Coordination Rapport (CCR) Scale: A Dual-Factor Scale to
  Measure Human-Robot Rapport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Han Lin, Hannah Dinner, Tsz Long Leung, Bilge Mutlu, J. Gregory Trafton, Sarah Sebo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots, particularly in service and companionship roles, must develop
positive relationships with people they interact with regularly to be
successful. These positive human-robot relationships can be characterized as
establishing "rapport," which indicates mutual understanding and interpersonal
connection that form the groundwork for successful long-term human-robot
interaction. However, the human-robot interaction research literature lacks
scale instruments to assess human-robot rapport in a variety of situations. In
this work, we developed the 18-item Connection-Coordination Rapport (CCR) Scale
to measure human-robot rapport. We first ran Study 1 (N = 288) where online
participants rated videos of human-robot interactions using a set of candidate
items. Our Study 1 results showed the discovery of two factors in our scale,
which we named "Connection" and "Coordination." We then evaluated this scale by
running Study 2 (N = 201) where online participants rated a new set of
human-robot interaction videos with our scale and an existing rapport scale
from virtual agents research for comparison. We also validated our scale by
replicating a prior in-person human-robot interaction study, Study 3 (N = 44),
and found that rapport is rated significantly greater when participants
interacted with a responsive robot (responsive condition) as opposed to an
unresponsive robot (unresponsive condition). Results from these studies
demonstrate high reliability and validity for the CCR scale, which can be used
to measure rapport in both first-person and third-person perspectives. We
encourage the adoption of this scale in future studies to measure rapport in a
variety of human-robot interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating High Quality RT Planning at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riqiang Gao, Mamadou Diallo, Han Liu, Anthony Magliari, Jonathan Sackett, Wilko Verbakel, Sandra Meyers, Masoud Zarepisheh, Rafe Mcbeth, Simon Arberet, Martin Kraus, Florin C. Ghesu, Ali Kamen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiotherapy (RT) planning is complex, subjective, and time-intensive.
Advances in artificial intelligence (AI) promise to improve its precision,
efficiency, and consistency, but progress is often limited by the scarcity of
large, standardized datasets. To address this, we introduce the Automated
Iterative RT Planning (AIRTP) system, a scalable solution for generating
high-quality treatment plans. This scalable solution is designed to generate
substantial volumes of consistently high-quality treatment plans, overcoming a
key obstacle in the advancement of AI-driven RT planning. Our AIRTP pipeline
adheres to clinical guidelines and automates essential steps, including
organ-at-risk (OAR) contouring, helper structure creation, beam setup,
optimization, and plan quality improvement, using AI integrated with RT
planning software like Eclipse of Varian. Furthermore, a novel approach for
determining optimization parameters to reproduce 3D dose distributions, i.e. a
method to convert dose predictions to deliverable treatment plans constrained
by machine limitations. A comparative analysis of plan quality reveals that our
automated pipeline produces treatment plans of quality comparable to those
generated manually, which traditionally require several hours of labor per
plan. Committed to public research, the first data release of our AIRTP
pipeline includes nine cohorts covering head-and-neck and lung cancer sites to
support an AAPM 2025 challenge. This data set features more than 10 times the
number of plans compared to the largest existing well-curated public data set
to our best knowledge.
Repo:{https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Related to GDP-HMM grand challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interaction <span class="highlight-title">Dataset</span> of Autonomous Vehicles with Traffic Lights and Signs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Li, Zhipeng Bao, Haoming Meng, Haotian Shi, Qianwen Li, Handong Yao, Xiaopeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the development of a comprehensive dataset capturing
interactions between Autonomous Vehicles (AVs) and traffic control devices,
specifically traffic lights and stop signs. Derived from the Waymo Motion
dataset, our work addresses a critical gap in the existing literature by
providing real-world trajectory data on how AVs navigate these traffic control
devices. We propose a methodology for identifying and extracting relevant
interaction trajectory data from the Waymo Motion dataset, incorporating over
37,000 instances with traffic lights and 44,000 with stop signs. Our
methodology includes defining rules to identify various interaction types,
extracting trajectory data, and applying a wavelet-based denoising method to
smooth the acceleration and speed profiles and eliminate anomalous values,
thereby enhancing the trajectory quality. Quality assessment metrics indicate
that trajectories obtained in this study have anomaly proportions in
acceleration and jerk profiles reduced to near-zero levels across all
interaction categories. By making this dataset publicly available, we aim to
address the current gap in datasets containing AV interaction behaviors with
traffic lights and signs. Based on the organized and published dataset, we can
gain a more in-depth understanding of AVs' behavior when interacting with
traffic lights and signs. This will facilitate research on AV integration into
existing transportation infrastructures and networks, supporting the
development of more accurate behavioral models and simulation tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELEGNT: Expressive and Functional Movement Design for
  Non-anthropomorphic Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Hu, Peide Huang, Mouli Sivapurapu, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonverbal behaviors such as posture, gestures, and gaze are essential for
conveying internal states, both consciously and unconsciously, in human
interaction. For robots to interact more naturally with humans, robot movement
design should likewise integrate expressive qualities, such as intention,
attention, and emotions, alongside traditional functional considerations like
task fulfillment and time efficiency. In this paper, we present the design and
prototyping of a lamp-like robot that explores the interplay between functional
and expressive objectives in movement design. Using a research-through-design
methodology, we document the hardware design process, define expressive
movement primitives, and outline a set of interaction scenario storyboards. We
propose a framework that incorporates both functional and expressive utilities
during movement generation, and implement the robot behavior sequences in
different function- and social- oriented tasks. Through a user study comparing
expression-driven versus function-driven movements across six task scenarios,
our findings indicate that expression-driven movements significantly enhance
user engagement and perceived robot qualities. This effect is especially
pronounced in social-oriented tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, manuscript under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TOFFE -- Temporally-binned Object Flow from Events for High-speed and
  Energy-Efficient Object Detection and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Kumar Kosta, Amogh Joshi, Arjun Roy, Rohan Kumar Manna, Manish Nagaraj, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection and tracking is an essential perception task for enabling
fully autonomous navigation in robotic systems. Edge robot systems such as
small drones need to execute complex maneuvers at high-speeds with limited
resources, which places strict constraints on the underlying algorithms and
hardware. Traditionally, frame-based cameras are used for vision-based
perception due to their rich spatial information and simplified synchronous
sensing capabilities. However, obtaining detailed information across frames
incurs high energy consumption and may not even be required. In addition, their
low temporal resolution renders them ineffective in high-speed motion
scenarios. Event-based cameras offer a biologically-inspired solution to this
by capturing only changes in intensity levels at exceptionally high temporal
resolution and low power consumption, making them ideal for high-speed motion
scenarios. However, their asynchronous and sparse outputs are not natively
suitable with conventional deep learning methods. In this work, we propose
TOFFE, a lightweight hybrid framework for performing event-based object motion
estimation (including pose, direction, and speed estimation), referred to as
Object Flow. TOFFE integrates bio-inspired Spiking Neural Networks (SNNs) and
conventional Analog Neural Networks (ANNs), to efficiently process events at
high temporal resolutions while being simple to train. Additionally, we present
a novel event-based synthetic dataset involving high-speed object motion to
train TOFFE. Our experimental results show that TOFFE achieves 5.7x/8.3x
reduction in energy consumption and 4.6x/5.8x reduction in latency on edge
GPU(Jetson TX2)/hybrid hardware(Loihi-2 and Jetson TX2), compared to previous
event-based object detection baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiCAR: pseudo-RGB LiDAR image for CAR segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ignacio de Loyola Páez-Ubieta, Edison P. Velasco-Sánchez, Santiago T. Puente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of computing resources, an increasing number of Neural
Networks (NNs) are appearing for image detection and segmentation appear.
However, these methods usually accept as input a RGB 2D image. On the other
side, Light Detection And Ranging (LiDAR) sensors with many layers provide
images that are similar to those obtained from a traditional low resolution RGB
camera. Following this principle, a new dataset for segmenting cars in
pseudo-RGB images has been generated. This dataset combines the information
given by the LiDAR sensor into a Spherical Range Image (SRI), concretely the
reflectivity, near infrared and signal intensity 2D images. These images are
then fed into instance segmentation NNs. These NNs segment the cars that appear
in these images, having as result a Bounding Box (BB) and mask precision of 88%
and 81.5% respectively with You Only Look Once (YOLO)-v8 large. By using this
segmentation NN, some trackers have been applied so as to follow each car
segmented instance along a video feed, having great performance in real world
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version of the work accepted at 5th International
  Conference on Robotics, Computer Vision and Intelligent Systems (ROBOVIS
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A causal learning approach to in-orbit inertial parameter estimation for
  multi-payload deployers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Platanitis, Miguel Arana-Catania, Saurabh Upadhyay, Leonard Felicetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses an approach to inertial parameter estimation for the
case of cargo carrying spacecraft that is based on causal learning, i.e.
learning from the responses of the spacecraft, under actuation. Different
spacecraft configurations (inertial parameter sets) are simulated under
different actuation profiles, in order to produce an optimised time-series
clustering classifier that can be used to distinguish between them. The
actuation is comprised of finite sequences of constant inputs that are applied
in order, based on typical actuators available. By learning from the system's
responses across multiple input sequences, and then applying measures of
time-series similarity and F1-score, an optimal actuation sequence can be
chosen either for one specific system configuration or for the overall set of
possible configurations. This allows for both estimation of the inertial
parameter set without any prior knowledge of state, as well as validation of
transitions between different configurations after a deployment event. The
optimisation of the actuation sequence is handled by a reinforcement learning
model that uses the proximal policy optimisation (PPO) algorithm, by repeatedly
trying different sequences and evaluating the impact on classifier performance
according to a multi-objective metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 18 figures, 1 table. Presented in 75th International
  Astronautical Congress (IAC), Milan, Italy, 14-18 October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoundationStereo: Zero-Shot Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tremendous progress has been made in deep stereo matching to excel on
benchmark datasets through per-domain fine-tuning. However, achieving strong
zero-shot generalization - a hallmark of foundation models in other computer
vision tasks - remains challenging for stereo matching. We introduce
FoundationStereo, a foundation model for stereo depth estimation designed to
achieve strong zero-shot generalization. To this end, we first construct a
large-scale (1M stereo pairs) synthetic training dataset featuring large
diversity and high photorealism, followed by an automatic self-curation
pipeline to remove ambiguous samples. We then design a number of network
architecture components to enhance scalability, including a side-tuning feature
backbone that adapts rich monocular priors from vision foundation models to
mitigate the sim-to-real gap, and long-range context reasoning for effective
cost volume filtering. Together, these components lead to strong robustness and
accuracy across domains, establishing a new standard in zero-shot stereo depth
estimation. Project page: https://nvlabs.github.io/FoundationStereo/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Search-to-Control Reinforcement Learning Based Framework for Quadrotor
  Local Planning in Dense Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00275v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00275v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohong Liu, Wenxuan Gao, Yinshuai Sun, Peng Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agile flight in complex environments poses significant challenges to current
motion planning methods, as they often fail to fully leverage the quadrotor's
dynamic potential, leading to performance failures and reduced efficiency
during aggressive maneuvers. Existing approaches frequently decouple trajectory
optimization from control generation and neglect the dynamics, further limiting
their ability to generate aggressive and feasible motions. To address these
challenges, we introduce an enhanced Search-to-Control planning framework that
integrates visibility path searching with reinforcement learning (RL) control
generation, directly accounting for dynamics and bridging the gap between
planning and control. Our method first extracts control points from
collision-free paths using a proposed heuristic search, which are then refined
by an RL policy to generate low-level control commands for the quadrotor's
controller, utilizing reduced-dimensional obstacle observations for efficient
inference with lightweight neural networks. We validate the framework through
simulations and real-world experiments, demonstrating improved time efficiency
and dynamic maneuverability compared to existing methods, while confirming its
robustness and applicability. To support further research, We will release our
implementation as an open-source package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RadaRays: Real-time Simulation of Rotating FMCW Radar for Mobile
  Robotics via Hardware-accelerated Ray Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Mock, Martin Magnusson, Joachim Hertzberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RadaRays allows for the accurate modeling and simulation of rotating FMCW
radar sensors in complex environments, including the simulation of reflection,
refraction, and scattering of radar waves. Our software is able to handle large
numbers of objects and materials in real-time, making it suitable for use in a
variety of mobile robotics applications. We demonstrate the effectiveness of
RadaRays through a series of experiments and show that it can more accurately
reproduce the behavior of FMCW radar sensors in a variety of environments,
compared to the ray casting-based lidar-like simulations that are commonly used
in simulators for autonomous driving such as CARLA. Our experiments
additionally serve as a valuable reference point for researchers to evaluate
their own radar simulations. By using RadaRays, developers can significantly
reduce the time and cost associated with prototyping and testing FMCW
radar-based algorithms. We also provide a Gazebo plugin that makes our work
accessible to the mobile robotics community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling-based Model Predictive Control Leveraging Parallelizable
  Physics Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09105v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09105v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Corrado Pezzato, Chadi Salmi, Elia Trevisan, Max Spahn, Javier Alonso-Mora, Carlos Hernández Corbato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for sampling-based model predictive control that makes
use of a generic physics simulator as the dynamical model. In particular, we
propose a Model Predictive Path Integral controller (MPPI), that uses the
GPU-parallelizable IsaacGym simulator to compute the forward dynamics of a
problem. By doing so, we eliminate the need for explicit encoding of robot
dynamics and contacts with objects for MPPI. Since no explicit dynamic modeling
is required, our method is easily extendable to different objects and robots
and allows one to solve complex navigation and contact-rich tasks. We
demonstrate the effectiveness of this method in several simulated and
real-world settings, among which mobile navigation with collision avoidance,
non-prehensile manipulation, and whole-body control for high-dimensional
configuration spaces. This method is a powerful and accessible open-source tool
to solve a large variety of contact-rich motion planning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for RA-L. Code and videos available at
  https://autonomousrobots.nl/paper_websites/isaac-mppi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concurrent-Learning Based Relative Localization in Shape Formation of
  Robot Swarms (Extended version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06052v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06052v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhu Lü, Kunrui Ze, Shuoyu Yue, Kexin Liu, Wei Wang, Guibin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the shape formation problem for massive robot
swarms in environments where external localization systems are unavailable.
Achieving this task effectively with solely onboard measurements is still
scarcely explored and faces some practical challenges. To solve this
challenging problem, we propose the following novel results. Firstly, to
estimate the relative positions among neighboring robots, a concurrent-learning
based estimator is proposed. It relaxes the persistent excitation condition
required in the classical ones such as least-square estimator. Secondly, we
introduce a finite-time agreement protocol to determine the shape location.
This is achieved by estimating the relative position between each robot and a
randomly assigned seed robot. The initial position of the seed one marks the
shape location. Thirdly, based on the theoretical results of the relative
localization, a novel behavior-based control strategy is devised. This strategy
not only enables adaptive shape formation of large group of robots but also
enhances the observability of inter-robot relative localization. Numerical
simulation results are provided to verify the performance of our proposed
strategy compared to the state-of-the-art ones. Additionally, outdoor
experiments on real robots further demonstrate the practical effectiveness and
robustness of our methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Consensus Seeking via Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaben Chen, Wenkang Ji, Lufeng Xu, Shiyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent systems driven by large language models (LLMs) have shown
promising abilities for solving complex tasks in a collaborative manner. This
work considers a fundamental problem in multi-agent collaboration: consensus
seeking. When multiple agents work together, we are interested in how they can
reach a consensus through inter-agent negotiation. To that end, this work
studies a consensus-seeking task where the state of each agent is a numerical
value and they negotiate with each other to reach a consensus value. It is
revealed that when not explicitly directed on which strategy should be adopted,
the LLM-driven agents primarily use the average strategy for consensus seeking
although they may occasionally use some other strategies. Moreover, this work
analyzes the impact of the agent number, agent personality, and network
topology on the negotiation process. The findings reported in this work can
potentially lay the foundations for understanding the behaviors of LLM-driven
multi-agent systems for solving more complex tasks. Furthermore, LLM-driven
consensus seeking is applied to a multi-robot aggregation task. This
application demonstrates the potential of LLM-driven agents to achieve
zero-shot autonomous planning for multi-robot collaboration tasks. Project
website: windylab.github.io/ConsensusLLM/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AirPilot: Interpretable PPO-based DRL Auto-Tuned Nonlinear PID Drone
  Controller for Robust Autonomous Flights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00204v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00204v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Zhang, Cristian Emanuel Ocampo Rivera, Kyle Tyni, Steven Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation precision, speed and stability are crucial for safe Unmanned
Aerial Vehicle (UAV) flight maneuvers and effective flight mission executions
in dynamic environments. Different flight missions may have varying objectives,
such as minimizing energy consumption, achieving precise positioning, or
maximizing speed. A controller that can adapt to different objectives on the
fly is highly valuable. Proportional Integral Derivative (PID) controllers are
one of the most popular and widely used control algorithms for drones and other
control systems, but their linear control algorithm fails to capture the
nonlinear nature of the dynamic wind conditions and complex drone system.
Manually tuning the PID gains for various missions can be time-consuming and
requires significant expertise. This paper aims to revolutionize drone flight
control by presenting the AirPilot, a nonlinear Deep Reinforcement Learning
(DRL) - enhanced Proportional Integral Derivative (PID) drone controller using
Proximal Policy Optimization (PPO). AirPilot controller combines the simplicity
and effectiveness of traditional PID control with the adaptability, learning
capability, and optimization potential of DRL. This makes it better suited for
modern drone applications where the environment is dynamic, and
mission-specific performance demands are high. We employed a COEX Clover
autonomous drone for training the DRL agent within the simulator and
implemented it in a real-world lab setting, which marks a significant milestone
as one of the first attempts to apply a DRL-based flight controller on an
actual drone. Airpilot is capable of reducing the navigation error of the
default PX4 PID position controller by 90%, improving effective navigation
speed of a fine-tuned PID controller by 21%, reducing settling time and
overshoot by 17% and 16% respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complementarity-Free Multi-Contact Modeling and Optimization for
  Dexterous Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanxin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant barrier preventing model-based methods from achieving real-time
and versatile dexterous robotic manipulation is the inherent complexity of
multi-contact dynamics. Traditionally formulated as complementarity models,
multi-contact dynamics introduces non-smoothness and combinatorial complexity,
complicating contact-rich planning and optimization. In this paper, we
circumvent these challenges by introducing a lightweight yet capable
multi-contact model. Our new model, derived from the duality of
optimization-based contact models, dispenses with the complementarity
constructs entirely, providing computational advantages such as closed-form
time stepping, differentiability, automatic satisfaction with Coulomb friction
law, and minimal hyperparameter tuning. We demonstrate the effectiveness and
efficiency of the model for planning and control in a range of challenging
dexterous manipulation tasks, including fingertip 3D in-air manipulation,
TriFinger in-hand manipulation, and Allegro hand on-palm reorientation, all
performed with diverse objects. Our method consistently achieves
state-of-the-art results: (I) a 96.5% average success rate across all objects
and tasks, (II) high manipulation accuracy with an average reorientation error
of 11{\deg} and position error of 7.8mm, and (III) contact-implicit model
predictive control running at 50-100 Hz for all objects and tasks. These
results are achieved with minimal hyperparameter tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video demo: https://youtu.be/NsL4hbSXvFg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Spatial-Temporal Triangulation for Bearing-Only Cooperative
  Motion Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15846v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15846v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canlun Zheng, Yize Mi, Hanqing Guo, Huaben Chen, Zhiyun Lin, Shiyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based cooperative motion estimation is an important problem for many
multi-robot systems such as cooperative aerial target pursuit. This problem can
be formulated as bearing-only cooperative motion estimation, where the visual
measurement is modeled as a bearing vector pointing from the camera to the
target. The conventional approaches for bearing-only cooperative estimation are
mainly based on the framework distributed Kalman filtering (DKF). In this
paper, we propose a new optimal bearing-only cooperative estimation algorithm,
named spatial-temporal triangulation, based on the method of distributed
recursive least squares, which provides a more flexible framework for designing
distributed estimators than DKF. The design of the algorithm fully incorporates
all the available information and the specific triangulation geometric
constraint. As a result, the algorithm has superior estimation performance than
the state-of-the-art DKF algorithms in terms of both accuracy and convergence
speed as verified by numerical simulation. We rigorously prove the exponential
convergence of the proposed algorithm. Moreover, to verify the effectiveness of
the proposed algorithm under practical challenging conditions, we develop a
vision-based cooperative aerial target pursuit system, which is the first of
such fully autonomous systems so far to the best of our knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tightly-Coupled LiDAR-IMU-Wheel Odometry with an Online Neural Kinematic
  Model Learning via Factor Graph Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08907v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08907v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Okawara, Kenji Koide, Shuji Oishi, Masashi Yokozuka, Atsuhiko Banno, Kentaro Uno, Kazuya Yoshida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environments lacking geometric features (e.g., tunnels and long straight
corridors) are challenging for LiDAR-based odometry algorithms because LiDAR
point clouds degenerate in such environments. For wheeled robots, a wheel
kinematic model (i.e., wheel odometry) can improve the reliability of the
odometry estimation. However, the kinematic model suffers from complex motions
(e.g., wheel slippage, lateral movement) in the case of skid-steering robots
particularly because this robot model rotates by skidding its wheels.
Furthermore, these errors change nonlinearly when the wheel slippage is large
(e.g., drifting) and are subject to terrain-dependent parameters. To
simultaneously tackle point cloud degeneration and the kinematic model errors,
we developed a LiDAR-IMU-wheel odometry algorithm incorporating online training
of a neural network that learns the kinematic model of wheeled robots with
nonlinearity. We propose to train the neural network online on a factor graph
along with robot states, allowing the learning-based kinematic model to adapt
to the current terrain condition. The proposed method jointly solves online
training of the neural network and LiDARIMUwheel odometry on a unified factor
graph to retain the consistency of all those constraints. Through experiments,
we first verified that the proposed network adapted to a changing environment,
resulting in an accurate odometry estimation across different environments. We
then confirmed that the proposed odometry estimation algorithm was robust
against point cloud degeneration and nonlinearity (e.g., large wheel slippage
by drifting) of the kinematic model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://youtu.be/CvRVhdda7Cw</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLAME: Learning to Navigate with Multimodal LLM in Urban Environments <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated potential in
Vision-and-Language Navigation (VLN) tasks, yet current applications face
challenges. While LLMs excel in general conversation scenarios, they struggle
with specialized navigation tasks, yielding suboptimal performance compared to
specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied
Agent), a novel Multimodal LLM-based agent and architecture designed for urban
VLN tasks that efficiently handles multiple observations. Our approach
implements a three-phase tuning technique for effective adaptation to
navigation tasks, including single perception tuning for street view
description, multiple perception tuning for route summarization, and end-to-end
training on VLN datasets. The augmented datasets are synthesized automatically.
Experimental results demonstrate FLAME's superiority over existing methods,
surpassing state-of-the-art methods by a 7.3% increase in task completion on
Touchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs)
in complex navigation tasks, representing an advancement towards applications
of MLLMs in the field of embodied intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DCPI-Depth: Explicitly Infusing Dense Correspondence Prior to
  Unsupervised Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengtan Zhang, Yi Feng, Qijun Chen, Rui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a recent surge of interest in learning to perceive depth from
monocular videos in an unsupervised fashion. A key challenge in this field is
achieving robust and accurate depth estimation in challenging scenarios,
particularly in regions with weak textures or where dynamic objects are
present. This study makes three major contributions by delving deeply into
dense correspondence priors to provide existing frameworks with explicit
geometric constraints. The first novelty is a contextual-geometric depth
consistency loss, which employs depth maps triangulated from dense
correspondences based on estimated ego-motion to guide the learning of depth
perception from contextual information, since explicitly triangulated depth
maps capture accurate relative distances among pixels. The second novelty
arises from the observation that there exists an explicit, deducible
relationship between optical flow divergence and depth gradient. A differential
property correlation loss is, therefore, designed to refine depth estimation
with a specific emphasis on local variations. The third novelty is a
bidirectional stream co-adjustment strategy that enhances the interaction
between rigid and optical flows, encouraging the former towards more accurate
correspondence and making the latter more adaptable across various scenarios
under the static scene hypotheses. DCPI-Depth, a framework that incorporates
all these innovative components and couples two bidirectional and collaborative
streams, achieves state-of-the-art performance and generalizability across
multiple public datasets, outperforming all existing prior arts. Specifically,
it demonstrates accurate depth estimation in texture-less and dynamic regions,
and shows more reasonable smoothness. Our source code will be publicly
available at mias.group/DCPI-Depth upon publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18313v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18313v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanting Xie, So Yeon Min, Pengliang Ji, Yue Yang, Tianyi Zhang, Kedi Xu, Aarav Bajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson, Yonatan Bisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is no limit to how much a robot might explore and learn, but all of
that knowledge needs to be searchable and actionable. Within language research,
retrieval augmented generation (RAG) has become the workhorse of large-scale
non-parametric knowledge; however, existing techniques do not directly transfer
to the embodied domain, which is multimodal, where data is highly correlated,
and perception requires abstraction. To address these challenges, we introduce
Embodied-RAG, a framework that enhances the foundational model of an embodied
agent with a non-parametric memory system capable of autonomously constructing
hierarchical knowledge for both navigation and language generation.
Embodied-RAG handles a full range of spatial and semantic resolutions across
diverse environments and query types, whether for a specific object or a
holistic description of ambiance. At its core, Embodied-RAG's memory is
structured as a semantic forest, storing language descriptions at varying
levels of detail. This hierarchical organization allows the system to
efficiently generate context-sensitive outputs across different robotic
platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the
robotics domain, successfully handling over 250 explanation and navigation
queries across kilometer-level environments, highlighting its promise as a
general-purpose non-parametric system for embodied agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Web: https://quanting-xie.github.io/Embodied-RAG-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentBKI: Open-Dictionary Continuous Mapping in Visual-Language Latent
  Spaces with Quantifiable Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joey Wilson, Ruihan Xu, Yile Sun, Parker Ewen, Minghan Zhu, Kira Barton, Maani Ghaffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel probabilistic mapping algorithm, LatentBKI,
which enables open-vocabulary mapping with quantifiable uncertainty.
Traditionally, semantic mapping algorithms focus on a fixed set of semantic
categories which limits their applicability for complex robotic tasks.
Vision-Language (VL) models have recently emerged as a technique to jointly
model language and visual features in a latent space, enabling semantic
recognition beyond a predefined, fixed set of semantic classes. LatentBKI
recurrently incorporates neural embeddings from VL models into a voxel map with
quantifiable uncertainty, leveraging the spatial correlations of nearby
observations through Bayesian Kernel Inference (BKI). LatentBKI is evaluated
against similar explicit semantic mapping and VL mapping frameworks on the
popular Matterport3D and Semantic KITTI datasets, demonstrating that LatentBKI
maintains the probabilistic benefits of continuous mapping with the additional
benefit of open-dictionary queries. Real-world experiments demonstrate
applicability to challenging indoor environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tightly Coupled SLAM with Imprecise Architectural Plans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Shaheer, Jose Andres Millan-Romera, Hriday Bavle, Marco Giberna, Jose Luis Sanchez-Lopez, Javier Civera, Holger Voos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots navigating indoor environments often have access to architectural
plans, which can serve as prior knowledge to enhance their localization and
mapping capabilities. While some SLAM algorithms leverage these plans for
global localization in real-world environments, they typically overlook a
critical challenge: the "as-planned" architectural designs frequently deviate
from the "as-built" real-world environments. To address this gap, we present a
novel algorithm that tightly couples LIDAR-based simultaneous localization and
mapping with architectural plans under the presence of deviations. Our method
utilizes a multi-layered semantic representation to not only localize the
robot, but also to estimate global alignment and structural deviations between
"as-planned" and as-built environments in real-time. To validate our approach,
we performed experiments in simulated and real datasets demonstrating
robustness to structural deviations up to 35 cm and 15 degrees. On average, our
method achieves 43% less localization error than baselines in simulated
environments, while in real environments, the as-built 3D maps show 7% lower
average alignment error
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-02-05T01:03:46.886921011Z">
            2025-02-05 01:03:46 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
