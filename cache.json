{"2025-01-21T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2501.09898v2","updated":"2025-01-21T18:46:52Z","published":"2025-01-17T01:01:44Z","title":"FoundationStereo: Zero-Shot Stereo Matching","summary":"  Tremendous progress has been made in deep stereo matching to excel on\nbenchmark datasets through per-domain fine-tuning. However, achieving strong\nzero-shot generalization - a hallmark of foundation models in other computer\nvision tasks - remains challenging for stereo matching. We introduce\nFoundationStereo, a foundation model for stereo depth estimation designed to\nachieve strong zero-shot generalization. To this end, we first construct a\nlarge-scale (1M stereo pairs) synthetic training dataset featuring large\ndiversity and high photorealism, followed by an automatic self-curation\npipeline to remove ambiguous samples. We then design a number of network\narchitecture components to enhance scalability, including a side-tuning feature\nbackbone that adapts rich monocular priors from vision foundation models to\nmitigate the sim-to-real gap, and long-range context reasoning for effective\ncost volume filtering. Together, these components lead to strong robustness and\naccuracy across domains, establishing a new standard in zero-shot stereo depth\nestimation. Project page: https://nvlabs.github.io/FoundationStereo/\n","authors":["Bowen Wen","Matthew Trepte","Joseph Aribido","Jan Kautz","Orazio Gallo","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2501.09898v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12234v1","updated":"2025-01-21T15:57:15Z","published":"2025-01-21T15:57:15Z","title":"Multi-Agent Feedback Motion Planning using Probably Approximately\n  Correct Nonlinear Model Predictive Control","summary":"  For many tasks, multi-robot teams often provide greater efficiency,\nrobustness, and resiliency. However, multi-robot collaboration in real-world\nscenarios poses a number of major challenges, especially when dynamic robots\nmust balance competing objectives like formation control and obstacle avoidance\nin the presence of stochastic dynamics and sensor uncertainty. In this paper,\nwe propose a distributed, multi-agent receding-horizon feedback motion planning\napproach using Probably Approximately Correct Nonlinear Model Predictive\nControl (PAC-NMPC) that is able to reason about both model and measurement\nuncertainty to achieve robust multi-agent formation control while navigating\ncluttered obstacle fields and avoiding inter-robot collisions. Our approach\nrelies not only on the underlying PAC-NMPC algorithm but also on a terminal\ncost-function derived from gyroscopic obstacle avoidance. Through numerical\nsimulation, we show that our distributed approach performs on par with a\ncentralized formulation, that it offers improved performance in the case of\nsignificant measurement noise, and that it can scale to more complex dynamical\nsystems.\n","authors":["Mark Gonzales","Adam Polevoy","Marin Kobilarov","Joseph Moore"],"pdf_url":"https://arxiv.org/pdf/2501.12234v1.pdf","comment":"10 pages, 12 figures"},{"id":"http://arxiv.org/abs/2408.00275v3","updated":"2025-01-21T15:33:35Z","published":"2024-08-01T04:29:34Z","title":"A Search-to-Control Reinforcement Learning Based Framework for Quadrotor\n  Local Planning in Dense Environments","summary":"  Agile flight in complex environments poses significant challenges to current\nmotion planning methods, as they often fail to fully leverage the quadrotor's\ndynamic potential, leading to performance failures and reduced efficiency\nduring aggressive maneuvers. Existing approaches frequently decouple trajectory\noptimization from control generation and neglect the dynamics, further limiting\ntheir ability to generate aggressive and feasible motions. To address these\nchallenges, we introduce an enhanced Search-to-Control planning framework that\nintegrates visibility path searching with reinforcement learning (RL) control\ngeneration, directly accounting for dynamics and bridging the gap between\nplanning and control. Our method first extracts control points from\ncollision-free paths using a proposed heuristic search, which are then refined\nby an RL policy to generate low-level control commands for the quadrotor's\ncontroller, utilizing reduced-dimensional obstacle observations for efficient\ninference with lightweight neural networks. We validate the framework through\nsimulations and real-world experiments, demonstrating improved time efficiency\nand dynamic maneuverability compared to existing methods, while confirming its\nrobustness and applicability. To support further research, We will release our\nimplementation as an open-source package.\n","authors":["Zhaohong Liu","Wenxuan Gao","Yinshuai Sun","Peng Dong"],"pdf_url":"https://arxiv.org/pdf/2408.00275v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12214v1","updated":"2025-01-21T15:32:33Z","published":"2025-01-21T15:32:33Z","title":"Improving robot understanding using conversational AI: demonstration and\n  feasibility study","summary":"  Explanations constitute an important aspect of successful human robot\ninteractions and can enhance robot understanding. To improve the understanding\nof the robot, we have developed four levels of explanation (LOE) based on two\nquestions: what needs to be explained, and why the robot has made a particular\ndecision. The understandable robot requires a communicative action when there\nis disparity between the human s mental model of the robot and the robots state\nof mind. This communicative action was generated by utilizing a conversational\nAI platform to generate explanations. An adaptive dialog was implemented for\ntransition from one LOE to another. Here, we demonstrate the adaptive dialog in\na collaborative task with errors and provide results of a feasibility study\nwith users.\n","authors":["Shikhar Kumar","Yael Edan"],"pdf_url":"https://arxiv.org/pdf/2501.12214v1.pdf","comment":"40th Anniversary, IEEE International Conference on Robotics and\n  Automation,2024"},{"id":"http://arxiv.org/abs/2501.12128v1","updated":"2025-01-21T13:42:06Z","published":"2025-01-21T13:42:06Z","title":"Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced\n  Human-Robot Interactions","summary":"  To achieve natural and intuitive interaction with people, HRI frameworks\ncombine a wide array of methods for human perception, intention communication,\nhuman-aware navigation and collaborative action. In practice, when encountering\nunpredictable behavior of people or unexpected states of the environment, these\nframeworks may lack the ability to dynamically recognize such states, adapt and\nrecover to resume the interaction. Large Language Models (LLMs), owing to their\nadvanced reasoning capabilities and context retention, present a promising\nsolution for enhancing robot adaptability. This potential, however, may not\ndirectly translate to improved interaction metrics. This paper considers a\nrepresentative interaction with an industrial robot involving approach,\ninstruction, and object manipulation, implemented in two conditions: (1) fully\nscripted and (2) including LLM-enhanced responses. We use gaze tracking and\nquestionnaires to measure the participants' task efficiency, engagement, and\nrobot perception. The results indicate higher subjective ratings for the LLM\ncondition, but objective metrics show that the scripted condition performs\ncomparably, particularly in efficiency and focus during simple tasks. We also\nnote that the scripted condition may have an edge over LLM-enhanced responses\nin terms of response latency and energy consumption, especially for trivial and\nrepetitive interactions.\n","authors":["Tim Schreiter","Jens V. RÃ¼ppel","Rishi Hazra","Andrey Rudenko","Martin Magnusson","Achim J. Lilienthal"],"pdf_url":"https://arxiv.org/pdf/2501.12128v1.pdf","comment":"Accepted as a Late-Breaking Report to the 2025, 20th ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI)"},{"id":"http://arxiv.org/abs/2501.12073v1","updated":"2025-01-21T11:59:07Z","published":"2025-01-21T11:59:07Z","title":"Towards autonomous photogrammetric forest inventory using a lightweight\n  under-canopy robotic drone","summary":"  Drones are increasingly used in forestry to capture high-resolution remote\nsensing data. While operations above the forest canopy are already highly\nautomated, flying inside forests remains challenging, primarily relying on\nmanual piloting. Inside dense forests, reliance on the Global Navigation\nSatellite System (GNSS) for localization is not feasible. Additionally, the\ndrone must autonomously adjust its flight path to avoid collisions. Recently,\nadvancements in robotics have enabled autonomous drone flights in GNSS-denied\nobstacle-rich areas. In this article, a step towards autonomous forest data\ncollection is taken by building a prototype of a robotic under-canopy drone\nutilizing state-of-the-art open-source methods and validating its performance\nfor data collection inside forests. The autonomous flight capability was\nevaluated through multiple test flights in two boreal forest test sites. The\ntree parameter estimation capability was studied by conducting diameter at\nbreast height (DBH) estimation using onboard stereo camera data and\nphotogrammetric methods. The prototype conducted flights in selected\nchallenging forest environments, and the experiments showed excellent\nperformance in forest reconstruction with a miniaturized stereoscopic\nphotogrammetric system. The stem detection algorithm managed to identify 79.31\n% of the stems. The DBH estimation had a root mean square error (RMSE) of 3.33\ncm (12.79 %) and a bias of 1.01 cm (3.87 %) across all trees. For trees with a\nDBH less than 30 cm, the RMSE was 1.16 cm (5.74 %), and the bias was 0.13 cm\n(0.64 %). When considering the overall performance in terms of DBH accuracy,\nautonomy, and forest complexity, the proposed approach was superior compared to\nmethods proposed in the scientific literature. Results provided valuable\ninsights into autonomous forest reconstruction using drones, and several\nfurther development topics were proposed.\n","authors":["VÃ¤inÃ¶ Karjalainen","Niko KoivumÃ¤ki","Teemu Hakala","Jesse Muhojoki","Eric HyyppÃ¤","Anand George","Juha Suomalainen","Eija Honkavaara"],"pdf_url":"https://arxiv.org/pdf/2501.12073v1.pdf","comment":"35 pages, 13 Figures"},{"id":"http://arxiv.org/abs/2310.03505v2","updated":"2025-01-21T10:37:18Z","published":"2023-10-05T12:35:09Z","title":"RadaRays: Real-time Simulation of Rotating FMCW Radar for Mobile\n  Robotics via Hardware-accelerated Ray Tracing","summary":"  RadaRays allows for the accurate modeling and simulation of rotating FMCW\nradar sensors in complex environments, including the simulation of reflection,\nrefraction, and scattering of radar waves. Our software is able to handle large\nnumbers of objects and materials in real-time, making it suitable for use in a\nvariety of mobile robotics applications. We demonstrate the effectiveness of\nRadaRays through a series of experiments and show that it can more accurately\nreproduce the behavior of FMCW radar sensors in a variety of environments,\ncompared to the ray casting-based lidar-like simulations that are commonly used\nin simulators for autonomous driving such as CARLA. Our experiments\nadditionally serve as a valuable reference point for researchers to evaluate\ntheir own radar simulations. By using RadaRays, developers can significantly\nreduce the time and cost associated with prototyping and testing FMCW\nradar-based algorithms. We also provide a Gazebo plugin that makes our work\naccessible to the mobile robotics community.\n","authors":["Alexander Mock","Martin Magnusson","Joachim Hertzberg"],"pdf_url":"https://arxiv.org/pdf/2310.03505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12025v1","updated":"2025-01-21T10:34:04Z","published":"2025-01-21T10:34:04Z","title":"Low-Cost 3D printed, Biocompatible Ionic Polymer Membranes for Soft\n  Actuators","summary":"  Ionic polymer actuators, in essence, consist of ion exchange polymers\nsandwiched between layers of electrodes. They have recently gained recognition\nas promising candidates for soft actuators due to their lightweight nature,\nnoise-free operation, and low-driving voltages. However, the materials\ntraditionally utilized to develop them are often not human/environmentally\nfriendly. Thus, to address this issue, researchers have been focusing on\ndeveloping biocompatible versions of this actuator. Despite this, such\nactuators still face challenges in achieving high performance, in payload\ncapacity, bending capabilities, and response time. In this paper, we present a\nbiocompatible ionic polymer actuator whose membrane is fully 3D printed\nutilizing a direct ink writing method. The structure of the printed membranes\nconsists of biodegradable ionic fluid encapsulated within layers of activated\ncarbon polymers. From the microscopic observations of its structure, we\nconfirmed that the ionic polymer is well encapsulated. The actuators can\nachieve a bending performance of up to 124$^\\circ$ (curvature of 0.82\n$\\text{cm}^{-1}$), which, to our knowledge, is the highest curvature attained\nby any bending ionic polymer actuator to date. It can operate comfortably up to\na 2 Hz driving frequency and can achieve blocked forces of up to 0.76 mN. Our\nresults showcase a promising, high-performing biocompatible ionic polymer\nactuator, whose membrane can be easily manufactured in a single step using a\nstandard FDM 3D printer. This approach paves the way for creating customized\ndesigns for functional soft robotic applications, including human-interactive\ndevices, in the near future.\n","authors":["Nils TrÃ¼mpler","Ryo Kanno","Niu David","Anja Huch","Pham Huy Nguyen","Maksims Jurinovs","Gustav NystrÃ¶m","Sergejs Gaidukovs","Mirko Kovac"],"pdf_url":"https://arxiv.org/pdf/2501.12025v1.pdf","comment":"6 pages, 8 figures, Accepted in IEEE International Conference on Soft\n  Robotics 2025 (Robosoft)"},{"id":"http://arxiv.org/abs/2307.09105v3","updated":"2025-01-21T09:41:04Z","published":"2023-07-18T09:54:01Z","title":"Sampling-based Model Predictive Control Leveraging Parallelizable\n  Physics Simulations","summary":"  We present a method for sampling-based model predictive control that makes\nuse of a generic physics simulator as the dynamical model. In particular, we\npropose a Model Predictive Path Integral controller (MPPI), that uses the\nGPU-parallelizable IsaacGym simulator to compute the forward dynamics of a\nproblem. By doing so, we eliminate the need for explicit encoding of robot\ndynamics and contacts with objects for MPPI. Since no explicit dynamic modeling\nis required, our method is easily extendable to different objects and robots\nand allows one to solve complex navigation and contact-rich tasks. We\ndemonstrate the effectiveness of this method in several simulated and\nreal-world settings, among which mobile navigation with collision avoidance,\nnon-prehensile manipulation, and whole-body control for high-dimensional\nconfiguration spaces. This method is a powerful and accessible open-source tool\nto solve a large variety of contact-rich motion planning tasks.\n","authors":["Corrado Pezzato","Chadi Salmi","Elia Trevisan","Max Spahn","Javier Alonso-Mora","Carlos HernÃ¡ndez Corbato"],"pdf_url":"https://arxiv.org/pdf/2307.09105v3.pdf","comment":"Accepted for RA-L. Code and videos available at\n  https://autonomousrobots.nl/paper_websites/isaac-mppi"},{"id":"http://arxiv.org/abs/2410.06052v4","updated":"2025-01-21T08:48:08Z","published":"2024-10-08T13:54:04Z","title":"Concurrent-Learning Based Relative Localization in Shape Formation of\n  Robot Swarms (Extended version)","summary":"  In this paper, we address the shape formation problem for massive robot\nswarms in environments where external localization systems are unavailable.\nAchieving this task effectively with solely onboard measurements is still\nscarcely explored and faces some practical challenges. To solve this\nchallenging problem, we propose the following novel results. Firstly, to\nestimate the relative positions among neighboring robots, a concurrent-learning\nbased estimator is proposed. It relaxes the persistent excitation condition\nrequired in the classical ones such as least-square estimator. Secondly, we\nintroduce a finite-time agreement protocol to determine the shape location.\nThis is achieved by estimating the relative position between each robot and a\nrandomly assigned seed robot. The initial position of the seed one marks the\nshape location. Thirdly, based on the theoretical results of the relative\nlocalization, a novel behavior-based control strategy is devised. This strategy\nnot only enables adaptive shape formation of large group of robots but also\nenhances the observability of inter-robot relative localization. Numerical\nsimulation results are provided to verify the performance of our proposed\nstrategy compared to the state-of-the-art ones. Additionally, outdoor\nexperiments on real robots further demonstrate the practical effectiveness and\nrobustness of our methods.\n","authors":["Jinhu LÃ¼","Kunrui Ze","Shuoyu Yue","Kexin Liu","Wei Wang","Guibin Sun"],"pdf_url":"https://arxiv.org/pdf/2410.06052v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11945v1","updated":"2025-01-21T07:41:12Z","published":"2025-01-21T07:41:12Z","title":"Learning to Hop for a Single-Legged Robot with Parallel Mechanism","summary":"  This work presents the application of reinforcement learning to improve the\nperformance of a highly dynamic hopping system with a parallel mechanism.\nUnlike serial mechanisms, parallel mechanisms can not be accurately simulated\ndue to the complexity of their kinematic constraints and closed-loop\nstructures. Besides, learning to hop suffers from prolonged aerial phase and\nthe sparse nature of the rewards. To address them, we propose a learning\nframework to encode long-history feedback to account for the under-actuation\nbrought by the prolonged aerial phase. In the proposed framework, we also\nintroduce a simplified serial configuration for the parallel design to avoid\ndirectly simulating parallel structure during the training. A torque-level\nconversion is designed to deal with the parallel-serial conversion to handle\nthe sim-to-real issue. Simulation and hardware experiments have been conducted\nto validate this framework.\n","authors":["Hongbo Zhang","Xiangyu Chu","Yanlin Chen","Yunxi Tang","Linzhu Yue","Yun-Hui Liu","Kwok Wai Samuel Au"],"pdf_url":"https://arxiv.org/pdf/2501.11945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11938v1","updated":"2025-01-21T07:27:14Z","published":"2025-01-21T07:27:14Z","title":"Navigating Robot Swarm Through a Virtual Tube with Flow-Adaptive\n  Distribution Control","summary":"  With the rapid development of robot swarm technology and its diverse\napplications, navigating robot swarms through complex environments has emerged\nas a critical research direction. To ensure safe navigation and avoid potential\ncollisions with obstacles, the concept of virtual tubes has been introduced to\ndefine safe and navigable regions. However, current control methods in virtual\ntubes face the congestion issues, particularly in narrow virtual tubes with low\nthroughput. To address these challenges, we first originally introduce the\nconcepts of virtual tube area and flow capacity, and develop an new evolution\nmodel for the spatial density function. Next, we propose a novel control method\nthat combines a modified artificial potential field (APF) for swarm navigation\nand density feedback control for distribution regulation, under which a\nsaturated velocity command is designed. Then, we generate a global velocity\nfield that not only ensures collision-free navigation through the virtual tube,\nbut also achieves locally input-to-state stability (LISS) for density tracking\nerrors, both of which are rigorously proven. Finally, numerical simulations and\nrealistic applications validate the effectiveness and advantages of the\nproposed method in managing robot swarms within narrow virtual tubes.\n","authors":["Yongwei Zhang","Shuli Lv","Kairong Liu","Quanyi Liang","Quan Quan","Zhikun She"],"pdf_url":"https://arxiv.org/pdf/2501.11938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11930v1","updated":"2025-01-21T07:08:53Z","published":"2025-01-21T07:08:53Z","title":"Nocturnal eye inspired liquid to gas phase change soft actuator with\n  Laser-Induced-Graphene: enhanced environmental light harvesting and\n  photothermal conversion","summary":"  Robotic systems' mobility is constrained by power sources and wiring. While\npneumatic actuators remain tethered to air supplies, we developed a new\nactuator utilizing light energy. Inspired by nocturnal animals' eyes, we\ndesigned a bilayer soft actuator incorporating Laser-Induced Graphene (LIG) on\nthe inner surface of a silicone layer. This design maintains silicone's\ntransparency and flexibility while achieving 54% faster response time compared\nto conventional actuators through enhanced photothermal conversion.\n","authors":["Maina Sogabe","Youhyun Kim","Kenji Kawashima"],"pdf_url":"https://arxiv.org/pdf/2501.11930v1.pdf","comment":"23pages, 8 figures, journal paper"},{"id":"http://arxiv.org/abs/2310.20151v2","updated":"2025-01-21T06:26:43Z","published":"2023-10-31T03:37:11Z","title":"Multi-Agent Consensus Seeking via Large Language Models","summary":"  Multi-agent systems driven by large language models (LLMs) have shown\npromising abilities for solving complex tasks in a collaborative manner. This\nwork considers a fundamental problem in multi-agent collaboration: consensus\nseeking. When multiple agents work together, we are interested in how they can\nreach a consensus through inter-agent negotiation. To that end, this work\nstudies a consensus-seeking task where the state of each agent is a numerical\nvalue and they negotiate with each other to reach a consensus value. It is\nrevealed that when not explicitly directed on which strategy should be adopted,\nthe LLM-driven agents primarily use the average strategy for consensus seeking\nalthough they may occasionally use some other strategies. Moreover, this work\nanalyzes the impact of the agent number, agent personality, and network\ntopology on the negotiation process. The findings reported in this work can\npotentially lay the foundations for understanding the behaviors of LLM-driven\nmulti-agent systems for solving more complex tasks. Furthermore, LLM-driven\nconsensus seeking is applied to a multi-robot aggregation task. This\napplication demonstrates the potential of LLM-driven agents to achieve\nzero-shot autonomous planning for multi-robot collaboration tasks. Project\nwebsite: windylab.github.io/ConsensusLLM/.\n","authors":["Huaben Chen","Wenkang Ji","Lufeng Xu","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.20151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00204v5","updated":"2025-01-21T06:08:46Z","published":"2024-03-30T00:46:43Z","title":"AirPilot: Interpretable PPO-based DRL Auto-Tuned Nonlinear PID Drone\n  Controller for Robust Autonomous Flights","summary":"  Navigation precision, speed and stability are crucial for safe Unmanned\nAerial Vehicle (UAV) flight maneuvers and effective flight mission executions\nin dynamic environments. Different flight missions may have varying objectives,\nsuch as minimizing energy consumption, achieving precise positioning, or\nmaximizing speed. A controller that can adapt to different objectives on the\nfly is highly valuable. Proportional Integral Derivative (PID) controllers are\none of the most popular and widely used control algorithms for drones and other\ncontrol systems, but their linear control algorithm fails to capture the\nnonlinear nature of the dynamic wind conditions and complex drone system.\nManually tuning the PID gains for various missions can be time-consuming and\nrequires significant expertise. This paper aims to revolutionize drone flight\ncontrol by presenting the AirPilot, a nonlinear Deep Reinforcement Learning\n(DRL) - enhanced Proportional Integral Derivative (PID) drone controller using\nProximal Policy Optimization (PPO). AirPilot controller combines the simplicity\nand effectiveness of traditional PID control with the adaptability, learning\ncapability, and optimization potential of DRL. This makes it better suited for\nmodern drone applications where the environment is dynamic, and\nmission-specific performance demands are high. We employed a COEX Clover\nautonomous drone for training the DRL agent within the simulator and\nimplemented it in a real-world lab setting, which marks a significant milestone\nas one of the first attempts to apply a DRL-based flight controller on an\nactual drone. Airpilot is capable of reducing the navigation error of the\ndefault PX4 PID position controller by 90%, improving effective navigation\nspeed of a fine-tuned PID controller by 21%, reducing settling time and\novershoot by 17% and 16% respectively.\n","authors":["Junyang Zhang","Cristian Emanuel Ocampo Rivera","Kyle Tyni","Steven Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.00204v5.pdf","comment":"9 pages, 20 figures"},{"id":"http://arxiv.org/abs/2408.07855v3","updated":"2025-01-21T05:59:47Z","published":"2024-08-14T23:48:26Z","title":"Complementarity-Free Multi-Contact Modeling and Optimization for\n  Dexterous Manipulation","summary":"  A significant barrier preventing model-based methods from achieving real-time\nand versatile dexterous robotic manipulation is the inherent complexity of\nmulti-contact dynamics. Traditionally formulated as complementarity models,\nmulti-contact dynamics introduces non-smoothness and combinatorial complexity,\ncomplicating contact-rich planning and optimization. In this paper, we\ncircumvent these challenges by introducing a lightweight yet capable\nmulti-contact model. Our new model, derived from the duality of\noptimization-based contact models, dispenses with the complementarity\nconstructs entirely, providing computational advantages such as closed-form\ntime stepping, differentiability, automatic satisfaction with Coulomb friction\nlaw, and minimal hyperparameter tuning. We demonstrate the effectiveness and\nefficiency of the model for planning and control in a range of challenging\ndexterous manipulation tasks, including fingertip 3D in-air manipulation,\nTriFinger in-hand manipulation, and Allegro hand on-palm reorientation, all\nperformed with diverse objects. Our method consistently achieves\nstate-of-the-art results: (I) a 96.5% average success rate across all objects\nand tasks, (II) high manipulation accuracy with an average reorientation error\nof 11{\\deg} and position error of 7.8mm, and (III) contact-implicit model\npredictive control running at 50-100 Hz for all objects and tasks. These\nresults are achieved with minimal hyperparameter tuning.\n","authors":["Wanxin Jin"],"pdf_url":"https://arxiv.org/pdf/2408.07855v3.pdf","comment":"Video demo: https://youtu.be/NsL4hbSXvFg"},{"id":"http://arxiv.org/abs/2310.15846v4","updated":"2025-01-21T05:44:03Z","published":"2023-10-24T13:58:10Z","title":"Optimal Spatial-Temporal Triangulation for Bearing-Only Cooperative\n  Motion Estimation","summary":"  Vision-based cooperative motion estimation is an important problem for many\nmulti-robot systems such as cooperative aerial target pursuit. This problem can\nbe formulated as bearing-only cooperative motion estimation, where the visual\nmeasurement is modeled as a bearing vector pointing from the camera to the\ntarget. The conventional approaches for bearing-only cooperative estimation are\nmainly based on the framework distributed Kalman filtering (DKF). In this\npaper, we propose a new optimal bearing-only cooperative estimation algorithm,\nnamed spatial-temporal triangulation, based on the method of distributed\nrecursive least squares, which provides a more flexible framework for designing\ndistributed estimators than DKF. The design of the algorithm fully incorporates\nall the available information and the specific triangulation geometric\nconstraint. As a result, the algorithm has superior estimation performance than\nthe state-of-the-art DKF algorithms in terms of both accuracy and convergence\nspeed as verified by numerical simulation. We rigorously prove the exponential\nconvergence of the proposed algorithm. Moreover, to verify the effectiveness of\nthe proposed algorithm under practical challenging conditions, we develop a\nvision-based cooperative aerial target pursuit system, which is the first of\nsuch fully autonomous systems so far to the best of our knowledge.\n","authors":["Canlun Zheng","Yize Mi","Hanqing Guo","Huaben Chen","Zhiyun Lin","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.15846v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11893v1","updated":"2025-01-21T05:03:06Z","published":"2025-01-21T05:03:06Z","title":"DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM","summary":"  Traditional Visual Simultaneous Localization and Mapping (vSLAM) systems\nfocus solely on static scene structures, overlooking dynamic elements in the\nenvironment. Although effective for accurate visual odometry in complex\nscenarios, these methods discard crucial information about moving objects. By\nincorporating this information into a Dynamic SLAM framework, the motion of\ndynamic entities can be estimated, enhancing navigation whilst ensuring\naccurate localization. However, the fundamental formulation of Dynamic SLAM\nremains an open challenge, with no consensus on the optimal approach for\naccurate motion estimation within a SLAM pipeline. Therefore, we developed\nDynoSAM, an open-source framework for Dynamic SLAM that enables the efficient\nimplementation, testing, and comparison of various Dynamic SLAM optimization\nformulations. DynoSAM integrates static and dynamic measurements into a unified\noptimization problem solved using factor graphs, simultaneously estimating\ncamera poses, static scene, object motion or poses, and object structures. We\nevaluate DynoSAM across diverse simulated and real-world datasets, achieving\nstate-of-the-art motion estimation in indoor and outdoor environments, with\nsubstantial improvements over existing systems. Additionally, we demonstrate\nDynoSAM utility in downstream applications, including 3D reconstruction of\ndynamic scenes and trajectory prediction, thereby showcasing potential for\nadvancing dynamic object-aware SLAM systems. DynoSAM is open-sourced at\nhttps://github.com/ACFR-RPG/DynOSAM.\n","authors":["Jesse Morris","Yiduo Wang","Mikolaj Kliniewski","Viorela Ila"],"pdf_url":"https://arxiv.org/pdf/2501.11893v1.pdf","comment":"20 pages, 10 figures. Submitted to T-RO Visual SLAM SI 2025"},{"id":"http://arxiv.org/abs/2501.11887v1","updated":"2025-01-21T04:53:17Z","published":"2025-01-21T04:53:17Z","title":"Connection-Coordination Rapport (CCR) Scale: A Dual-Factor Scale to\n  Measure Human-Robot Rapport","summary":"  Robots, particularly in service and companionship roles, must develop\npositive relationships with people they interact with regularly to be\nsuccessful. These positive human-robot relationships can be characterized as\nestablishing \"rapport,\" which indicates mutual understanding and interpersonal\nconnection that form the groundwork for successful long-term human-robot\ninteraction. However, the human-robot interaction research literature lacks\nscale instruments to assess human-robot rapport in a variety of situations. In\nthis work, we developed the 18-item Connection-Coordination Rapport (CCR) Scale\nto measure human-robot rapport. We first ran Study 1 (N = 288) where online\nparticipants rated videos of human-robot interactions using a set of candidate\nitems. Our Study 1 results showed the discovery of two factors in our scale,\nwhich we named \"Connection\" and \"Coordination.\" We then evaluated this scale by\nrunning Study 2 (N = 201) where online participants rated a new set of\nhuman-robot interaction videos with our scale and an existing rapport scale\nfrom virtual agents research for comparison. We also validated our scale by\nreplicating a prior in-person human-robot interaction study, Study 3 (N = 44),\nand found that rapport is rated significantly greater when participants\ninteracted with a responsive robot (responsive condition) as opposed to an\nunresponsive robot (unresponsive condition). Results from these studies\ndemonstrate high reliability and validity for the CCR scale, which can be used\nto measure rapport in both first-person and third-person perspectives. We\nencourage the adoption of this scale in future studies to measure rapport in a\nvariety of human-robot interactions.\n","authors":["Ting-Han Lin","Hannah Dinner","Tsz Long Leung","Bilge Mutlu","J. Gregory Trafton","Sarah Sebo"],"pdf_url":"https://arxiv.org/pdf/2501.11887v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.08907v3","updated":"2025-01-21T04:09:42Z","published":"2024-07-12T00:57:36Z","title":"Tightly-Coupled LiDAR-IMU-Wheel Odometry with an Online Neural Kinematic\n  Model Learning via Factor Graph Optimization","summary":"  Environments lacking geometric features (e.g., tunnels and long straight\ncorridors) are challenging for LiDAR-based odometry algorithms because LiDAR\npoint clouds degenerate in such environments. For wheeled robots, a wheel\nkinematic model (i.e., wheel odometry) can improve the reliability of the\nodometry estimation. However, the kinematic model suffers from complex motions\n(e.g., wheel slippage, lateral movement) in the case of skid-steering robots\nparticularly because this robot model rotates by skidding its wheels.\nFurthermore, these errors change nonlinearly when the wheel slippage is large\n(e.g., drifting) and are subject to terrain-dependent parameters. To\nsimultaneously tackle point cloud degeneration and the kinematic model errors,\nwe developed a LiDAR-IMU-wheel odometry algorithm incorporating online training\nof a neural network that learns the kinematic model of wheeled robots with\nnonlinearity. We propose to train the neural network online on a factor graph\nalong with robot states, allowing the learning-based kinematic model to adapt\nto the current terrain condition. The proposed method jointly solves online\ntraining of the neural network and LiDARIMUwheel odometry on a unified factor\ngraph to retain the consistency of all those constraints. Through experiments,\nwe first verified that the proposed network adapted to a changing environment,\nresulting in an accurate odometry estimation across different environments. We\nthen confirmed that the proposed odometry estimation algorithm was robust\nagainst point cloud degeneration and nonlinearity (e.g., large wheel slippage\nby drifting) of the kinematic model.\n","authors":["Taku Okawara","Kenji Koide","Shuji Oishi","Masashi Yokozuka","Atsuhiko Banno","Kentaro Uno","Kazuya Yoshida"],"pdf_url":"https://arxiv.org/pdf/2407.08907v3.pdf","comment":"https://youtu.be/CvRVhdda7Cw"},{"id":"http://arxiv.org/abs/2408.11051v2","updated":"2025-01-21T04:06:09Z","published":"2024-08-20T17:57:46Z","title":"FLAME: Learning to Navigate with Multimodal LLM in Urban Environments","summary":"  Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for route summarization, and end-to-end\ntraining on VLN datasets. The augmented datasets are synthesized automatically.\nExperimental results demonstrate FLAME's superiority over existing methods,\nsurpassing state-of-the-art methods by a 7.3% increase in task completion on\nTouchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs)\nin complex navigation tasks, representing an advancement towards applications\nof MLLMs in the field of embodied intelligence.\n","authors":["Yunzhe Xu","Yiyuan Pan","Zhe Liu","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.11051v2.pdf","comment":"Accepted to AAAI 2025 (Oral)"},{"id":"http://arxiv.org/abs/2405.16960v2","updated":"2025-01-21T03:49:48Z","published":"2024-05-27T08:55:17Z","title":"DCPI-Depth: Explicitly Infusing Dense Correspondence Prior to\n  Unsupervised Monocular Depth Estimation","summary":"  There has been a recent surge of interest in learning to perceive depth from\nmonocular videos in an unsupervised fashion. A key challenge in this field is\nachieving robust and accurate depth estimation in challenging scenarios,\nparticularly in regions with weak textures or where dynamic objects are\npresent. This study makes three major contributions by delving deeply into\ndense correspondence priors to provide existing frameworks with explicit\ngeometric constraints. The first novelty is a contextual-geometric depth\nconsistency loss, which employs depth maps triangulated from dense\ncorrespondences based on estimated ego-motion to guide the learning of depth\nperception from contextual information, since explicitly triangulated depth\nmaps capture accurate relative distances among pixels. The second novelty\narises from the observation that there exists an explicit, deducible\nrelationship between optical flow divergence and depth gradient. A differential\nproperty correlation loss is, therefore, designed to refine depth estimation\nwith a specific emphasis on local variations. The third novelty is a\nbidirectional stream co-adjustment strategy that enhances the interaction\nbetween rigid and optical flows, encouraging the former towards more accurate\ncorrespondence and making the latter more adaptable across various scenarios\nunder the static scene hypotheses. DCPI-Depth, a framework that incorporates\nall these innovative components and couples two bidirectional and collaborative\nstreams, achieves state-of-the-art performance and generalizability across\nmultiple public datasets, outperforming all existing prior arts. Specifically,\nit demonstrates accurate depth estimation in texture-less and dynamic regions,\nand shows more reasonable smoothness. Our source code will be publicly\navailable at mias.group/DCPI-Depth upon publication.\n","authors":["Mengtan Zhang","Yi Feng","Qijun Chen","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2405.16960v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.18313v5","updated":"2025-01-21T02:38:32Z","published":"2024-09-26T21:44:11Z","title":"Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and\n  Generation","summary":"  There is no limit to how much a robot might explore and learn, but all of\nthat knowledge needs to be searchable and actionable. Within language research,\nretrieval augmented generation (RAG) has become the workhorse of large-scale\nnon-parametric knowledge; however, existing techniques do not directly transfer\nto the embodied domain, which is multimodal, where data is highly correlated,\nand perception requires abstraction. To address these challenges, we introduce\nEmbodied-RAG, a framework that enhances the foundational model of an embodied\nagent with a non-parametric memory system capable of autonomously constructing\nhierarchical knowledge for both navigation and language generation.\nEmbodied-RAG handles a full range of spatial and semantic resolutions across\ndiverse environments and query types, whether for a specific object or a\nholistic description of ambiance. At its core, Embodied-RAG's memory is\nstructured as a semantic forest, storing language descriptions at varying\nlevels of detail. This hierarchical organization allows the system to\nefficiently generate context-sensitive outputs across different robotic\nplatforms. We demonstrate that Embodied-RAG effectively bridges RAG to the\nrobotics domain, successfully handling over 250 explanation and navigation\nqueries across kilometer-level environments, highlighting its promise as a\ngeneral-purpose non-parametric system for embodied agents.\n","authors":["Quanting Xie","So Yeon Min","Pengliang Ji","Yue Yang","Tianyi Zhang","Kedi Xu","Aarav Bajaj","Ruslan Salakhutdinov","Matthew Johnson-Roberson","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2409.18313v5.pdf","comment":"Web: https://quanting-xie.github.io/Embodied-RAG-web/"},{"id":"http://arxiv.org/abs/2501.11803v1","updated":"2025-01-21T00:44:18Z","published":"2025-01-21T00:44:18Z","title":"Automating High Quality RT Planning at Scale","summary":"  Radiotherapy (RT) planning is complex, subjective, and time-intensive.\nAdvances in artificial intelligence (AI) promise to improve its precision,\nefficiency, and consistency, but progress is often limited by the scarcity of\nlarge, standardized datasets. To address this, we introduce the Automated\nIterative RT Planning (AIRTP) system, a scalable solution for generating\nhigh-quality treatment plans. This scalable solution is designed to generate\nsubstantial volumes of consistently high-quality treatment plans, overcoming a\nkey obstacle in the advancement of AI-driven RT planning. Our AIRTP pipeline\nadheres to clinical guidelines and automates essential steps, including\norgan-at-risk (OAR) contouring, helper structure creation, beam setup,\noptimization, and plan quality improvement, using AI integrated with RT\nplanning software like Eclipse of Varian. Furthermore, a novel approach for\ndetermining optimization parameters to reproduce 3D dose distributions, i.e. a\nmethod to convert dose predictions to deliverable treatment plans constrained\nby machine limitations. A comparative analysis of plan quality reveals that our\nautomated pipeline produces treatment plans of quality comparable to those\ngenerated manually, which traditionally require several hours of labor per\nplan. Committed to public research, the first data release of our AIRTP\npipeline includes nine cohorts covering head-and-neck and lung cancer sites to\nsupport an AAPM 2025 challenge. This data set features more than 10 times the\nnumber of plans compared to the largest existing well-curated public data set\nto our best knowledge.\nRepo:{https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge}\n","authors":["Riqiang Gao","Mamadou Diallo","Han Liu","Anthony Magliari","Jonathan Sackett","Wilko Verbakel","Sandra Meyers","Masoud Zarepisheh","Rafe Mcbeth","Simon Arberet","Martin Kraus","Florin C. Ghesu","Ali Kamen"],"pdf_url":"https://arxiv.org/pdf/2501.11803v1.pdf","comment":"Related to GDP-HMM grand challenge"},{"id":"http://arxiv.org/abs/2501.12536v1","updated":"2025-01-21T22:59:50Z","published":"2025-01-21T22:59:50Z","title":"Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs","summary":"  This paper presents the development of a comprehensive dataset capturing\ninteractions between Autonomous Vehicles (AVs) and traffic control devices,\nspecifically traffic lights and stop signs. Derived from the Waymo Motion\ndataset, our work addresses a critical gap in the existing literature by\nproviding real-world trajectory data on how AVs navigate these traffic control\ndevices. We propose a methodology for identifying and extracting relevant\ninteraction trajectory data from the Waymo Motion dataset, incorporating over\n37,000 instances with traffic lights and 44,000 with stop signs. Our\nmethodology includes defining rules to identify various interaction types,\nextracting trajectory data, and applying a wavelet-based denoising method to\nsmooth the acceleration and speed profiles and eliminate anomalous values,\nthereby enhancing the trajectory quality. Quality assessment metrics indicate\nthat trajectories obtained in this study have anomaly proportions in\nacceleration and jerk profiles reduced to near-zero levels across all\ninteraction categories. By making this dataset publicly available, we aim to\naddress the current gap in datasets containing AV interaction behaviors with\ntraffic lights and signs. Based on the organized and published dataset, we can\ngain a more in-depth understanding of AVs' behavior when interacting with\ntraffic lights and signs. This will facilitate research on AV integration into\nexisting transportation infrastructures and networks, supporting the\ndevelopment of more accurate behavioral models and simulation tools.\n","authors":["Zheng Li","Zhipeng Bao","Haoming Meng","Haotian Shi","Qianwen Li","Handong Yao","Xiaopeng Li"],"pdf_url":"https://arxiv.org/pdf/2501.12536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11783v2","updated":"2025-01-21T21:46:26Z","published":"2024-10-15T17:02:32Z","title":"LatentBKI: Open-Dictionary Continuous Mapping in Visual-Language Latent\n  Spaces with Quantifiable Uncertainty","summary":"  This paper introduces a novel probabilistic mapping algorithm, LatentBKI,\nwhich enables open-vocabulary mapping with quantifiable uncertainty.\nTraditionally, semantic mapping algorithms focus on a fixed set of semantic\ncategories which limits their applicability for complex robotic tasks.\nVision-Language (VL) models have recently emerged as a technique to jointly\nmodel language and visual features in a latent space, enabling semantic\nrecognition beyond a predefined, fixed set of semantic classes. LatentBKI\nrecurrently incorporates neural embeddings from VL models into a voxel map with\nquantifiable uncertainty, leveraging the spatial correlations of nearby\nobservations through Bayesian Kernel Inference (BKI). LatentBKI is evaluated\nagainst similar explicit semantic mapping and VL mapping frameworks on the\npopular Matterport3D and Semantic KITTI datasets, demonstrating that LatentBKI\nmaintains the probabilistic benefits of continuous mapping with the additional\nbenefit of open-dictionary queries. Real-world experiments demonstrate\napplicability to challenging indoor environments.\n","authors":["Joey Wilson","Ruihan Xu","Yile Sun","Parker Ewen","Minghan Zhu","Kira Barton","Maani Ghaffari"],"pdf_url":"https://arxiv.org/pdf/2410.11783v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01737v2","updated":"2025-01-21T21:30:18Z","published":"2024-08-03T10:39:42Z","title":"Tightly Coupled SLAM with Imprecise Architectural Plans","summary":"  Robots navigating indoor environments often have access to architectural\nplans, which can serve as prior knowledge to enhance their localization and\nmapping capabilities. While some SLAM algorithms leverage these plans for\nglobal localization in real-world environments, they typically overlook a\ncritical challenge: the \"as-planned\" architectural designs frequently deviate\nfrom the \"as-built\" real-world environments. To address this gap, we present a\nnovel algorithm that tightly couples LIDAR-based simultaneous localization and\nmapping with architectural plans under the presence of deviations. Our method\nutilizes a multi-layered semantic representation to not only localize the\nrobot, but also to estimate global alignment and structural deviations between\n\"as-planned\" and as-built environments in real-time. To validate our approach,\nwe performed experiments in simulated and real datasets demonstrating\nrobustness to structural deviations up to 35 cm and 15 degrees. On average, our\nmethod achieves 43% less localization error than baselines in simulated\nenvironments, while in real environments, the as-built 3D maps show 7% lower\naverage alignment error\n","authors":["Muhammad Shaheer","Jose Andres Millan-Romera","Hriday Bavle","Marco Giberna","Jose Luis Sanchez-Lopez","Javier Civera","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2408.01737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12493v1","updated":"2025-01-21T20:43:33Z","published":"2025-01-21T20:43:33Z","title":"ELEGNT: Expressive and Functional Movement Design for\n  Non-anthropomorphic Robot","summary":"  Nonverbal behaviors such as posture, gestures, and gaze are essential for\nconveying internal states, both consciously and unconsciously, in human\ninteraction. For robots to interact more naturally with humans, robot movement\ndesign should likewise integrate expressive qualities, such as intention,\nattention, and emotions, alongside traditional functional considerations like\ntask fulfillment and time efficiency. In this paper, we present the design and\nprototyping of a lamp-like robot that explores the interplay between functional\nand expressive objectives in movement design. Using a research-through-design\nmethodology, we document the hardware design process, define expressive\nmovement primitives, and outline a set of interaction scenario storyboards. We\npropose a framework that incorporates both functional and expressive utilities\nduring movement generation, and implement the robot behavior sequences in\ndifferent function- and social- oriented tasks. Through a user study comparing\nexpression-driven versus function-driven movements across six task scenarios,\nour findings indicate that expression-driven movements significantly enhance\nuser engagement and perceived robot qualities. This effect is especially\npronounced in social-oriented tasks.\n","authors":["Yuhan Hu","Peide Huang","Mouli Sivapurapu","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.12493v1.pdf","comment":"13 pages, manuscript under review"},{"id":"http://arxiv.org/abs/2501.12482v1","updated":"2025-01-21T20:20:34Z","published":"2025-01-21T20:20:34Z","title":"TOFFE -- Temporally-binned Object Flow from Events for High-speed and\n  Energy-Efficient Object Detection and Tracking","summary":"  Object detection and tracking is an essential perception task for enabling\nfully autonomous navigation in robotic systems. Edge robot systems such as\nsmall drones need to execute complex maneuvers at high-speeds with limited\nresources, which places strict constraints on the underlying algorithms and\nhardware. Traditionally, frame-based cameras are used for vision-based\nperception due to their rich spatial information and simplified synchronous\nsensing capabilities. However, obtaining detailed information across frames\nincurs high energy consumption and may not even be required. In addition, their\nlow temporal resolution renders them ineffective in high-speed motion\nscenarios. Event-based cameras offer a biologically-inspired solution to this\nby capturing only changes in intensity levels at exceptionally high temporal\nresolution and low power consumption, making them ideal for high-speed motion\nscenarios. However, their asynchronous and sparse outputs are not natively\nsuitable with conventional deep learning methods. In this work, we propose\nTOFFE, a lightweight hybrid framework for performing event-based object motion\nestimation (including pose, direction, and speed estimation), referred to as\nObject Flow. TOFFE integrates bio-inspired Spiking Neural Networks (SNNs) and\nconventional Analog Neural Networks (ANNs), to efficiently process events at\nhigh temporal resolutions while being simple to train. Additionally, we present\na novel event-based synthetic dataset involving high-speed object motion to\ntrain TOFFE. Our experimental results show that TOFFE achieves 5.7x/8.3x\nreduction in energy consumption and 4.6x/5.8x reduction in latency on edge\nGPU(Jetson TX2)/hybrid hardware(Loihi-2 and Jetson TX2), compared to previous\nevent-based object detection baselines.\n","authors":["Adarsh Kumar Kosta","Amogh Joshi","Arjun Roy","Rohan Kumar Manna","Manish Nagaraj","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2501.12482v1.pdf","comment":"8 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.13960v1","updated":"2025-01-21T07:57:07Z","published":"2025-01-21T07:57:07Z","title":"LiCAR: pseudo-RGB LiDAR image for CAR segmentation","summary":"  With the advancement of computing resources, an increasing number of Neural\nNetworks (NNs) are appearing for image detection and segmentation appear.\nHowever, these methods usually accept as input a RGB 2D image. On the other\nside, Light Detection And Ranging (LiDAR) sensors with many layers provide\nimages that are similar to those obtained from a traditional low resolution RGB\ncamera. Following this principle, a new dataset for segmenting cars in\npseudo-RGB images has been generated. This dataset combines the information\ngiven by the LiDAR sensor into a Spherical Range Image (SRI), concretely the\nreflectivity, near infrared and signal intensity 2D images. These images are\nthen fed into instance segmentation NNs. These NNs segment the cars that appear\nin these images, having as result a Bounding Box (BB) and mask precision of 88%\nand 81.5% respectively with You Only Look Once (YOLO)-v8 large. By using this\nsegmentation NN, some trackers have been applied so as to follow each car\nsegmented instance along a video feed, having great performance in real world\nexperiments.\n","authors":["Ignacio de Loyola PÃ¡ez-Ubieta","Edison P. Velasco-SÃ¡nchez","Santiago T. Puente"],"pdf_url":"https://arxiv.org/pdf/2501.13960v1.pdf","comment":"This is a preprint version of the work accepted at 5th International\n  Conference on Robotics, Computer Vision and Intelligent Systems (ROBOVIS\n  2025)"},{"id":"http://arxiv.org/abs/2501.14824v1","updated":"2025-01-21T16:37:17Z","published":"2025-01-21T16:37:17Z","title":"A causal learning approach to in-orbit inertial parameter estimation for\n  multi-payload deployers","summary":"  This paper discusses an approach to inertial parameter estimation for the\ncase of cargo carrying spacecraft that is based on causal learning, i.e.\nlearning from the responses of the spacecraft, under actuation. Different\nspacecraft configurations (inertial parameter sets) are simulated under\ndifferent actuation profiles, in order to produce an optimised time-series\nclustering classifier that can be used to distinguish between them. The\nactuation is comprised of finite sequences of constant inputs that are applied\nin order, based on typical actuators available. By learning from the system's\nresponses across multiple input sequences, and then applying measures of\ntime-series similarity and F1-score, an optimal actuation sequence can be\nchosen either for one specific system configuration or for the overall set of\npossible configurations. This allows for both estimation of the inertial\nparameter set without any prior knowledge of state, as well as validation of\ntransitions between different configurations after a deployment event. The\noptimisation of the actuation sequence is handled by a reinforcement learning\nmodel that uses the proximal policy optimisation (PPO) algorithm, by repeatedly\ntrying different sequences and evaluating the impact on classifier performance\naccording to a multi-objective metric.\n","authors":["Konstantinos Platanitis","Miguel Arana-Catania","Saurabh Upadhyay","Leonard Felicetti"],"pdf_url":"https://arxiv.org/pdf/2501.14824v1.pdf","comment":"10 pages, 18 figures, 1 table. Presented in 75th International\n  Astronautical Congress (IAC), Milan, Italy, 14-18 October 2024"}]},"2025-01-22T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2501.13072v1","updated":"2025-01-22T18:34:51Z","published":"2025-01-22T18:34:51Z","title":"AdaWM: Adaptive World Model based Planning for Autonomous Driving","summary":"  World model based reinforcement learning (RL) has emerged as a promising\napproach for autonomous driving, which learns a latent dynamics model and uses\nit to train a planning policy. To speed up the learning process, the\npretrain-finetune paradigm is often used, where online RL is initialized by a\npretrained model and a policy learned offline. However, naively performing such\ninitialization in RL may result in dramatic performance degradation during the\nonline interactions in the new task. To tackle this challenge, we first analyze\nthe performance degradation and identify two primary root causes therein: the\nmismatch of the planning policy and the mismatch of the dynamics model, due to\ndistribution shift. We further analyze the effects of these factors on\nperformance degradation during finetuning, and our findings reveal that the\nchoice of finetuning strategies plays a pivotal role in mitigating these\neffects. We then introduce AdaWM, an Adaptive World Model based planning\nmethod, featuring two key steps: (a) mismatch identification, which quantifies\nthe mismatches and informs the finetuning strategy, and (b) alignment-driven\nfinetuning, which selectively updates either the policy or the model as needed\nusing efficient low-rank updates. Extensive experiments on the challenging\nCARLA driving tasks demonstrate that AdaWM significantly improves the\nfinetuning process, resulting in more robust and efficient performance in\nautonomous driving systems.\n","authors":["Hang Wang","Xin Ye","Feng Tao","Abhirup Mallik","Burhaneddin Yaman","Liu Ren","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.13072v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2403.01536v2","updated":"2025-01-22T17:06:19Z","published":"2024-03-03T15:30:31Z","title":"Fast Ergodic Search with Kernel Functions","summary":"  Ergodic search enables optimal exploration of an information distribution\nwhile guaranteeing the asymptotic coverage of the search space. However,\ncurrent methods typically have exponential computation complexity in the search\nspace dimension and are restricted to Euclidean space. We introduce a\ncomputationally efficient ergodic search method. Our contributions are\ntwo-fold. First, we develop a kernel-based ergodic metric and generalize it\nfrom Euclidean space to Lie groups. We formally prove the proposed metric is\nconsistent with the standard ergodic metric while guaranteeing linear\ncomplexity in the search space dimension. Secondly, we derive the first-order\noptimality condition of the kernel ergodic metric for nonlinear systems, which\nenables efficient trajectory optimization. Comprehensive numerical benchmarks\nshow that the proposed method is at least two orders of magnitude faster than\nthe state-of-the-art algorithm. Finally, we demonstrate the proposed algorithm\nwith a peg-in-hole insertion task. We formulate the problem as a coverage task\nin the space of SE(3) and use a 30-second-long human demonstration as the prior\ndistribution for ergodic coverage. Ergodicity guarantees the asymptotic\nsolution of the peg-in-hole problem so long as the solution resides within the\nprior information distribution, which is seen in the 100% success rate.\n","authors":["Max Muchen Sun","Ayush Gaggar","Peter Trautman","Todd Murphey"],"pdf_url":"https://arxiv.org/pdf/2403.01536v2.pdf","comment":"Accepted to IEEE Transactions on Robotics (T-RO). 20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2501.12869v1","updated":"2025-01-22T13:25:52Z","published":"2025-01-22T13:25:52Z","title":"Drone Carrier: An Integrated Unmanned Surface Vehicle for Autonomous\n  Inspection and Intervention in GNSS-Denied Maritime Environment","summary":"  This paper introduces an innovative drone carrier concept that is applied in\nmaritime port security or offshore rescue. This system works with a\nheterogeneous system consisting of multiple Unmanned Aerial Vehicles (UAVs) and\nUnmanned Surface Vehicles (USVs) to perform inspection and intervention tasks\nin GNSS-denied or interrupted environments. The carrier, an electric catamaran\nmeasuring 4m by 7m, features a 4m by 6m deck supporting automated takeoff and\nlanding for four DJI M300 drones, along with a 10kg-payload manipulator\noperable in up to level 3 sea conditions. Utilizing an offshore gimbal camera\nfor navigation, the carrier can autonomously navigate, approach and dock with\nnon-cooperative vessels, guided by an onboard camera, LiDAR, and Doppler\nVelocity Log (DVL) over a 3 km$^2$ area. UAVs equipped with onboard\nUltra-Wideband (UWB) technology execute mapping, detection, and manipulation\ntasks using a versatile gripper designed for wet, saline conditions.\nAdditionally, two UAVs can coordinate to transport large objects to the\nmanipulator or interact directly with them. These procedures are fully\nautomated and were successfully demonstrated at the Mohammed Bin Zayed\nInternational Robotic Competition (MBZIRC2024), where the drone carrier\nequipped with four UAVS and one manipulator, automatically accomplished the\nintervention tasks in sea-level-3 (wave height 1.25m) based on the rough target\ninformation.\n","authors":["Yihao Dong","Muhayyu Ud Din","Francesco Lagala","Hailiang Kuang","Jianjun Sun","Siyuan Yang","Irfan Hussain","Shaoming He"],"pdf_url":"https://arxiv.org/pdf/2501.12869v1.pdf","comment":"15 pages, 12pages"},{"id":"http://arxiv.org/abs/2501.12812v1","updated":"2025-01-22T11:42:19Z","published":"2025-01-22T11:42:19Z","title":"PSGSL: A Probabilistic Framework Integrating Semantic Scene\n  Understanding and Gas Sensing for Gas Source Localization","summary":"  Semantic scene understanding allows a robotic agent to reason about problems\nin complex ways, using information from multiple and varied sensors to make\ndeductions about a particular matter. As a result, this form of intelligent\nrobotics is capable of performing more complex tasks and achieving more precise\nresults than simpler approaches based on single data sources. However, these\nimproved capabilities come at the cost of higher complexity, both computational\nand in terms of design. Due to the increased design complexity, formal\napproaches for exploiting semantic understanding become necessary.\n  We present here a probabilistic formulation for integrating semantic\nknowledge into the process of gas source localization (GSL). The problem of GSL\nposes many unsolved challenges, and proposed solutions need to contend with the\nconstraining limitations of sensing hardware. By exploiting semantic scene\nunderstanding, we can leverage other sources of information, such as vision, to\nimprove the estimation of the source location. We show how our formulation can\nbe applied to pre-existing GSL algorithms and the effect that including\nsemantic data has on the produced estimations of the location of the source.\n","authors":["Pepe Ojeda","Javier Monroy","Javier Gonzalez-Jimenez"],"pdf_url":"https://arxiv.org/pdf/2501.12812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12799v1","updated":"2025-01-22T11:13:31Z","published":"2025-01-22T11:13:31Z","title":"Int2Planner: An Intention-based Multi-modal Motion Planner for\n  Integrated Prediction and Planning","summary":"  Motion planning is a critical module in autonomous driving, with the primary\nchallenge of uncertainty caused by interactions with other participants. As\nmost previous methods treat prediction and planning as separate tasks, it is\ndifficult to model these interactions. Furthermore, since the route path\nnavigates ego vehicles to a predefined destination, it provides relatively\nstable intentions for ego vehicles and helps constrain uncertainty. On this\nbasis, we construct Int2Planner, an \\textbf{Int}ention-based\n\\textbf{Int}egrated motion \\textbf{Planner} achieves multi-modal planning and\nprediction. Instead of static intention points, Int2Planner utilizes route\nintention points for ego vehicles and generates corresponding planning\ntrajectories for each intention point to facilitate multi-modal planning. The\nexperiments on the private dataset and the public nuPlan benchmark show the\neffectiveness of route intention points, and Int2Planner achieves\nstate-of-the-art performance. We also deploy it in real-world vehicles and have\nconducted autonomous driving for hundreds of kilometers in urban areas. It\nfurther verifies that Int2Planner can continuously interact with the traffic\nenvironment. Code will be avaliable at https://github.com/cxlz/Int2Planner.\n","authors":["Xiaolei Chen","Junchi Yan","Wenlong Liao","Tao He","Pai Peng"],"pdf_url":"https://arxiv.org/pdf/2501.12799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12764v1","updated":"2025-01-22T10:00:28Z","published":"2025-01-22T10:00:28Z","title":"Grid-based Submap Joining: An Efficient Algorithm for Simultaneously\n  Optimizing Global Occupancy Map and Local Submap Frames","summary":"  Optimizing robot poses and the map simultaneously has been shown to provide\nmore accurate SLAM results. However, for non-feature based SLAM approaches,\ndirectly optimizing all the robot poses and the whole map will greatly increase\nthe computational cost, making SLAM problems difficult to solve in large-scale\nenvironments. To solve the 2D non-feature based SLAM problem in large-scale\nenvironments more accurately and efficiently, we propose the grid-based submap\njoining method. Specifically, we first formulate the 2D grid-based submap\njoining problem as a non-linear least squares (NLLS) form to optimize the\nglobal occupancy map and local submap frames simultaneously. We then prove that\nin solving the NLLS problem using Gauss-Newton (GN) method, the increments of\nthe poses in each iteration are independent of the occupancy values of the\nglobal occupancy map. Based on this property, we propose a poseonly GN\nalgorithm equivalent to full GN method to solve the NLLS problem. The proposed\nsubmap joining algorithm is very efficient due to the independent property and\nthe pose-only solution. Evaluations using simulations and publicly available\npractical 2D laser datasets confirm the outperformance of our proposed method\ncompared to the state-of-the-art methods in terms of efficiency and accuracy,\nas well as the ability to solve the grid-based SLAM problem in very large-scale\nenvironments.\n","authors":["Yingyu Wang","Liang Zhao","Shoudong Huang"],"pdf_url":"https://arxiv.org/pdf/2501.12764v1.pdf","comment":"Accepted by IROS 2024"},{"id":"http://arxiv.org/abs/2410.12345v3","updated":"2025-01-22T08:55:37Z","published":"2024-10-16T08:05:56Z","title":"A Data-driven Contact Estimation Method for Wheeled-Biped Robots","summary":"  Contact estimation is a key ability for limbed robots, where making and\nbreaking contacts has a direct impact on state estimation and balance control.\nExisting approaches typically rely on gate-cycle priors or designated contact\nsensors. We design a contact estimator that is suitable for the emerging\nwheeled-biped robot types that do not have these features. To this end, we\npropose a Bayes filter in which update steps are learned from real-robot torque\nmeasurements while prediction steps rely on inertial measurements. We evaluate\nthis approach in extensive real-robot and simulation experiments. Our method\nachieves better performance while being considerably more sample efficient than\na comparable deep-learning baseline.\n","authors":["Ã. Bora GÃ¶kbakan","Frederike DÃ¼mbgen","StÃ©phane Caron"],"pdf_url":"https://arxiv.org/pdf/2410.12345v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10074v2","updated":"2025-01-22T08:36:33Z","published":"2025-01-17T09:46:27Z","title":"SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and\n  Chain-of-Thought for Embodied Task Planning","summary":"  Spatial reasoning is an essential problem in embodied AI research. Efforts to\nenhance spatial reasoning abilities through supplementary spatial data and\nfine-tuning have proven limited and ineffective when addressing complex\nembodied tasks, largely due to their dependence on language-based outputs.\nWhile some approaches have introduced a point-based action space to mitigate\nthis issue, they fall short in managing more intricate tasks within complex\nenvironments. This deficiency arises from their failure to fully exploit the\ninherent thinking and reasoning capabilities that are fundamental strengths of\nVision-Language Models (VLMs). To address these limitations, we propose a novel\napproach named SpatialCoT, specifically designed to bolster the spatial\nreasoning capabilities of VLMs. Our approach comprises two stages: spatial\ncoordinate bi-directional alignment, which aligns vision-language inputs with\nspatial coordinates, and chain-of-thought spatial grounding, which harnesses\nthe reasoning capabilities of language models for advanced spatial reasoning.\nWe evaluate SpatialCoT on challenging navigation and manipulation tasks, both\nin simulation and real-world settings. Experimental results demonstrate that\nour method significantly outperforms previous state-of-the-art approaches in\nboth tasks.\n","authors":["Yuecheng Liu","Dafeng Chi","Shiguang Wu","Zhanguang Zhang","Yaochen Hu","Lingfeng Zhang","Yingxue Zhang","Shuang Wu","Tongtong Cao","Guowei Huang","Helong Huang","Guangjian Tian","Weichao Qiu","Xingyue Quan","Jianye Hao","Yuzheng Zhuang"],"pdf_url":"https://arxiv.org/pdf/2501.10074v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2501.00368v2","updated":"2025-01-22T07:49:36Z","published":"2024-12-31T09:44:18Z","title":"Design Optimizer for Soft Growing Robot Manipulators in\n  Three-Dimensional Environments","summary":"  Soft growing robots are novel devices that mimic plant-like growth for\nnavigation in cluttered or dangerous environments. Their ability to adapt to\nsurroundings, combined with advancements in actuation and manufacturing\ntechnologies, allows them to perform specialized manipulation tasks. This work\npresents an approach for design optimization of soft growing robots;\nspecifically, the three-dimensional extension of the optimizer designed for\nplanar manipulators. This tool is intended to be used by engineers and robot\nenthusiasts before manufacturing their robot: it suggests the optimal size of\nthe robot for solving a specific task. The design process models a\nmulti-objective optimization problem to refine a soft manipulator's kinematic\nchain. Thanks to the novel Rank Partitioning algorithm integrated into\nEvolutionary Computation (EC) algorithms, this method achieves high precision\nin reaching targets and is efficient in resource usage. Results show\nsignificantly high performance in solving three-dimensional tasks, whereas\ncomparative experiments indicate that the optimizer features robust output when\ntested with different EC algorithms, particularly genetic algorithms.\n","authors":["Ahmet Astar","Ozan Nurcan","Erk Demirel","Emir Ozen","Ozan Kutlar","Fabio Stroppa"],"pdf_url":"https://arxiv.org/pdf/2501.00368v2.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.12654v1","updated":"2025-01-22T05:33:05Z","published":"2025-01-22T05:33:05Z","title":"AnyNav: Visual Neuro-Symbolic Friction Learning for Off-road Navigation","summary":"  Off-road navigation is essential for a wide range of applications in field\nrobotics such as planetary exploration and disaster response. However, it\nremains an unresolved challenge due to the unstructured environments and\ninherent complexity of terrain-vehicle interactions. Traditional physics-based\nmethods struggle to accurately model the nonlinear dynamics of these\ninteractions, while data-driven approaches often suffer from overfitting to\nspecific motion patterns, vehicle sizes, and types, limiting their\ngeneralizability. To overcome these challenges, we introduce a vision-based\nfriction estimation framework grounded in neuro-symbolic principles,\nintegrating neural networks for visual perception with symbolic reasoning for\nphysical modeling. This enables significantly improved generalization abilities\nthrough explicit physical reasoning incorporating the predicted friction.\nAdditionally, we develop a physics-informed planner that leverages the learned\nfriction coefficient to generate physically feasible and efficient paths, along\nwith corresponding speed profiles. We refer to our approach as AnyNav and\nevaluate it in both simulation and real-world experiments, demonstrating its\nutility and robustness across various off-road scenarios and multiple types of\nfour-wheeled vehicles. These results mark an important step toward developing\nneuro-symbolic spatial intelligence to reason about complex, unstructured\nenvironments and enable autonomous off-road navigation in challenging\nscenarios. Video demonstrations are available at https://sairlab.org/anynav/,\nwhere the source code will also be released.\n","authors":["Taimeng Fu","Zitong Zhan","Zhipeng Zhao","Shaoshu Su","Xiao Lin","Ehsan Tarkesh Esfahani","Karthik Dantu","Souma Chowdhury","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2501.12654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12594v1","updated":"2025-01-22T02:42:27Z","published":"2025-01-22T02:42:27Z","title":"A 3-Step Optimization Framework with Hybrid Models for a Humanoid\n  Robot's Jump Motion","summary":"  High dynamic jump motions are challenging tasks for humanoid robots to\nachieve environment adaptation and obstacle crossing. The trajectory\noptimization is a practical method to achieve high-dynamic and explosive\njumping. This paper proposes a 3-step trajectory optimization framework for\ngenerating a jump motion for a humanoid robot. To improve iteration speed and\nachieve ideal performance, the framework comprises three sub-optimizations. The\nfirst optimization incorporates momentum, inertia, and center of pressure\n(CoP), treating the robot as a static reaction momentum pendulum (SRMP) model\nto generate corresponding trajectories. The second optimization maps these\ntrajectories to joint space using effective Quadratic Programming (QP) solvers.\nFinally, the third optimization generates whole-body joint trajectories\nutilizing trajectories generated by previous parts. With the combined\nconsideration of momentum and inertia, the robot achieves agile forward jump\nmotions. A simulation and experiments (Fig. \\ref{Fig First page fig}) of\nforward jump with a distance of 1.0 m and 0.5 m height are presented in this\npaper, validating the applicability of the proposed framework.\n","authors":["Haoxiang Qi","Zhangguo Yu","Xuechao Chen","Yaliang Liu","Chuanku Yi","Chencheng Dong","Fei Meng","Qiang Huang"],"pdf_url":"https://arxiv.org/pdf/2501.12594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13548v3","updated":"2025-01-22T02:30:41Z","published":"2024-12-18T06:49:46Z","title":"TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness","summary":"  Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://telepreview.github.io.\n","authors":["Jingxiang Guo","Jiayu Luo","Zhenyu Wei","Yiwen Hou","Zhixuan Xu","Xiaoyi Lin","Chongkai Gao","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2412.13548v3.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2501.09905v2","updated":"2025-01-22T01:48:31Z","published":"2025-01-17T01:32:18Z","title":"SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon\n  Visuomotor Learning","summary":"  We present a low-cost legged mobile manipulation system that solves\nlong-horizon real-world tasks, trained by reinforcement learning purely in\nsimulation. This system is made possible by 1) a hierarchical design of a\nhigh-level policy for visual-mobile manipulation following instructions and a\nlow-level policy for quadruped movement and limb control, 2) a progressive\nexploration and learning approach that leverages privileged task decomposition\ninformation to train the teacher policy for long-horizon tasks, which will\nguide an imitation-based student policy for efficient training of the\nhigh-level visuomotor policy, and 3) a suite of techniques for minimizing\nsim-to-real gaps.\n  In contrast to previous approaches that use high-end equipment, our system\ndemonstrates effective performance with more accessible hardware -\nspecifically, a Unitree Go1 quadruped, a WidowX250S arm, and a single\nwrist-mounted RGB camera - despite the increased challenges of sim-to-real\ntransfer. When fully trained in simulation, a single policy autonomously solves\nlong-horizon tasks such as search, move, grasp, and drop-into, achieving nearly\n80% success. This performance is comparable to that of expert human\nteleoperation on the same tasks but operates in a more efficient way, at 1.5\ntimes the speed of human expert. The sim-to-real transfer is fluid across\ndiverse indoor and outdoor scenes under varying lighting conditions. Finally,\nwe discuss the key techniques that enable the entire pipeline, including\nefficient RL training and sim-to-real, to work effectively for legged mobile\nmanipulation, and present their ablation results.\n","authors":["Haichao Zhang","Haonan Yu","Le Zhao","Andrew Choi","Qinxun Bai","Break Yang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2501.09905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14957v2","updated":"2025-01-22T22:15:13Z","published":"2024-10-19T03:08:10Z","title":"Offline-to-online Reinforcement Learning for Image-based Grasping with\n  Scarce Demonstrations","summary":"  Offline-to-online reinforcement learning (O2O RL) aims to obtain a\ncontinually improving policy as it interacts with the environment, while\nensuring the initial policy behaviour is satisficing. This satisficing\nbehaviour is necessary for robotic manipulation where random exploration can be\ncostly due to catastrophic failures and time. O2O RL is especially compelling\nwhen we can only obtain a scarce amount of (potentially suboptimal)\ndemonstrations$\\unicode{x2014}$a scenario where behavioural cloning (BC) is\nknown to suffer from distribution shift. Previous works have outlined the\nchallenges in applying O2O RL algorithms under the image-based environments. In\nthis work, we propose a novel O2O RL algorithm that can learn in a real-life\nimage-based robotic vacuum grasping task with a small number of demonstrations\nwhere BC fails majority of the time. The proposed algorithm replaces the target\nnetwork in off-policy actor-critic algorithms with a regularization technique\ninspired by neural tangent kernel. We demonstrate that the proposed algorithm\ncan reach above 90\\% success rate in under two hours of interaction time, with\nonly 50 human demonstrations, while BC and existing commonly-used RL algorithms\nfail to achieve similar performance.\n","authors":["Bryan Chan","Anson Leung","James Bergstra"],"pdf_url":"https://arxiv.org/pdf/2410.14957v2.pdf","comment":"In CoRL Workshop on Mastering Robot Manipulation in a World of\n  Abundant Data 2024"},{"id":"http://arxiv.org/abs/2306.09600v2","updated":"2025-01-22T22:01:33Z","published":"2023-06-16T02:59:20Z","title":"From Novice to Skilled: RL-based Shared Autonomy Communicating with\n  Pilots in UAV Multi-Task Missions","summary":"  Multi-task missions for unmanned aerial vehicles (UAVs) involving inspection\nand landing tasks are challenging for novice pilots due to the difficulties\nassociated with depth perception and the control interface. We propose a shared\nautonomy system, alongside supplementary information displays, to assist pilots\nto successfully complete multi-task missions without any pilot training. Our\napproach comprises of three modules: (1) a perception module that encodes\nvisual information onto a latent representation, (2) a policy module that\naugments pilot's actions, and (3) an information augmentation module that\nprovides additional information to the pilot. The policy module is trained in\nsimulation with simulated users and transferred to the real world without\nmodification in a user study (n=29), alongside alternative supplementary\ninformation schemes including learnt red/green light feedback cues and an\naugmented reality display. The pilot's intent is unknown to the policy module\nand is inferred from the pilot's input and UAV's states. The assistant\nincreased task success rate for the landing and inspection tasks from [16.67% &\n54.29%] respectively to [95.59% & 96.22%]. With the assistant, inexperienced\npilots achieved similar performance to experienced pilots. Red/green light\nfeedback cues reduced the required time by 19.53% and trajectory length by\n17.86% for the inspection task, where participants rated it as their preferred\ncondition due to the intuitive interface and providing reassurance. This work\ndemonstrates that simple user models can train shared autonomy systems in\nsimulation, and transfer to physical tasks to estimate user intent and provide\neffective assistance and information to the pilot.\n","authors":["Kal Backman","Dana KuliÄ","Hoam Chung"],"pdf_url":"https://arxiv.org/pdf/2306.09600v2.pdf","comment":"37 pages, 11 figures, 6 tables. Accepted to ACM Transactions on\n  Human-Robot Interaction (THRI)"},{"id":"http://arxiv.org/abs/2501.13233v1","updated":"2025-01-22T21:34:39Z","published":"2025-01-22T21:34:39Z","title":"\"See You Later, Alligator\": Impacts of Robot Small Talk on Task,\n  Rapport, and Interaction Dynamics in Human-Robot Collaboration","summary":"  Small talk can foster rapport building in human-human teamwork; yet how\nnon-anthropomorphic robots, such as collaborative manipulators commonly used in\nindustry, may capitalize on these social communications remains unclear. This\nwork investigates how robot-initiated small talk influences task performance,\nrapport, and interaction dynamics in human-robot collaboration. We developed an\nautonomous robot system that assists a human in an assembly task while\ninitiating and engaging in small talk. A user study ($N = 58$) was conducted in\nwhich participants worked with either a functional robot, which engaged in only\ntask-oriented speech, or a social robot, which also initiated small talk. Our\nstudy found that participants in the social condition reported significantly\nhigher levels of rapport with the robot. Moreover, all participants in the\nsocial condition responded to the robot's small talk attempts; 59% initiated\nquestions to the robot, and 73% engaged in lingering conversations after\nrequesting the final task item. Although active working times were similar\nacross conditions, participants in the social condition recorded longer task\ndurations than those in the functional condition. We discuss the design and\nimplications of robot small talk in shaping human-robot collaboration.\n","authors":["Kaitlynn Taylor Pineda","Ethan Brown","Chien-Ming Huang"],"pdf_url":"https://arxiv.org/pdf/2501.13233v1.pdf","comment":"8 pages, 4 figures, preprint for HRI25, the 20th edition of the\n  IEEE/ACM International Conference on Human-Robot Interaction"},{"id":"http://arxiv.org/abs/2501.13203v1","updated":"2025-01-22T20:20:51Z","published":"2025-01-22T20:20:51Z","title":"Safe and Efficient Robot Action Planning in the Presence of Unconcerned\n  Humans","summary":"  This paper proposes a robot action planning scheme that provides an efficient\nand probabilistically safe plan for a robot interacting with an unconcerned\nhuman -- someone who is either unaware of the robot's presence or unwilling to\nengage in ensuring safety. The proposed scheme is predictive, meaning that the\nrobot is required to predict human actions over a finite future horizon; such\npredictions are often inaccurate in real-world scenarios. One possible approach\nto reduce the uncertainties is to provide the robot with the capability of\nreasoning about the human's awareness of potential dangers. This paper\ndiscusses that by using a binary variable, so-called danger awareness\ncoefficient, it is possible to differentiate between concerned and unconcerned\nhumans, and provides a learning algorithm to determine this coefficient by\nobserving human actions. Moreover, this paper argues how humans rely on\npredictions of other agents' future actions (including those of robots in\nhuman-robot interaction) in their decision-making. It also shows that ignoring\nthis aspect in predicting human's future actions can significantly degrade the\nefficiency of the interaction, causing agents to deviate from their optimal\npaths. The proposed robot action planning scheme is verified and validated via\nextensive simulation and experimental studies on a LoCoBot WidowX-250.\n","authors":["Mohsen Amiri","Mehdi Hosseinzadeh"],"pdf_url":"https://arxiv.org/pdf/2501.13203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13201v1","updated":"2025-01-22T20:09:32Z","published":"2025-01-22T20:09:32Z","title":"Polyhedral Collision Detection via Vertex Enumeration","summary":"  Collision detection is a critical functionality for robotics. The degree to\nwhich objects collide cannot be represented as a continuously differentiable\nfunction for any shapes other than spheres. This paper proposes a framework for\nhandling collision detection between polyhedral shapes. We frame the signed\ndistance between two polyhedral bodies as the optimal value of a convex\noptimization, and consider constraining the signed distance in a bilevel\noptimization problem. To avoid relying on specialized bilevel solvers, our\nmethod exploits the fact that the signed distance is the minimal point of a\nconvex region related to the two bodies. Our method enumerates the values\nobtained at all extreme points of this region and lists them as constraints in\nthe higher-level problem. We compare our formulation to existing methods in\nterms of reliability and speed when solved using the same mixed complementarity\nproblem solver. We demonstrate that our approach more reliably solves difficult\ncollision detection problems with multiple obstacles than other methods, and is\nfaster than existing methods in some cases.\n","authors":["Andrew Cinar","Yue Zhao","Forrest Laine"],"pdf_url":"https://arxiv.org/pdf/2501.13201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13189v1","updated":"2025-01-22T19:40:04Z","published":"2025-01-22T19:40:04Z","title":"Map Prediction and Generative Entropy for Multi-Agent Exploration","summary":"  Traditionally, autonomous reconnaissance applications have acted on explicit\nsets of historical observations. Aided by recent breakthroughs in generative\ntechnologies, this work enables robot teams to act beyond what is currently\nknown about the environment by inferring a distribution of reasonable\ninterpretations of the scene. We developed a map predictor that inpaints the\nunknown space in a multi-agent 2D occupancy map during an exploration mission.\nFrom a comparison of several inpainting methods, we found that a fine-tuned\nlatent diffusion inpainting model could provide rich and coherent\ninterpretations of simulated urban environments with relatively little\ncomputation time. By iteratively inferring interpretations of the scene\nthroughout an exploration run, we are able to identify areas that exhibit high\nuncertainty in the prediction, which we formalize with the concept of\ngenerative entropy. We prioritize tasks in regions of high generative entropy,\nhypothesizing that this will expedite convergence on an accurate predicted map\nof the scene. In our study we juxtapose this new paradigm of task ranking with\nthe state of the art, which ranks regions to explore by those which maximize\nexpected information recovery. We compare both of these methods in a simulated\nurban environment with three vehicles. Our results demonstrate that by using\nour new task ranking method, we can predict a correct scene significantly\nfaster than with a traditional information-guided method.\n","authors":["Alexander Spinos","Bradley Woosley","Justin Rokisky","Christopher Korpela","John G. Rogers III","Brian A. Bittner"],"pdf_url":"https://arxiv.org/pdf/2501.13189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04577v2","updated":"2025-01-22T19:28:38Z","published":"2025-01-08T15:47:04Z","title":"A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word\n  GRNG for AI Uncertainty Estimation","summary":"  Uncertainty estimation is an indispensable capability for AI-enabled,\nsafety-critical applications, e.g. autonomous vehicles or medical diagnosis.\nBayesian neural networks (BNNs) use Bayesian statistics to provide both\nclassification predictions and uncertainty estimation, but they suffer from\nhigh computational overhead associated with random number generation and\nrepeated sample iterations. Furthermore, BNNs are not immediately amenable to\nacceleration through compute-in-memory architectures due to the frequent memory\nwrites necessary after each RNG operation. To address these challenges, we\npresent an ASIC that integrates 360 fJ/Sample Gaussian RNG directly into the\nSRAM memory words. This integration reduces RNG overhead and enables\nfully-parallel compute-in-memory operations for BNNs. The prototype chip\nachieves 5.12 GSa/s RNG throughput and 102 GOp/s neural network throughput\nwhile occupying 0.45 mm2, bringing AI uncertainty estimation to edge\ncomputation.\n","authors":["Zephan M. Enciso","Boyang Cheng","Likai Pei","Jianbo Liu","Steven Davis","Michael Niemier","Ningyuan Cao"],"pdf_url":"https://arxiv.org/pdf/2501.04577v2.pdf","comment":"7 pages, 12 figures"},{"id":"http://arxiv.org/abs/2501.13132v1","updated":"2025-01-22T02:41:36Z","published":"2025-01-22T02:41:36Z","title":"A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat\n  Using Leader-Follower Strategy","summary":"  Multi-UAV air combat is a complex task involving multiple autonomous UAVs, an\nevolving field in both aerospace and artificial intelligence. This paper aims\nto enhance adversarial performance through collaborative strategies. Previous\napproaches predominantly discretize the action space into predefined actions,\nlimiting UAV maneuverability and complex strategy implementation. Others\nsimplify the problem to 1v1 combat, neglecting the cooperative dynamics among\nmultiple UAVs. To address the high-dimensional challenges inherent in\nsix-degree-of-freedom space and improve cooperation, we propose a hierarchical\nframework utilizing the Leader-Follower Multi-Agent Proximal Policy\nOptimization (LFMAPPO) strategy. Specifically, the framework is structured into\nthree levels. The top level conducts a macro-level assessment of the\nenvironment and guides execution policy. The middle level determines the angle\nof the desired action. The bottom level generates precise action commands for\nthe high-dimensional action space. Moreover, we optimize the state-value\nfunctions by assigning distinct roles with the leader-follower strategy to\ntrain the top-level policy, followers estimate the leader's utility, promoting\neffective cooperation among agents. Additionally, the incorporation of a target\nselector, aligned with the UAVs' posture, assesses the threat level of targets.\nFinally, simulation experiments validate the effectiveness of our proposed\nmethod.\n","authors":["Jinhui Pang","Jinglin He","Noureldin Mohamed Abdelaal Ahmed Mohamed","Changqing Lin","Zhihui Zhang","Xiaoshuai Hao"],"pdf_url":"https://arxiv.org/pdf/2501.13132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13973v1","updated":"2025-01-22T19:32:07Z","published":"2025-01-22T19:32:07Z","title":"A Spatio-temporal Graph Network Allowing Incomplete Trajectory Input for\n  Pedestrian Trajectory Prediction","summary":"  Pedestrian trajectory prediction is important in the research of mobile robot\nnavigation in environments with pedestrians. Most pedestrian trajectory\nprediction algorithms require the input historical trajectories to be complete.\nIf a pedestrian is unobservable in any frame in the past, then its historical\ntrajectory become incomplete, the algorithm will not predict its future\ntrajectory. To address this limitation, we propose the STGN-IT, a\nspatio-temporal graph network allowing incomplete trajectory input, which can\npredict the future trajectories of pedestrians with incomplete historical\ntrajectories. STGN-IT uses the spatio-temporal graph with an additional\nencoding method to represent the historical trajectories and observation states\nof pedestrians. Moreover, STGN-IT introduces static obstacles in the\nenvironment that may affect the future trajectories as nodes to further improve\nthe prediction accuracy. A clustering algorithm is also applied in the\nconstruction of spatio-temporal graphs. Experiments on public datasets show\nthat STGN-IT outperforms state of the art algorithms on these metrics.\n","authors":["Juncen Long","Gianluca Bardaro","Simone Mentasti","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2501.13973v1.pdf","comment":null}]},"2025-01-23T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2501.13928v1","updated":"2025-01-23T18:59:55Z","published":"2025-01-23T18:59:55Z","title":"Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass","summary":"  Multi-view 3D reconstruction remains a core challenge in computer vision,\nparticularly in applications requiring accurate and scalable representations\nacross diverse perspectives. Current leading methods such as DUSt3R employ a\nfundamentally pairwise approach, processing images in pairs and necessitating\ncostly global alignment procedures to reconstruct from multiple views. In this\nwork, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view\ngeneralization to DUSt3R that achieves efficient and scalable 3D reconstruction\nby processing many views in parallel. Fast3R's Transformer-based architecture\nforwards N images in a single forward pass, bypassing the need for iterative\nalignment. Through extensive experiments on camera pose estimation and 3D\nreconstruction, Fast3R demonstrates state-of-the-art performance, with\nsignificant improvements in inference speed and reduced error accumulation.\nThese results establish Fast3R as a robust alternative for multi-view\napplications, offering enhanced scalability without compromising reconstruction\naccuracy.\n","authors":["Jianing Yang","Alexander Sax","Kevin J. Liang","Mikael Henaff","Hao Tang","Ang Cao","Joyce Chai","Franziska Meier","Matt Feiszli"],"pdf_url":"https://arxiv.org/pdf/2501.13928v1.pdf","comment":"Project website: https://fast3r-3d.github.io/"},{"id":"http://arxiv.org/abs/2501.13919v1","updated":"2025-01-23T18:58:03Z","published":"2025-01-23T18:58:03Z","title":"Temporal Preference Optimization for Long-Form Video Understanding","summary":"  Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website.\n","authors":["Rui Li","Xiaohan Wang","Yuhui Zhang","Zeyu Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2501.13919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13876v1","updated":"2025-01-23T17:49:49Z","published":"2025-01-23T17:49:49Z","title":"FAST-LIVO2 on Resource-Constrained Platforms: LiDAR-Inertial-Visual\n  Odometry with Efficient Memory and Computation","summary":"  This paper presents a lightweight LiDAR-inertial-visual odometry system\noptimized for resource-constrained platforms. It integrates a\ndegeneration-aware adaptive visual frame selector into error-state iterated\nKalman filter (ESIKF) with sequential updates, improving computation efficiency\nsignificantly while maintaining a similar level of robustness. Additionally, a\nmemory-efficient mapping structure combining a locally unified visual-LiDAR map\nand a long-term visual map achieves a good trade-off between performance and\nmemory usage. Extensive experiments on x86 and ARM platforms demonstrate the\nsystem's robustness and efficiency. On the Hilti dataset, our system achieves a\n33% reduction in per-frame runtime and 47% lower memory usage compared to\nFAST-LIVO2, with only a 3 cm increase in RMSE. Despite this slight accuracy\ntrade-off, our system remains competitive, outperforming state-of-the-art\n(SOTA) LIO methods such as FAST-LIO2 and most existing LIVO systems. These\nresults validate the system's capability for scalable deployment on\nresource-constrained edge computing platforms.\n","authors":["Bingyang Zhou","Chunran Zheng","Ziming Wang","Fangcheng Zhu","Yixi Cai","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.13876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13855v1","updated":"2025-01-23T17:24:24Z","published":"2025-01-23T17:24:24Z","title":"First Lessons Learned of an Artificial Intelligence Robotic System for\n  Autonomous Coarse Waste Recycling Using Multispectral Imaging-Based Methods","summary":"  Current disposal facilities for coarse-grained waste perform manual sorting\nof materials with heavy machinery. Large quantities of recyclable materials are\nlost to coarse waste, so more effective sorting processes must be developed to\nrecover them. Two key aspects to automate the sorting process are object\ndetection with material classification in mixed piles of waste, and autonomous\ncontrol of hydraulic machinery. Because most objects in those accumulations of\nwaste are damaged or destroyed, object detection alone is not feasible in the\nmajority of cases. To address these challenges, we propose a classification of\nmaterials with multispectral images of ultraviolet (UV), visual (VIS), near\ninfrared (NIR), and short-wave infrared (SWIR) spectrums. Solution for\nautonomous control of hydraulic heavy machines for sorting of bulky waste is\nbeing investigated using cost-effective cameras and artificial\nintelligence-based controllers.\n","authors":["Timo Lange","Ajish Babu","Philipp Meyer","Matthis Keppner","Tim Tiedemann","Martin Wittmaier","Sebastian Wolff","Thomas VÃ¶gele"],"pdf_url":"https://arxiv.org/pdf/2501.13855v1.pdf","comment":"Published in Proceedings of Sardinia 2023, 19th International\n  Symposium on Waste Management, Resource Recovery and Sustainable Landfilling"},{"id":"http://arxiv.org/abs/2501.13817v1","updated":"2025-01-23T16:39:08Z","published":"2025-01-23T16:39:08Z","title":"Temporal Logic Guided Safe Navigation for Autonomous Vehicles","summary":"  Safety verification for autonomous vehicles (AVs) and ground robots is\ncrucial for ensuring reliable operation given their uncertain environments.\nFormal language tools provide a robust and sound method to verify safety rules\nfor such complex cyber-physical systems. In this paper, we propose a hybrid\napproach that combines the strengths of formal verification languages like\nLinear Temporal Logic (LTL) and Signal Temporal Logic (STL) to generate safe\ntrajectories and optimal control inputs for autonomous vehicle navigation. We\nimplement a symbolic path planning approach using LTL to generate a formally\nsafe reference trajectory. A mixed integer linear programming (MILP) solver is\nthen used on this reference trajectory to solve for the control inputs while\nsatisfying the state, control and safety constraints described by STL. We test\nour proposed solution on two environments and compare the results with popular\npath planning algorithms. In contrast to conventional path planning algorithms,\nour formally safe solution excels in handling complex specification scenarios\nwhile ensuring both safety and comparable computation times.\n","authors":["Aditya Parameshwaran","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2501.13817v1.pdf","comment":"6 pages, 5 figures, Modelling Estimation and Controls Conference-2024"},{"id":"http://arxiv.org/abs/2501.13804v1","updated":"2025-01-23T16:23:15Z","published":"2025-01-23T16:23:15Z","title":"Towards Real-World Validation of a Physics-Based Ship Motion Prediction\n  Model","summary":"  The maritime industry aims towards a sustainable future, which requires\nsignificant improvements in operational efficiency. Current approaches focus on\nminimising fuel consumption and emissions through greater autonomy. Efficient\nand safe autonomous navigation requires high-fidelity ship motion models\napplicable to real-world conditions. Although physics-based ship motion models\ncan predict ships' motion with sub-second resolution, their validation in\nreal-world conditions is rarely found in the literature. This study presents a\nphysics-based 3D dynamics motion model that is tailored to a container-ship,\nand compares its predictions against real-world voyages. The model integrates\nvessel motion over time and accounts for its hydrodynamic behavior under\ndifferent environmental conditions. The model's predictions are evaluated\nagainst real vessel data both visually and using multiple distance measures.\nBoth methodologies demonstrate that the model's predictions align closely with\nthe real-world trajectories of the container-ship.\n","authors":["Michail Mathioudakis","Christos Papandreou","Theodoros Stouraitis","Vicky Margari","Antonios Nikitakis","Stavros Paschalakis","Konstantinos Kyriakopoulos","Kostas J. Spyrou"],"pdf_url":"https://arxiv.org/pdf/2501.13804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04909v2","updated":"2025-01-23T15:31:37Z","published":"2024-02-07T14:38:51Z","title":"Entanglement Definitions for Tethered Robots: Exploration and Analysis","summary":"  In this article we consider the problem of tether entanglement for tethered\nmobile robots. One of the main risks of using a tethered connection between a\nmobile robot and an anchor point is that the tether may get entangled with the\nobstacles present in the environment or with itself. To avoid these situations,\na non-entanglement constraint can be considered in the motion planning problem\nfor tethered robots. This constraint is typically expressed as a set of\nspecific tether configurations that must be avoided. However, the literature\nlacks a generally accepted definition of entanglement, with existing\ndefinitions being limited and partial in the sense that they only focus on\nspecific instances of entanglement. In practice, this means that the existing\ndefinitions do not effectively cover all instances of tether entanglement. Our\ngoal in this article is to bridge this gap and to provide new definitions of\nentanglement, which, together with the existing ones, can be effectively used\nto qualify the entanglement state of a tethered robot in diverse situations.\nThe new definitions find application in motion planning for tethered robots,\nwhere they can be used to obtain more safe and robust entanglement-free\ntrajectories.\n","authors":["Gianpietro Battocletti","Dimitris Boskos","Domagoj ToliÄ","Ivana Palunko","Bart De Schutter"],"pdf_url":"https://arxiv.org/pdf/2402.04909v2.pdf","comment":"18 pages, 9 figures. Published on IEEE Access"},{"id":"http://arxiv.org/abs/2501.13725v1","updated":"2025-01-23T14:58:49Z","published":"2025-01-23T14:58:49Z","title":"You Only Crash Once v2: Perceptually Consistent Strong Features for\n  One-Stage Domain Adaptive Detection of Space Terrain","summary":"  The in-situ detection of planetary, lunar, and small-body surface terrain is\ncrucial for autonomous spacecraft applications, where learning-based computer\nvision methods are increasingly employed to enable intelligence without prior\ninformation or human intervention. However, many of these methods remain\ncomputationally expensive for spacecraft processors and prevent real-time\noperation. Training of such algorithms is additionally complex due to the\nscarcity of labeled data and reliance on supervised learning approaches.\nUnsupervised Domain Adaptation (UDA) offers a promising solution by\nfacilitating model training with disparate data sources such as simulations or\nsynthetic scenes, although UDA is difficult to apply to celestial environments\nwhere challenging feature spaces are paramount. To alleviate such issues, You\nOnly Crash Once (YOCOv1) has studied the integration of Visual Similarity-based\nAlignment (VSA) into lightweight one-stage object detection architectures to\nimprove space terrain UDA. Although proven effective, the approach faces\nnotable limitations, including performance degradations in multi-class and\nhigh-altitude scenarios. Building upon the foundation of YOCOv1, we propose\nnovel additions to the VSA scheme that enhance terrain detection capabilities\nunder UDA, and our approach is evaluated across both simulated and real-world\ndata. Our second YOCO rendition, YOCOv2, is capable of achieving\nstate-of-the-art UDA performance on surface terrain detection, where we\nshowcase improvements upwards of 31% compared with YOCOv1 and terrestrial\nstate-of-the-art. We demonstrate the practical utility of YOCOv2 with\nspacecraft flight hardware performance benchmarking and qualitative evaluation\nof NASA mission data.\n","authors":["Timothy Chase Jr","Christopher Wilson","Karthik Dantu"],"pdf_url":"https://arxiv.org/pdf/2501.13725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11683v3","updated":"2025-01-23T14:45:03Z","published":"2024-11-18T16:09:26Z","title":"TrojanRobot: Physical-World Backdoor Attacks Against VLM-based Robotic\n  Manipulation","summary":"  Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link \\url{https://trojanrobot.github.io}.\n","authors":["Xianlong Wang","Hewen Pan","Hangtao Zhang","Minghui Li","Shengshan Hu","Ziqi Zhou","Lulu Xue","Peijin Guo","Yichen Wang","Wei Wan","Aishan Liu","Leo Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11683v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13641v1","updated":"2025-01-23T13:18:52Z","published":"2025-01-23T13:18:52Z","title":"The Road to Learning Explainable Inverse Kinematic Models: Graph Neural\n  Networks as Inductive Bias for Symbolic Regression","summary":"  This paper shows how a Graph Neural Network (GNN) can be used to learn an\nInverse Kinematics (IK) based on an automatically generated dataset. The\ngenerated Inverse Kinematics is generalized to a family of manipulators with\nthe same Degree of Freedom (DOF), but varying link length configurations. The\nresults indicate a position error of less than 1.0 cm for 3 DOF and 4.5 cm for\n5 DOF, and orientation error of 2$^\\circ$ for 3 DOF and 8.2$^\\circ$ for 6 DOF,\nwhich allows the deployment to certain real world-problems. However,\nout-of-domain errors and lack of extrapolation can be observed in the resulting\nGNN. An extensive analysis of these errors indicates potential for enhancement\nin the future. Consequently, the generated GNNs are tailored to be used in\nfuture work as an inductive bias to generate analytical equations through\nsymbolic regression.\n","authors":["Pravin Pandey","Julia Reuter","Christoph Steup","Sanaz Mostaghim"],"pdf_url":"https://arxiv.org/pdf/2501.13641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11221v2","updated":"2025-01-23T12:35:51Z","published":"2024-02-17T08:32:22Z","title":"MOB-Net: Limb-modularized Uncertainty Torque Learning of Humanoids for\n  Sensorless External Torque Estimation","summary":"  Momentum observer (MOB) can estimate external joint torque without requiring\nadditional sensors, such as force/torque or joint torque sensors. However, the\nestimation performance of MOB deteriorates due to the model uncertainty which\nencompasses the modeling errors and the joint friction. Moreover, the\nestimation error is significant when MOB is applied to high-dimensional\nfloating-base humanoids, which prevents the estimated external joint torque\nfrom being used for force control or collision detection in the real humanoid\nrobot. In this paper, the pure external joint torque estimation method named\nMOB-Net, is proposed for humanoids. MOB-Net learns the model uncertainty torque\nand calibrates the estimated signal of MOB. The external joint torque can be\nestimated in the generalized coordinate including whole-body and virtual joints\nof the floating-base robot with only internal sensors (an IMU on the pelvis and\nencoders in the joints). Our method substantially reduces the estimation errors\nof MOB, and the robust performance of MOB-Net for the unseen data is validated\nthrough extensive simulations, real robot experiments, and ablation studies.\nFinally, various collision handling scenarios are presented using the estimated\nexternal joint torque from MOB-Net: contact wrench feedback control for\nlocomotion, collision detection, and collision reaction for safety.\n","authors":["Daegyu Lim","Myeong-Ju Kim","Junhyeok Cha","Jaeheung Park"],"pdf_url":"https://arxiv.org/pdf/2402.11221v2.pdf","comment":"Published to IJRR"},{"id":"http://arxiv.org/abs/2501.09600v4","updated":"2025-01-23T11:25:43Z","published":"2025-01-16T15:22:06Z","title":"Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid\n  Prototyping in Virtual Reality Applications","summary":"  SLAM is a foundational technique with broad applications in robotics and\nAR/VR. SLAM simulations evaluate new concepts, but testing on\nresource-constrained devices, such as VR HMDs, faces challenges: high\ncomputational cost and restricted sensor data access. This work proposes a\nsparse framework using mesh geometry projections as features, which improves\nefficiency and circumvents direct sensor data access, advancing SLAM research\nas we demonstrate in VR and through numerical evaluation.\n","authors":["Carlos Augusto Pinheiro de Sousa","Heiko Hamann","Oliver Deussen"],"pdf_url":"https://arxiv.org/pdf/2501.09600v4.pdf","comment":"Accepted to ENPT XR at IEEE VR 2025"},{"id":"http://arxiv.org/abs/2501.13507v1","updated":"2025-01-23T09:43:16Z","published":"2025-01-23T09:43:16Z","title":"Iterative Shaping of Multi-Particle Aggregates based on Action Trees and\n  VLM","summary":"  In this paper, we address the problem of manipulating multi-particle\naggregates using a bimanual robotic system. Our approach enables the autonomous\ntransport of dispersed particles through a series of shaping and pushing\nactions using robotically-controlled tools. Achieving this advanced\nmanipulation capability presents two key challenges: high-level task planning\nand trajectory execution. For task planning, we leverage Vision Language Models\n(VLMs) to enable primitive actions such as tool affordance grasping and\nnon-prehensile particle pushing. For trajectory execution, we represent the\nevolving particle aggregate's contour using truncated Fourier series, providing\nefficient parametrization of its closed shape. We adaptively compute trajectory\nwaypoints based on group cohesion and the geometric centroid of the aggregate,\naccounting for its spatial distribution and collective motion. Through\nreal-world experiments, we demonstrate the effectiveness of our methodology in\nactively shaping and manipulating multi-particle aggregates while maintaining\nhigh system cohesion.\n","authors":["Hoi-Yin Lee","Peng Zhou","Anqing Duan","Chenguang Yang","David Navarro-Alarcon"],"pdf_url":"https://arxiv.org/pdf/2501.13507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13461v1","updated":"2025-01-23T08:23:45Z","published":"2025-01-23T08:23:45Z","title":"Knowledge-Informed Multi-Agent Trajectory Prediction at Signalized\n  Intersections for Infrastructure-to-Everything","summary":"  Multi-agent trajectory prediction at signalized intersections is crucial for\ndeveloping efficient intelligent transportation systems and safe autonomous\ndriving systems. Due to the complexity of intersection scenarios and the\nlimitations of single-vehicle perception, the performance of vehicle-centric\nprediction methods has reached a plateau. Furthermore, most works underutilize\ncritical intersection information, including traffic signals, and behavior\npatterns induced by road structures. Therefore, we propose a multi-agent\ntrajectory prediction framework at signalized intersections dedicated to\nInfrastructure-to-Everything (I2XTraj). Our framework leverages dynamic graph\nattention to integrate knowledge from traffic signals and driving behaviors. A\ncontinuous signal-informed mechanism is proposed to adaptively process\nreal-time traffic signals from infrastructure devices. Additionally, leveraging\nthe prior knowledge of the intersection topology, we propose a driving strategy\nawareness mechanism to model the joint distribution of goal intentions and\nmaneuvers. To the best of our knowledge, I2XTraj represents the first\nmulti-agent trajectory prediction framework explicitly designed for\ninfrastructure deployment, supplying subscribable prediction services to all\nvehicles at intersections. I2XTraj demonstrates state-of-the-art performance on\nboth the Vehicle-to-Infrastructure dataset V2X-Seq and the aerial-view dataset\nSinD for signalized intersections. Quantitative evaluations show that our\napproach outperforms existing methods by more than 30% in both multi-agent and\nsingle-agent scenarios.\n","authors":["Huilin Yin","Yangwenhui Xu","Jiaxiang Li","Hao Zhang","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2501.13461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13457v1","updated":"2025-01-23T08:15:52Z","published":"2025-01-23T08:15:52Z","title":"Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks","summary":"  Signal Temporal Logic (STL) is a powerful specification language for\ndescribing complex temporal behaviors of continuous signals, making it\nwell-suited for high-level robotic task descriptions. However, generating\nexecutable plans for STL tasks is challenging, as it requires consideration of\nthe coupling between the task specification and the system dynamics. Existing\napproaches either follow a model-based setting that explicitly requires\nknowledge of the system dynamics or adopt a task-oriented data-driven approach\nto learn plans for specific tasks. In this work, we investigate the problem of\ngenerating executable STL plans for systems whose dynamics are unknown a\npriori. We propose a new planning framework that uses only task-agnostic data\nduring the offline training stage, enabling zero-shot generalization to new STL\ntasks. Our framework is hierarchical, involving: (i) decomposing the STL task\ninto a set of progress and time constraints, (ii) searching for time-aware\nwaypoints guided by task-agnostic data, and (iii) generating trajectories using\na pre-trained safe diffusion model. Simulation results demonstrate the\neffectiveness of our method indeed in achieving zero-shot generalization to\nvarious STL tasks.\n","authors":["Ruijia Liu","Ancheng Hou","Xiao Yu","Xiang Yin"],"pdf_url":"https://arxiv.org/pdf/2501.13457v1.pdf","comment":"submitted"},{"id":"http://arxiv.org/abs/2501.13432v1","updated":"2025-01-23T07:35:47Z","published":"2025-01-23T07:35:47Z","title":"Emotion estimation from video footage with LSTM","summary":"  Emotion estimation in general is a field that has been studied for a long\ntime, and several approaches exist using machine learning. in this paper, we\npresent an LSTM model, that processes the blend-shapes produced by the library\nMediaPipe, for a face detected in a live stream of a camera, to estimate the\nmain emotion from the facial expressions, this model is trained on the FER2013\ndataset and delivers a result of 71% accuracy and 62% f1-score which meets the\naccuracy benchmark of the FER2013 dataset, with significantly reduced\ncomputation costs. https://github.com/\nSamir-atra/Emotion_estimation_from_video_footage_with_LSTM_ML_algorithm\n","authors":["Samer Attrah"],"pdf_url":"https://arxiv.org/pdf/2501.13432v1.pdf","comment":"11 pages, 6 figures, 32 references, 4 tables"},{"id":"http://arxiv.org/abs/2501.00368v3","updated":"2025-01-23T07:04:34Z","published":"2024-12-31T09:44:18Z","title":"Design Optimizer for Soft Growing Robot Manipulators in\n  Three-Dimensional Environments","summary":"  Soft growing robots are novel devices that mimic plant-like growth for\nnavigation in cluttered or dangerous environments. Their ability to adapt to\nsurroundings, combined with advancements in actuation and manufacturing\ntechnologies, allows them to perform specialized manipulation tasks. This work\npresents an approach for design optimization of soft growing robots;\nspecifically, the three-dimensional extension of the optimizer designed for\nplanar manipulators. This tool is intended to be used by engineers and robot\nenthusiasts before manufacturing their robot: it suggests the optimal size of\nthe robot for solving a specific task. The design process models a\nmulti-objective optimization problem to refine a soft manipulator's kinematic\nchain. Thanks to the novel Rank Partitioning algorithm integrated into\nEvolutionary Computation (EC) algorithms, this method achieves high precision\nin reaching targets and is efficient in resource usage. Results show\nsignificantly high performance in solving three-dimensional tasks, whereas\ncomparative experiments indicate that the optimizer features robust output when\ntested with different EC algorithms, particularly genetic algorithms.\n","authors":["Ahmet Astar","Ozan Nurcan","Erk Demirel","Emir Ozen","Ozan Kutlar","Fabio Stroppa"],"pdf_url":"https://arxiv.org/pdf/2501.00368v3.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.13417v1","updated":"2025-01-23T06:43:38Z","published":"2025-01-23T06:43:38Z","title":"GeomGS: LiDAR-Guided Geometry-Aware Gaussian Splatting for Robot\n  Localization","summary":"  Mapping and localization are crucial problems in robotics and autonomous\ndriving. Recent advances in 3D Gaussian Splatting (3DGS) have enabled precise\n3D mapping and scene understanding by rendering photo-realistic images.\nHowever, existing 3DGS methods often struggle to accurately reconstruct a 3D\nmap that reflects the actual scale and geometry of the real world, which\ndegrades localization performance. To address these limitations, we propose a\nnovel 3DGS method called Geometry-Aware Gaussian Splatting (GeomGS). This\nmethod fully integrates LiDAR data into 3D Gaussian primitives via a\nprobabilistic approach, as opposed to approaches that only use LiDAR as initial\npoints or introduce simple constraints for Gaussian points. To this end, we\nintroduce a Geometric Confidence Score (GCS), which identifies the structural\nreliability of each Gaussian point. The GCS is optimized simultaneously with\nGaussians under probabilistic distance constraints to construct a precise\nstructure. Furthermore, we propose a novel localization method that fully\nutilizes both the geometric and photometric properties of GeomGS. Our GeomGS\ndemonstrates state-of-the-art geometric and localization performance across\nseveral benchmarks, while also improving photometric performance.\n","authors":["Jaewon Lee","Mangyu Kong","Minseong Park","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2501.13417v1.pdf","comment":"Preprint, Under review"},{"id":"http://arxiv.org/abs/2501.13416v1","updated":"2025-01-23T06:42:28Z","published":"2025-01-23T06:42:28Z","title":"M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction\n  with Person-aware Blockwise Attention","summary":"  Understanding social signals in multi-party conversations is important for\nhuman-robot interaction and artificial social intelligence. Multi-party\ninteractions include social signals like body pose, head pose, speech, and\ncontext-specific activities like acquiring and taking bites of food when\ndining. Incorporating all the multimodal signals in a multi-party interaction\nis difficult, and past work tends to build task-specific models for predicting\nsocial signals. In this work, we address the challenge of predicting multimodal\nsocial signals in multi-party settings in a single model. We introduce M3PT, a\ncausal transformer architecture with modality and temporal blockwise attention\nmasking which allows for the simultaneous processing of multiple social cues\nacross multiple participants and their temporal interactions. This approach\nbetter captures social dynamics over time by considering longer horizons of\nsocial signals between individuals. We train and evaluate our unified model on\nthe Human-Human Commensality Dataset (HHCD), and demonstrate that using\nmultiple modalities improves bite timing and speaking status prediction. Source\ncode: https://github.com/AbrarAnwar/masked-social-signals/\n","authors":["Yiming Tang","Abrar Anwar","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2501.13416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13402v1","updated":"2025-01-23T06:01:03Z","published":"2025-01-23T06:01:03Z","title":"VIGS SLAM: IMU-based Large-Scale 3D Gaussian Splatting SLAM","summary":"  Recently, map representations based on radiance fields such as 3D Gaussian\nSplatting and NeRF, which excellent for realistic depiction, have attracted\nconsiderable attention, leading to attempts to combine them with SLAM. While\nthese approaches can build highly realistic maps, large-scale SLAM still\nremains a challenge because they require a large number of Gaussian images for\nmapping and adjacent images as keyframes for tracking. We propose a novel 3D\nGaussian Splatting SLAM method, VIGS SLAM, that utilizes sensor fusion of RGB-D\nand IMU sensors for large-scale indoor environments. To reduce the\ncomputational load of 3DGS-based tracking, we adopt an ICP-based tracking\nframework that combines IMU preintegration to provide a good initial guess for\naccurate pose estimation. Our proposed method is the first to propose that\nGaussian Splatting-based SLAM can be effectively performed in large-scale\nenvironments by integrating IMU sensor measurements. This proposal not only\nenhances the performance of Gaussian Splatting SLAM beyond room-scale scenarios\nbut also achieves SLAM performance comparable to state-of-the-art methods in\nlarge-scale indoor environments.\n","authors":["Gyuhyeon Pak","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2501.13402v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2209.08812v5","updated":"2025-01-23T05:49:17Z","published":"2022-09-19T07:52:02Z","title":"Generative Graphical Inverse Kinematics","summary":"  Quickly and reliably finding accurate inverse kinematics (IK) solutions\nremains a challenging problem for many robot manipulators. Existing numerical\nsolvers are broadly applicable but typically only produce a single solution and\nrely on local search techniques to minimize nonconvex objective functions. More\nrecent learning-based approaches that approximate the entire feasible set of\nsolutions have shown promise as a means to generate multiple fast and accurate\nIK results in parallel. However, existing learning-based techniques have a\nsignificant drawback: each robot of interest requires a specialized model that\nmust be trained from scratch. To address this key shortcoming, we propose a\nnovel distance-geometric robot representation coupled with a graph structure\nthat allows us to leverage the sample efficiency of Euclidean equivariant\nfunctions and the generalizability of graph neural networks (GNNs). Our\napproach is generative graphical inverse kinematics (GGIK), the first learned\nIK solver able to accurately and efficiently produce a large number of diverse\nsolutions in parallel while also displaying the ability to generalize -- a\nsingle learned model can be used to produce IK solutions for a variety of\ndifferent robots. When compared to several other learned IK methods, GGIK\nprovides more accurate solutions with the same amount of data. GGIK can\ngeneralize reasonably well to robot manipulators unseen during training.\nAdditionally, GGIK can learn a constrained distribution that encodes joint\nlimits and scales efficiently to larger robots and a high number of sampled\nsolutions. Finally, GGIK can be used to complement local IK solvers by\nproviding reliable initializations for a local optimization process.\n","authors":["Oliver Limoyo","Filip MariÄ","Matthew Giamou","Petra Alexson","Ivan PetroviÄ","Jonathan Kelly"],"pdf_url":"https://arxiv.org/pdf/2209.08812v5.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2501.13072v2","updated":"2025-01-23T04:15:32Z","published":"2025-01-22T18:34:51Z","title":"AdaWM: Adaptive World Model based Planning for Autonomous Driving","summary":"  World model based reinforcement learning (RL) has emerged as a promising\napproach for autonomous driving, which learns a latent dynamics model and uses\nit to train a planning policy. To speed up the learning process, the\npretrain-finetune paradigm is often used, where online RL is initialized by a\npretrained model and a policy learned offline. However, naively performing such\ninitialization in RL may result in dramatic performance degradation during the\nonline interactions in the new task. To tackle this challenge, we first analyze\nthe performance degradation and identify two primary root causes therein: the\nmismatch of the planning policy and the mismatch of the dynamics model, due to\ndistribution shift. We further analyze the effects of these factors on\nperformance degradation during finetuning, and our findings reveal that the\nchoice of finetuning strategies plays a pivotal role in mitigating these\neffects. We then introduce AdaWM, an Adaptive World Model based planning\nmethod, featuring two key steps: (a) mismatch identification, which quantifies\nthe mismatches and informs the finetuning strategy, and (b) alignment-driven\nfinetuning, which selectively updates either the policy or the model as needed\nusing efficient low-rank updates. Extensive experiments on the challenging\nCARLA driving tasks demonstrate that AdaWM significantly improves the\nfinetuning process, resulting in more robust and efficient performance in\nautonomous driving systems.\n","authors":["Hang Wang","Xin Ye","Feng Tao","Chenbin Pan","Abhirup Mallik","Burhaneddin Yaman","Liu Ren","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.13072v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.13338v1","updated":"2025-01-23T02:39:04Z","published":"2025-01-23T02:39:04Z","title":"CuriousBot: Interactive Mobile Exploration via Actionable 3D Relational\n  Object Graph","summary":"  Mobile exploration is a longstanding challenge in robotics, yet current\nmethods primarily focus on active perception instead of active interaction,\nlimiting the robot's ability to interact with and fully explore its\nenvironment. Existing robotic exploration approaches via active interaction are\noften restricted to tabletop scenes, neglecting the unique challenges posed by\nmobile exploration, such as large exploration spaces, complex action spaces,\nand diverse object relations. In this work, we introduce a 3D relational object\ngraph that encodes diverse object relations and enables exploration through\nactive interaction. We develop a system based on this representation and\nevaluate it across diverse scenes. Our qualitative and quantitative results\ndemonstrate the system's effectiveness and generalization capabilities,\noutperforming methods that rely solely on vision-language models (VLMs).\n","authors":["Yixuan Wang","Leonor Fermoselle","Tarik Kelestemur","Jiuguang Wang","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2501.13338v1.pdf","comment":"Project Page: https://curiousbot.theaiinstitute.com/"},{"id":"http://arxiv.org/abs/2410.14565v2","updated":"2025-01-23T02:34:56Z","published":"2024-10-18T16:10:50Z","title":"Graph Optimality-Aware Stochastic LiDAR Bundle Adjustment with\n  Progressive Spatial Smoothing","summary":"  Large-scale LiDAR Bundle Adjustment (LBA) to refine sensor orientation and\npoint cloud accuracy simultaneously to build the navigation map is a\nfundamental task in logistics and robotics. Unlike pose-graph-based methods\nthat rely solely on pairwise relationships between LiDAR frames, LBA leverages\nraw LiDAR correspondences to achieve more precise results, especially when\ninitial pose estimates are unreliable for low-cost sensors. However, existing\nLBA methods face challenges such as simplistic planar correspondences,\nextensive observations, and dense normal matrices in the least-squares problem,\nwhich limit robustness, efficiency, and scalability. To address these issues,\nwe propose a Graph Optimality-aware Stochastic Optimization scheme with\nProgressive Spatial Smoothing, namely PSS-GOSO, to achieve \\textit{robust},\n\\textit{efficient}, and \\textit{scalable} LBA. The Progressive Spatial\nSmoothing (PSS) module extracts \\textit{robust} LiDAR feature association\nexploiting the prior structure information obtained by the polynomial smooth\nkernel. The Graph Optimality-aware Stochastic Optimization (GOSO) module first\nsparsifies the graph according to optimality for an \\textit{efficient}\noptimization. GOSO then utilizes stochastic clustering and graph\nmarginalization to solve the large-scale state estimation problem for a\n\\textit{scalable} LBA. We validate PSS-GOSO across diverse scenes captured by\nvarious platforms, demonstrating its superior performance compared to existing\nmethods. Moreover, the resulting point cloud maps are used for automatic\nlast-mile delivery in large-scale complex scenes. The project page can be found\nat: \\url{https://kafeiyin00.github.io/PSS-GOSO/}.\n","authors":["Jianping Li","Thien-Minh Nguyen","Muqing Cao","Shenghai Yuan","Tzu-Yi Hung","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2410.14565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10074v3","updated":"2025-01-23T02:31:25Z","published":"2025-01-17T09:46:27Z","title":"SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and\n  Chain-of-Thought for Embodied Task Planning","summary":"  Spatial reasoning is an essential problem in embodied AI research. Efforts to\nenhance spatial reasoning abilities through supplementary spatial data and\nfine-tuning have proven limited and ineffective when addressing complex\nembodied tasks, largely due to their dependence on language-based outputs.\nWhile some approaches have introduced a point-based action space to mitigate\nthis issue, they fall short in managing more intricate tasks within complex\nenvironments. This deficiency arises from their failure to fully exploit the\ninherent thinking and reasoning capabilities that are fundamental strengths of\nVision-Language Models (VLMs). To address these limitations, we propose a novel\napproach named SpatialCoT, specifically designed to bolster the spatial\nreasoning capabilities of VLMs. Our approach comprises two stages: spatial\ncoordinate bi-directional alignment, which aligns vision-language inputs with\nspatial coordinates, and chain-of-thought spatial grounding, which harnesses\nthe reasoning capabilities of language models for advanced spatial reasoning.\nWe evaluate SpatialCoT on challenging navigation and manipulation tasks, both\nin simulation and real-world settings. Experimental results demonstrate that\nour method significantly outperforms previous state-of-the-art approaches in\nboth tasks.\n","authors":["Yuecheng Liu","Dafeng Chi","Shiguang Wu","Zhanguang Zhang","Yaochen Hu","Lingfeng Zhang","Yingxue Zhang","Shuang Wu","Tongtong Cao","Guowei Huang","Helong Huang","Guangjian Tian","Weichao Qiu","Xingyue Quan","Jianye Hao","Yuzheng Zhuang"],"pdf_url":"https://arxiv.org/pdf/2501.10074v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2309.12397v2","updated":"2025-01-23T22:17:17Z","published":"2023-09-21T18:00:34Z","title":"POLAR-Sim: Augmenting NASA's POLAR Dataset for Data-Driven Lunar\n  Perception and Rover Simulation","summary":"  NASA's POLAR dataset contains approximately 2,600 pairs of high dynamic range\nstereo photos captured across 13 varied terrain scenarios, including areas with\nsparse or dense rock distributions, craters, and rocks of different sizes. The\npurpose of these photos is to spur development in robotics, AI-based\nperception, and autonomous navigation. Acknowledging a scarcity of lunar images\nfrom around the lunar poles, NASA Ames produced on Earth but in controlled\nconditions images that resemble rover operating conditions from these regions\nof the Moon. We report on the outcomes of an effort aimed at accomplishing two\ntasks. In Task 1, we provided bounding boxes and semantic segmentation\ninformation for all the images in NASA's POLAR dataset. This effort resulted in\n23,000 labels and semantic segmentation annotations pertaining to rocks,\nshadows, and craters. In Task 2, we generated the digital twins of the 13\nscenarios that have been used to produce all the photos in the POLAR dataset.\nSpecifically, for each of these scenarios, we produced individual meshes,\ntexture information, and material properties associated with the ground and the\nrocks in each scenario. This allows anyone with a camera model to synthesize\nimages associated with any of the 13 scenarios of the POLAR dataset.\nEffectively, one can generate as many semantically labeled synthetic images as\ndesired -- with different locations and exposure values in the scene, for\ndifferent positions of the sun, with or without the presence of active\nillumination, etc. The benefit of this work is twofold. Using outcomes of Task\n1, one can train and/or test perception algorithms that deal with Moon images.\nFor Task 2, one can produce as much data as desired to train and test AI\nalgorithms that are anticipated to work in lunar conditions. All the outcomes\nof this work are available in a public repository for unfettered use and\ndistribution.\n","authors":["Bo-Hsun Chen","Peter Negrut","Thomas Liang","Nevindu Batagoda","Harry Zhang","Dan Negrut"],"pdf_url":"https://arxiv.org/pdf/2309.12397v2.pdf","comment":"11 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2308.03496v2","updated":"2025-01-23T21:53:16Z","published":"2023-08-07T11:43:02Z","title":"Design and Implementation of an Efficient Onboard Computer System for\n  CanSat Atmosphere Monitoring","summary":"  With advancements in technology, the smaller versions of satellites have\ngained momentum in the space industry for earth monitoring and\ncommunication-based applications. The rise of CanSat technology has\nsignificantly impacted the space industry by providing a cost-effective\nsolution for space exploration. CanSat is a simulation model of a real\nsatellite and plays a crucial role in collecting and transmitting atmospheric\ndata. This paper discusses the design of an Onboard Computer System forCanSat,\nused to study various environmental parameters by monitoring the concentrations\nof gases in the atmosphere. The Onboard Computer System uses GPS,\naccelerometer, altitude, temperature, pressure, gyroscope, magnetometer, UV\nradiation, and air quality sensors for atmospheric sensing. A highly efficient\nand low-power ESP32 microcontroller and a transceiver module are used to\nacquire data, facilitate seamless communication and transmit the collected data\nto the ground station.\n","authors":["Abhijit Gadekar"],"pdf_url":"https://arxiv.org/pdf/2308.03496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14099v1","updated":"2025-01-23T21:14:55Z","published":"2025-01-23T21:14:55Z","title":"The Perceived Danger (PD) Scale: Development and Validation","summary":"  There are currently no psychometrically valid tools to measure the perceived\ndanger of robots. To fill this gap, we provided a definition of perceived\ndanger and developed and validated a 12-item bifactor scale through four\nstudies. An exploratory factor analysis revealed four subdimensions of\nperceived danger: affective states, physical vulnerability, ominousness, and\ncognitive readiness. A confirmatory factor analysis confirmed the bifactor\nmodel. We then compared the perceived danger scale to the Godspeed perceived\nsafety scale and found that the perceived danger scale is a better predictor of\nempirical data. We also validated the scale in an in-person setting and found\nthat the perceived danger scale is sensitive to robot speed manipulations,\nconsistent with previous empirical findings. Results across experiments suggest\nthat the perceived danger scale is reliable, valid, and an adequate predictor\nof both perceived safety and perceived danger in human-robot interaction\ncontexts.\n","authors":["Jaclyn Molan","Laura Saad","Eileen Roesler","J. Malcolm McCurry","Nathaniel Gyory","J. Gregory Trafton"],"pdf_url":"https://arxiv.org/pdf/2501.14099v1.pdf","comment":"9 pages, 2 figures, to be published in the Proceedings of the 2025\n  ACM/IEEE International Conference on Human-Robot Interaction (HRI)"},{"id":"http://arxiv.org/abs/2501.13996v1","updated":"2025-01-23T10:57:27Z","published":"2025-01-23T10:57:27Z","title":"Integrating Persian Lip Reading in Surena-V Humanoid Robot for\n  Human-Robot Interaction","summary":"  Lip reading is vital for robots in social settings, improving their ability\nto understand human communication. This skill allows them to communicate more\neasily in crowded environments, especially in caregiving and customer service\nroles. Generating a Persian Lip-reading dataset, this study integrates Persian\nlip-reading technology into the Surena-V humanoid robot to improve its speech\nrecognition capabilities. Two complementary methods are explored, an indirect\nmethod using facial landmark tracking and a direct method leveraging\nconvolutional neural networks (CNNs) and long short-term memory (LSTM)\nnetworks. The indirect method focuses on tracking key facial landmarks,\nespecially around the lips, to infer movements, while the direct method\nprocesses raw video data for action and speech recognition. The best-performing\nmodel, LSTM, achieved 89\\% accuracy and has been successfully implemented into\nthe Surena-V robot for real-time human-robot interaction. The study highlights\nthe effectiveness of these methods, particularly in environments where verbal\ncommunication is limited.\n","authors":["Ali Farshian Abbasi","Aghil Yousefi-Koma","Soheil Dehghani Firouzabadi","Parisa Rashidi","Alireza Naeini"],"pdf_url":"https://arxiv.org/pdf/2501.13996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13994v1","updated":"2025-01-23T10:44:35Z","published":"2025-01-23T10:44:35Z","title":"CSAOT: Cooperative Multi-Agent System for Active Object Tracking","summary":"  Object Tracking is essential for many computer vision applications, such as\nautonomous navigation, surveillance, and robotics. Unlike Passive Object\nTracking (POT), which relies on static camera viewpoints to detect and track\nobjects across consecutive frames, Active Object Tracking (AOT) requires a\ncontroller agent to actively adjust its viewpoint to maintain visual contact\nwith a moving target in complex environments. Existing AOT solutions are\npredominantly single-agent-based, which struggle in dynamic and complex\nscenarios due to limited information gathering and processing capabilities,\noften resulting in suboptimal decision-making. Alleviating these limitations\nnecessitates the development of a multi-agent system where different agents\nperform distinct roles and collaborate to enhance learning and robustness in\ndynamic and complex environments. Although some multi-agent approaches exist\nfor AOT, they typically rely on external auxiliary agents, which require\nadditional devices, making them costly. In contrast, we introduce the\nCollaborative System for Active Object Tracking (CSAOT), a method that\nleverages multi-agent deep reinforcement learning (MADRL) and a Mixture of\nExperts (MoE) framework to enable multiple agents to operate on a single\ndevice, thereby improving tracking performance and reducing costs. Our approach\nenhances robustness against occlusions and rapid motion while optimizing camera\nmovements to extend tracking duration. We validated the effectiveness of CSAOT\non various interactive maps with dynamic and stationary obstacles.\n","authors":["Hy Nguyen","Bao Pham","Hung Du","Srikanth Thudumu","Rajesh Vasa","Kon Mouzakis"],"pdf_url":"https://arxiv.org/pdf/2501.13994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13988v1","updated":"2025-01-23T08:27:15Z","published":"2025-01-23T08:27:15Z","title":"MCRL4OR: Multimodal Contrastive Representation Learning for Off-Road\n  Environmental Perception","summary":"  Most studies on environmental perception for autonomous vehicles (AVs) focus\non urban traffic environments, where the objects/stuff to be perceived are\nmainly from man-made scenes and scalable datasets with dense annotations can be\nused to train supervised learning models. By contrast, it is hard to densely\nannotate a large-scale off-road driving dataset manually due to the inherently\nunstructured nature of off-road environments. In this paper, we propose a\nMultimodal Contrastive Representation Learning approach for Off-Road\nenvironmental perception, namely MCRL4OR. This approach aims to jointly learn\nthree encoders for processing visual images, locomotion states, and control\nactions by aligning the locomotion states with the fused features of visual\nimages and control actions within a contrastive learning framework. The\ncausation behind this alignment strategy is that the inertial locomotion state\nis the result of taking a certain control action under the current\nlandform/terrain condition perceived by visual sensors. In experiments, we\npre-train the MCRL4OR with a large-scale off-road driving dataset and adopt the\nlearned multimodal representations for various downstream perception tasks in\noff-road driving scenarios. The superior performance in downstream tasks\ndemonstrates the advantages of the pre-trained multimodal representations. The\ncodes can be found in \\url{https://github.com/1uciusy/MCRL4OR}.\n","authors":["Yi Yang","Zhang Zhang","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2501.13988v1.pdf","comment":"Github repository: https://github.com/1uciusy/MCRL4OR"},{"id":"http://arxiv.org/abs/2501.17173v1","updated":"2025-01-23T19:01:18Z","published":"2025-01-23T19:01:18Z","title":"Model Evaluation of a Transformable CubeSat for Nonholonomic Attitude\n  Reorientation Using a Drop Tower","summary":"  This paper presents a design for a drop tower test to evaluate a numerical\nmodel for a structurally reconfigurable spacecraft with actuatable joints,\nreferred to as a transformable spacecraft. A mock-up robot for a 3U-sized\ntransformable spacecraft is designed to fit in a limited time and space of the\nmicrogravity environment available in the drop tower. The robot performs agile\nreorientation, referred to as nonholonomic attitude control, by actuating\njoints in a particular manner. To adapt to the very short duration of\nmicrogravity in the drop tower test, a successive joint actuation maneuver is\noptimized to maximize the amount of attitude reorientation within the time\nconstraint. The robot records the angular velocity history of all four bodies,\nand the data is analyzed to evaluate the accuracy of the numerical model. We\nconfirm that the constructed numerical model sufficiently replicates the\nrobot's motion and show that the post-experiment model corrections further\nimprove the accuracy of the numerical simulations. Finally, the difference\nbetween this drop tower test and the actual orbit demonstration is discussed to\nshow the prospect.\n","authors":["Yuki Kubo","Tsubasa Ando","Hirona Kawahara","Shu Miyata","Naoya Uchiyama","Kazutoshi Ito","Yoshiki Sugawara"],"pdf_url":"https://arxiv.org/pdf/2501.17173v1.pdf","comment":"22 pages, 20 figures"},{"id":"http://arxiv.org/abs/2501.17172v1","updated":"2025-01-23T14:11:32Z","published":"2025-01-23T14:11:32Z","title":"Towards spiking analog hardware implementation of a trajectory\n  interpolation mechanism for smooth closed-loop control of a spiking robot arm","summary":"  Neuromorphic engineering aims to incorporate the computational principles\nfound in animal brains, into modern technological systems. Following this\napproach, in this work we propose a closed-loop neuromorphic control system for\nan event-based robotic arm. The proposed system consists of a shifted\nWinner-Take-All spiking network for interpolating a reference trajectory and a\nspiking comparator network responsible for controlling the flow continuity of\nthe trajectory, which is fed back to the actual position of the robot. The\ncomparator model is based on a differential position comparison neural network,\nwhich governs the execution of the next trajectory points to close the control\nloop between both components of the system. To evaluate the system, we\nimplemented and deployed the model on a mixed-signal analog-digital\nneuromorphic platform, the DYNAP-SE2, to facilitate integration and\ncommunication with the ED-Scorbot robotic arm platform. Experimental results on\none joint of the robot validate the use of this architecture and pave the way\nfor future neuro-inspired control of the entire robot.\n","authors":["Daniel Casanueva-Morato","Chenxi Wu","Giacomo Indiveri","Juan P. Dominguez-Morales","Alejandro Linares-Barranco"],"pdf_url":"https://arxiv.org/pdf/2501.17172v1.pdf","comment":"5 pages, 7 figures, conference, ISCAS 2025, accepted for publication,\n  Spiking Neural Network"}]},"2025-01-24T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2501.14678v1","updated":"2025-01-24T17:57:00Z","published":"2025-01-24T17:57:00Z","title":"A Predictive Approach for Enhancing Accuracy in Remote Robotic Surgery\n  Using Informer Model","summary":"  Precise and real-time estimation of the robotic arm's position on the\npatient's side is essential for the success of remote robotic surgery in\nTactile Internet (TI) environments. This paper presents a prediction model\nbased on the Transformer-based Informer framework for accurate and efficient\nposition estimation. Additionally, it combines a Four-State Hidden Markov Model\n(4-State HMM) to simulate realistic packet loss scenarios. The proposed\napproach addresses challenges such as network delays, jitter, and packet loss\nto ensure reliable and precise operation in remote surgical applications. The\nmethod integrates the optimization problem into the Informer model by embedding\nconstraints such as energy efficiency, smoothness, and robustness into its\ntraining process using a differentiable optimization layer. The Informer\nframework uses features such as ProbSparse attention, attention distilling, and\na generative-style decoder to focus on position-critical features while\nmaintaining a low computational complexity of O(L log L). The method is\nevaluated using the JIGSAWS dataset, achieving a prediction accuracy of over 90\npercent under various network scenarios. A comparison with models such as TCN,\nRNN, and LSTM demonstrates the Informer framework's superior performance in\nhandling position prediction and meeting real-time requirements, making it\nsuitable for Tactile Internet-enabled robotic surgery.\n","authors":["Muhammad Hanif Lashari","Shakil Ahmed","Wafa Batayneh","Ashfaq Khokhar"],"pdf_url":"https://arxiv.org/pdf/2501.14678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14672v1","updated":"2025-01-24T17:48:29Z","published":"2025-01-24T17:48:29Z","title":"Gaussian-Process-based Adaptive Tracking Control with Dynamic Active\n  Learning for Autonomous Ground Vehicles","summary":"  This article proposes an active-learning-based adaptive trajectory tracking\ncontrol method for autonomous ground vehicles to compensate for modeling errors\nand unmodeled dynamics. The nominal vehicle model is decoupled into lateral and\nlongitudinal subsystems, which are augmented with online Gaussian Processes\n(GPs), using measurement data. The estimated mean functions of the GPs are used\nto construct a feedback compensator, which, together with an LPV state feedback\ncontroller designed for the nominal system, gives the adaptive control\nstructure. To assist exploration of the dynamics, the paper proposes a new,\ndynamic active learning method to collect the most informative samples to\naccelerate the training process. To analyze the performance of the overall\nlearning tool-chain provided controller, a novel iterative,\ncounterexample-based algorithm is proposed for calculating the induced L2 gain\nbetween the reference trajectory and the tracking error. The analysis can be\nexecuted for a set of possible realizations of the to-be-controlled system,\ngiving robust performance certificate of the learning method under variation of\nthe vehicle dynamics. The efficiency of the proposed control approach is shown\non a high-fidelity physics simulator and in real experiments using a 1/10 scale\nF1TENTH electric car.\n","authors":["KristÃ³f Floch","TamÃ¡s PÃ©ni","Roland TÃ³th"],"pdf_url":"https://arxiv.org/pdf/2501.14672v1.pdf","comment":"Submitted to IEEE Transactions on Control Systems Technology"},{"id":"http://arxiv.org/abs/2409.18592v2","updated":"2025-01-24T17:21:45Z","published":"2024-09-27T09:51:45Z","title":"From One to the Power of Many: Invariance to Multi-LiDAR Perception from\n  Single-Sensor Datasets","summary":"  Recently, LiDAR segmentation methods for autonomous vehicles, powered by deep\nneural networks, have experienced steep growth in performance on classic\nbenchmarks, such as nuScenes and SemanticKITTI. However, there are still large\ngaps in performance when deploying models trained on such single-sensor setups\nto modern vehicles with multiple high-resolution LiDAR sensors. In this work,\nwe introduce a new metric for feature-level invariance which can serve as a\nproxy to measure cross-domain generalization without requiring labeled data.\nAdditionally, we propose two application-specific data augmentations, which\nfacilitate better transfer to multi-sensor LiDAR setups, when trained on\nsingle-sensor datasets. We provide experimental evidence on both simulated and\nreal data, that our proposed augmentations improve invariance across LiDAR\nsetups, leading to improved generalization.\n","authors":["Marc Uecker","J. Marius ZÃ¶llner"],"pdf_url":"https://arxiv.org/pdf/2409.18592v2.pdf","comment":"Accepted for publication at the ML4AD Workshop @ AAAI Conference 2025"},{"id":"http://arxiv.org/abs/2501.14616v1","updated":"2025-01-24T16:33:56Z","published":"2025-01-24T16:33:56Z","title":"QuIP: Experimental design for expensive simulators with many Qualitative\n  factors via Integer Programming","summary":"  The need to explore and/or optimize expensive simulators with many\nqualitative factors arises in broad scientific and engineering problems. Our\nmotivating application lies in path planning - the exploration of feasible\npaths for navigation, which plays an important role in robotics, surgical\nplanning and assembly planning. Here, the feasibility of a path is evaluated\nvia expensive virtual experiments, and its parameter space is typically\ndiscrete and high-dimensional. A carefully selected experimental design is thus\nessential for timely decision-making. We propose here a novel framework, called\nQuIP, for experimental design of Qualitative factors via Integer Programming\nunder a Gaussian process surrogate model with an exchangeable covariance\nfunction. For initial design, we show that its asymptotic D-optimal design can\nbe formulated as a variant of the well-known assignment problem in operations\nresearch, which can be efficiently solved to global optimality using\nstate-of-the-art integer programming solvers. For sequential design\n(specifically, for active learning or black-box optimization), we show that its\ndesign criterion can similarly be formulated as an assignment problem, thus\nenabling efficient and reliable optimization with existing solvers. We then\ndemonstrate the effectiveness of QuIP over existing methods in a suite of path\nplanning experiments and an application to rover trajectory optimization.\n","authors":["Yen-Chun Liu","Simon Mak"],"pdf_url":"https://arxiv.org/pdf/2501.14616v1.pdf","comment":"40 pages, 6 figures, submitted to JCGS"},{"id":"http://arxiv.org/abs/2501.14587v1","updated":"2025-01-24T15:48:41Z","published":"2025-01-24T15:48:41Z","title":"Visual Localization via Semantic Structures in Autonomous Photovoltaic\n  Power Plant Inspection","summary":"  Inspection systems utilizing unmanned aerial vehicles (UAVs) equipped with\nthermal cameras are increasingly popular for the maintenance of photovoltaic\n(PV) power plants. However, automation of the inspection task is a challenging\nproblem as it requires precise navigation to capture images from optimal\ndistances and viewing angles.\n  This paper presents a novel localization pipeline that directly integrates PV\nmodule detection with UAV navigation, allowing precise positioning during\ninspection. Detections are used to identify the power plant structures in the\nimage and associate these with the power plant model. We define visually\nrecognizable anchor points for the initial association and use object tracking\nto discern global associations. We present three distinct methods for visual\nsegmentation of PV modules based on traditional computer vision, deep learning,\nand their fusion, and we evaluate their performance in relation to the proposed\nlocalization pipeline.\n  The presented methods were verified and evaluated using custom aerial\ninspection data sets, demonstrating their robustness and applicability for\nreal-time navigation. Additionally, we evaluate the influence of the power\nplant model's precision on the localization methods.\n","authors":["Viktor KozÃ¡k","Karel KoÅ¡nar","Jan Chudoba","Miroslav Kulich","Libor PÅeuÄil"],"pdf_url":"https://arxiv.org/pdf/2501.14587v1.pdf","comment":"47 pages, 22 figures"},{"id":"http://arxiv.org/abs/2501.14557v1","updated":"2025-01-24T15:02:18Z","published":"2025-01-24T15:02:18Z","title":"Optimizing Grasping Precision for Industrial Pick-and-Place Tasks\n  Through a Novel Visual Servoing Approach","summary":"  The integration of robotic arm manipulators into industrial manufacturing\nlines has become common, thanks to their efficiency and effectiveness in\nexecuting specific tasks. With advancements in camera technology, visual\nsensors and perception systems have been incorporated to address more complex\noperations. This study introduces a novel visual serving control system\ndesigned for robotic operations in challenging environments, where accurate\nobject pose estimation is hindered by factors such as vibrations, tool path\ndeviations, and machining marks. To overcome these obstacles, our solution\nfocuses on enhancing the accuracy of picking and placing tasks, ensuring\nreliable performance across various scenarios. This is accomplished by a novel\nvisual servoing method based on the integration of two complementary\nmethodologies: a technique for object localization and a separate approach for\nprecise control through visual feedback, leveraging their strengths to address\nthe challenges posed by the industrial context and thereby improving overall\ngrasping accuracy. Our method employ feedback from perception sensors to adjust\nthe control loop efficiently, enabling the robotic system to adeptly pick and\nplace objects. We have introduced a controller capable of seamlessly managing\nthe detection and manipulation of various shapes and types of objects within an\nindustrial context, addressing numerous challenges that arise in such\nenvironments.\n","authors":["Khairidine Benali"],"pdf_url":"https://arxiv.org/pdf/2501.14557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14526v1","updated":"2025-01-24T14:29:58Z","published":"2025-01-24T14:29:58Z","title":"Robustified Time-optimal Point-to-point Motion Planning and Control\n  under Uncertainty","summary":"  This paper proposes a novel approach to formulate time-optimal point-to-point\nmotion planning and control under uncertainty. The approach defines a\nrobustified two-stage Optimal Control Problem (OCP), in which stage 1, with a\nfixed time grid, is seamlessly stitched with stage 2, which features a variable\ntime grid. Stage 1 optimizes not only the nominal trajectory, but also feedback\ngains and corresponding state covariances, which robustify constraints in both\nstages. The outcome is a minimized uncertainty in stage 1 and a minimized total\nmotion time for stage 2, both contributing to the time optimality and safety of\nthe total motion. A timely replanning strategy is employed to handle changes in\nconstraints and maintain feasibility, while a tailored iterative algorithm is\nproposed for efficient, real-time OCP execution.\n","authors":["Shuhao Zhang","Jan Swevers"],"pdf_url":"https://arxiv.org/pdf/2501.14526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14513v1","updated":"2025-01-24T14:18:22Z","published":"2025-01-24T14:18:22Z","title":"ABPT: Amended Backpropagation through Time with Partially Differentiable\n  Rewards","summary":"  Using the exact gradients of the rewards to directly optimize policy\nparameters via backpropagation-through-time (BPTT) enables high training\nperformance for quadrotor tasks. However, designing a fully differentiable\nreward architecture is often challenging. Partially differentiable rewards will\nresult in biased gradient propagation that degrades training performance. To\novercome this limitation, we propose Amended Backpropagation-through-Time\n(ABPT), a novel approach that mitigates gradient bias while preserving the\ntraining efficiency of BPTT. ABPT combines 0-step and N-step returns,\neffectively reducing the bias by leveraging value gradients from the learned\nQ-value function. Additionally, it adopts entropy regularization and state\ninitialization mechanisms to encourage exploration during training. We evaluate\nABPT on four representative quadrotor flight tasks. Experimental results\ndemonstrate that ABPT converges significantly faster and achieves higher\nultimate rewards than existing learning algorithms, particularly in tasks\ninvolving partially differentiable rewards.\n","authors":["Fanxing Li","Fangyu Sun","Tianbao Zhang","Danping Zou"],"pdf_url":"https://arxiv.org/pdf/2501.14513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14503v1","updated":"2025-01-24T14:01:53Z","published":"2025-01-24T14:01:53Z","title":"Benchmarking global optimization techniques for unmanned aerial vehicle\n  path planning","summary":"  The Unmanned Aerial Vehicle (UAV) path planning problem is a complex\noptimization problem in the field of robotics. In this paper, we investigate\nthe possible utilization of this problem in benchmarking global optimization\nmethods. We devise a problem instance generator and pick 56 representative\ninstances, which we compare to established benchmarking suits through\nExploratory Landscape Analysis to show their uniqueness. For the computational\ncomparison, we select twelve well-performing global optimization techniques\nfrom both subfields of stochastic algorithms (evolutionary computation methods)\nand deterministic algorithms (Dividing RECTangles, or DIRECT-type methods). The\nexperiments were conducted in settings with varying dimensionality and\ncomputational budgets. The results were analyzed through several criteria\n(number of best-found solutions, mean relative error, Friedman ranks) and\nutilized established statistical tests. The best-ranking methods for the UAV\nproblems were almost universally the top-performing evolutionary techniques\nfrom recent competitions on numerical optimization at the Institute of\nElectrical and Electronics Engineers Congress on Evolutionary Computation.\nLastly, we discussed the variable dimension characteristics of the studied UAV\nproblems that remain still largely under-investigated.\n","authors":["Mhd Ali Shehadeh","Jakub Kudela"],"pdf_url":"https://arxiv.org/pdf/2501.14503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14502v1","updated":"2025-01-24T14:01:51Z","published":"2025-01-24T14:01:51Z","title":"LiDAR-Based Vehicle Detection and Tracking for Autonomous Racing","summary":"  Autonomous racing provides a controlled environment for testing the software\nand hardware of autonomous vehicles operating at their performance limits.\nCompetitive interactions between multiple autonomous racecars however introduce\nchallenging and potentially dangerous scenarios. Accurate and consistent\nvehicle detection and tracking is crucial for overtaking maneuvers, and\nlow-latency sensor processing is essential to respond quickly to hazardous\nsituations. This paper presents the LiDAR-based perception algorithms deployed\non Team PoliMOVE's autonomous racecar, which won multiple competitions in the\nIndy Autonomous Challenge series. Our Vehicle Detection and Tracking pipeline\nis composed of a novel fast Point Cloud Segmentation technique and a specific\nVehicle Pose Estimation methodology, together with a variable-step Multi-Target\nTracking algorithm. Experimental results demonstrate the algorithm's\nperformance, robustness, computational efficiency, and suitability for\nautonomous racing applications, enabling fully autonomous overtaking maneuvers\nat velocities exceeding 275 km/h.\n","authors":["Marcello Cellina","Matteo Corno","Sergio Matteo Savaresi"],"pdf_url":"https://arxiv.org/pdf/2501.14502v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2501.14486v1","updated":"2025-01-24T13:40:33Z","published":"2025-01-24T13:40:33Z","title":"Visual-Lidar Map Alignment for Infrastructure Inspections","summary":"  Routine and repetitive infrastructure inspections present safety, efficiency,\nand consistency challenges as they are performed manually, often in challenging\nor hazardous environments. They can also introduce subjectivity and errors into\nthe process, resulting in undesirable outcomes. Simultaneous localization and\nmapping (SLAM) presents an opportunity to generate high-quality 3D maps that\ncan be used to extract accurate and objective inspection data. Yet, many SLAM\nalgorithms are limited in their ability to align 3D maps from repeated\ninspections in GPS-denied settings automatically. This limitation hinders\npractical long-term asset health assessments by requiring tedious manual\nalignment for data association across scans from previous inspections. This\npaper introduces a versatile map alignment algorithm leveraging both visual and\nlidar data for improved place recognition robustness and presents an\ninfrastructure-focused dataset tailored for consecutive inspections. By\ndetaching map alignment from SLAM, our approach enhances infrastructure\ninspection pipelines, supports monitoring asset degradation over time, and\ninvigorates SLAM research by permitting exploration beyond existing\nmulti-session SLAM algorithms.\n","authors":["Jake McLaughlin","Nicholas Charron","Sriram Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2501.14486v1.pdf","comment":"8 pages, 8 figures, for associated code see\n  https://github.com/jakemclaughlin6/vlma"},{"id":"http://arxiv.org/abs/2501.06605v3","updated":"2025-01-24T13:29:33Z","published":"2025-01-11T18:11:07Z","title":"RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation","summary":"  Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench.\n","authors":["Zixuan Chen","Jing Huo","Yangtao Chen","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2501.06605v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2501.14451v1","updated":"2025-01-24T12:34:04Z","published":"2025-01-24T12:34:04Z","title":"MARL-OT: Multi-Agent Reinforcement Learning Guided Online Fuzzing to\n  Detect Safety Violation in Autonomous Driving Systems","summary":"  Autonomous Driving Systems (ADSs) are safety-critical, as real-world safety\nviolations can result in significant losses. Rigorous testing is essential\nbefore deployment, with simulation testing playing a key role. However, ADSs\nare typically complex, consisting of multiple modules such as perception and\nplanning, or well-trained end-to-end autonomous driving systems. Offline\nmethods, such as the Genetic Algorithm (GA), can only generate predefined\ntrajectories for dynamics, which struggle to cause safety violations for ADSs\nrapidly and efficiently in different scenarios due to their evolutionary\nnature. Online methods, such as single-agent reinforcement learning (RL), can\nquickly adjust the dynamics' trajectory online to adapt to different scenarios,\nbut they struggle to capture complex corner cases of ADS arising from the\nintricate interplay among multiple vehicles. Multi-agent reinforcement learning\n(MARL) has a strong ability in cooperative tasks. On the other hand, it faces\nits own challenges, particularly with convergence. This paper introduces\nMARL-OT, a scalable framework that leverages MARL to detect safety violations\nof ADS resulting from surrounding vehicles' cooperation. MARL-OT employs MARL\nfor high-level guidance, triggering various dangerous scenarios for the\nrule-based online fuzzer to explore potential safety violations of ADS, thereby\ngenerating dynamic, realistic safety violation scenarios. Our approach improves\nthe detected safety violation rate by up to 136.2% compared to the\nstate-of-the-art (SOTA) testing technique.\n","authors":["Linfeng Liang","Xi Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.14451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14443v1","updated":"2025-01-24T12:23:12Z","published":"2025-01-24T12:23:12Z","title":"Learning more with the same effort: how randomization improves the\n  robustness of a robotic deep reinforcement learning agent","summary":"  The industrial application of Deep Reinforcement Learning (DRL) is frequently\nslowed down because of the inability to generate the experience required to\ntrain the models. Collecting data often involves considerable time and economic\neffort that is unaffordable in most cases. Fortunately, devices like robots can\nbe trained with synthetic experience thanks to virtual environments. With this\napproach, the sample efficiency problems of artificial agents are mitigated,\nbut another issue arises: the need for efficiently transferring the synthetic\nexperience into the real world (sim-to-real).\n  This paper analyzes the robustness of a state-of-the-art sim-to-real\ntechnique known as progressive neural networks (PNNs) and studies how adding\ndiversity to the synthetic experience can complement it. To better understand\nthe drivers that lead to a lack of robustness, the robotic agent is still\ntested in a virtual environment to ensure total control on the divergence\nbetween the simulated and real models.\n  The results show that a PNN-like agent exhibits a substantial decrease in its\nrobustness at the beginning of the real training phase. Randomizing certain\nvariables during simulation-based training significantly mitigates this issue.\nOn average, the increase in the model's accuracy is around 25% when diversity\nis introduced in the training process. This improvement can be translated into\na decrease in the required real experience for the same final robustness\nperformance. Notwithstanding, adding real experience to agents should still be\nbeneficial regardless of the quality of the virtual experience fed into the\nagent.\n","authors":["LucÃ­a GÃ¼itta-LÃ³pez","Jaime Boal","Ãlvaro J. LÃ³pez-LÃ³pez"],"pdf_url":"https://arxiv.org/pdf/2501.14443v1.pdf","comment":"This article was accepted and published in Applied Intelligence\n  (10.1007/s10489-022-04227-3)"},{"id":"http://arxiv.org/abs/2501.14400v1","updated":"2025-01-24T11:11:53Z","published":"2025-01-24T11:11:53Z","title":"SKIL: Semantic Keypoint Imitation Learning for Generalizable\n  Data-efficient Manipulation","summary":"  Real-world tasks such as garment manipulation and table rearrangement demand\nrobots to perform generalizable, highly precise, and long-horizon actions.\nAlthough imitation learning has proven to be an effective approach for teaching\nrobots new skills, large amounts of expert demonstration data are still\nindispensible for these complex tasks, resulting in high sample complexity and\ncostly data collection. To address this, we propose Semantic Keypoint Imitation\nLearning (SKIL), a framework which automatically obtain semantic keypoints with\nhelp of vision foundation models, and forms the descriptor of semantic\nkeypoints that enables effecient imitation learning of complex robotic tasks\nwith significantly lower sample complexity. In real world experiments, SKIL\ndoubles the performance of baseline methods in tasks such as picking a cup or\nmouse, while demonstrating exceptional robustness to variations in objects,\nenvironmental changes, and distractors. For long-horizon tasks like hanging a\ntowel on a rack where previous methods fail completely, SKIL achieves a mean\nsuccess rate of 70\\% with as few as 30 demonstrations. Furthermore, SKIL\nnaturally supports cross-embodiment learning due to its semantic keypoints\nabstraction, our experiments demonstrate that even human videos bring\nconsiderable improvement to the learning performance. All these results\ndemonstrate the great success of SKIL in achieving data-efficint generalizable\nrobotic learning. Visualizations and code are available at:\nhttps://skil-robotics.github.io/SKIL-robotics/.\n","authors":["Shengjie Wang","Jiacheng You","Yihang Hu","Jiongye Li","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2501.14400v1.pdf","comment":"22 pages, 22 figures"},{"id":"http://arxiv.org/abs/2410.23085v3","updated":"2025-01-24T10:46:53Z","published":"2024-10-30T15:00:06Z","title":"S3PT: Scene Semantics and Structure Guided Clustering to Boost\n  Self-Supervised Pre-Training for Autonomous Driving","summary":"  Recent self-supervised clustering-based pre-training techniques like DINO and\nCribo have shown impressive results for downstream detection and segmentation\ntasks. However, real-world applications such as autonomous driving face\nchallenges with imbalanced object class and size distributions and complex\nscene geometries. In this paper, we propose S3PT a novel scene semantics and\nstructure guided clustering to provide more scene-consistent objectives for\nself-supervised training. Specifically, our contributions are threefold: First,\nwe incorporate semantic distribution consistent clustering to encourage better\nrepresentation of rare classes such as motorcycles or animals. Second, we\nintroduce object diversity consistent spatial clustering, to handle imbalanced\nand diverse object sizes, ranging from large background areas to small objects\nsuch as pedestrians and traffic signs. Third, we propose a depth-guided spatial\nclustering to regularize learning based on geometric information of the scene,\nthus further refining region separation on the feature level. Our learned\nrepresentations significantly improve performance in downstream semantic\nsegmentation and 3D object detection tasks on the nuScenes, nuImages, and\nCityscapes datasets and show promising domain translation properties.\n","authors":["Maciej K. Wozniak","Hariprasath Govindarajan","Marvin Klingner","Camille Maurice","B Ravi Kiran","Senthil Yogamani"],"pdf_url":"https://arxiv.org/pdf/2410.23085v3.pdf","comment":"Accepted for WACV 2025 (Oral)"},{"id":"http://arxiv.org/abs/2501.14377v1","updated":"2025-01-24T10:24:39Z","published":"2025-01-24T10:24:39Z","title":"Dream to Fly: Model-Based Reinforcement Learning for Vision-Based Drone\n  Flight","summary":"  Autonomous drone racing has risen as a challenging robotic benchmark for\ntesting the limits of learning, perception, planning, and control. Expert human\npilots are able to agilely fly a drone through a race track by mapping the\nreal-time feed from a single onboard camera directly to control commands.\nRecent works in autonomous drone racing attempting direct pixel-to-commands\ncontrol policies (without explicit state estimation) have relied on either\nintermediate representations that simplify the observation space or performed\nextensive bootstrapping using Imitation Learning (IL). This paper introduces an\napproach that learns policies from scratch, allowing a quadrotor to\nautonomously navigate a race track by directly mapping raw onboard camera\npixels to control commands, just as human pilots do. By leveraging model-based\nreinforcement learning~(RL) - specifically DreamerV3 - we train visuomotor\npolicies capable of agile flight through a race track using only raw pixel\nobservations. While model-free RL methods such as PPO struggle to learn under\nthese conditions, DreamerV3 efficiently acquires complex visuomotor behaviors.\nMoreover, because our policies learn directly from pixel inputs, the\nperception-aware reward term employed in previous RL approaches to guide the\ntraining process is no longer needed. Our experiments demonstrate in both\nsimulation and real-world flight how the proposed approach can be deployed on\nagile quadrotors. This approach advances the frontier of vision-based\nautonomous flight and shows that model-based RL is a promising direction for\nreal-world robotics.\n","authors":["Angel Romero","Ashwin Shenai","Ismail Geles","Elie Aljalbout","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2501.14377v1.pdf","comment":"11 pages, 7 Figures"},{"id":"http://arxiv.org/abs/2501.14319v1","updated":"2025-01-24T08:25:48Z","published":"2025-01-24T08:25:48Z","title":"Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and\n  3D Reconstruction from Noisy Video","summary":"  We aim to redefine robust ego-motion estimation and photorealistic 3D\nreconstruction by addressing a critical limitation: the reliance on noise-free\ndata in existing models. While such sanitized conditions simplify evaluation,\nthey fail to capture the unpredictable, noisy complexities of real-world\nenvironments. Dynamic motion, sensor imperfections, and synchronization\nperturbations lead to sharp performance declines when these models are deployed\nin practice, revealing an urgent need for frameworks that embrace and excel\nunder real-world noise. To bridge this gap, we tackle three core challenges:\nscalable data generation, comprehensive benchmarking, and model robustness\nenhancement. First, we introduce a scalable noisy data synthesis pipeline that\ngenerates diverse datasets simulating complex motion, sensor imperfections, and\nsynchronization errors. Second, we leverage this pipeline to create\nRobust-Ego3D, a benchmark rigorously designed to expose noise-induced\nperformance degradation, highlighting the limitations of current learning-based\nmethods in ego-motion accuracy and 3D reconstruction quality. Third, we propose\nCorrespondence-guided Gaussian Splatting (CorrGS), a novel test-time adaptation\nmethod that progressively refines an internal clean 3D representation by\naligning noisy observations with rendered RGB-D frames from clean 3D map,\nenhancing geometric alignment and appearance restoration through visual\ncorrespondence. Extensive experiments on synthetic and real-world data\ndemonstrate that CorrGS consistently outperforms prior state-of-the-art\nmethods, particularly in scenarios involving rapid motion and dynamic\nillumination.\n","authors":["Xiaohao Xu","Tianyi Zhang","Shibo Zhao","Xiang Li","Sibo Wang","Yongqi Chen","Ye Li","Bhiksha Raj","Matthew Johnson-Roberson","Sebastian Scherer","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2501.14319v1.pdf","comment":"Accepted by ICLR 2025; 92 Pages; Project Repo:\n  https://github.com/Xiaohao-Xu/SLAM-under-Perturbation. arXiv admin note:\n  substantial text overlap with arXiv:2406.16850"},{"id":"http://arxiv.org/abs/2501.14280v1","updated":"2025-01-24T06:51:48Z","published":"2025-01-24T06:51:48Z","title":"Enhancing Robotic Precision in Construction: A Modular Factor\n  Graph-Based Framework to Deflection and Backlash Compensation Using\n  High-Accuracy Accelerometers","summary":"  Accurate positioning is crucial in the construction industry, where labor\nshortages highlight the need for automation. Robotic systems with long\nkinematic chains are required to reach complex workspaces, including floors,\nwalls, and ceilings. These requirements significantly impact positioning\naccuracy due to effects such as deflection and backlash in various parts along\nthe kinematic chain. In this work, we introduce a novel approach that\nintegrates deflection and backlash compensation models with high-accuracy\naccelerometers, significantly enhancing position accuracy. Our method employs a\nmodular framework based on a factor graph formulation to estimate the state of\nthe kinematic chain, leveraging acceleration measurements to inform the model.\nExtensive testing on publicly released datasets, reflecting real-world\nconstruction disturbances, demonstrates the advantages of our approach. The\nproposed method reduces the $95\\%$ error threshold in the xy-plane by $50\\%$\ncompared to the state-of-the-art Virtual Joint Method, and by $31\\%$ when\nincorporating base tilt compensation.\n","authors":["Julien Kindle","Michael Loetscher","Andrea Alessandretti","Cesar Cadena","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2501.14280v1.pdf","comment":"8 pages, 7 figures, Accepted on November 2024 at IEEE Robotics and\n  Automation Letters"},{"id":"http://arxiv.org/abs/2501.14238v1","updated":"2025-01-24T04:50:16Z","published":"2025-01-24T04:50:16Z","title":"Point-LN: A Lightweight Framework for Efficient Point Cloud\n  Classification Using Non-Parametric Positional Encoding","summary":"  We introduce Point-LN, a novel lightweight framework engineered for efficient\n3D point cloud classification. Point-LN integrates essential non-parametric\ncomponents-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),\nand non-learnable positional encoding-with a streamlined learnable classifier\nthat significantly enhances classification accuracy while maintaining a minimal\nparameter footprint. This hybrid architecture ensures low computational costs\nand rapid inference speeds, making Point-LN ideal for real-time and\nresource-constrained applications. Comprehensive evaluations on benchmark\ndatasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN\nachieves competitive performance compared to state-of-the-art methods, all\nwhile offering exceptional efficiency. These results establish Point-LN as a\nrobust and scalable solution for diverse point cloud classification tasks,\nhighlighting its potential for widespread adoption in various computer vision\napplications.\n","authors":["Marzieh Mohammadi","Amir Salarpour","Pedram MohajerAnsari"],"pdf_url":"https://arxiv.org/pdf/2501.14238v1.pdf","comment":"This paper has been accepted for presentation at the 29th\n  International Computer Conference, Computer Society of Iran (CSICC) 2025"},{"id":"http://arxiv.org/abs/2501.14208v1","updated":"2025-01-24T03:26:41Z","published":"2025-01-24T03:26:41Z","title":"You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from\n  Video Demonstrations","summary":"  Bimanual robotic manipulation is a long-standing challenge of embodied\nintelligence due to its characteristics of dual-arm spatial-temporal\ncoordination and high-dimensional action spaces. Previous studies rely on\npre-defined action taxonomies or direct teleoperation to alleviate or\ncircumvent these issues, often making them lack simplicity, versatility and\nscalability. Differently, we believe that the most effective and efficient way\nfor teaching bimanual manipulation is learning from human demonstrated videos,\nwhere rich features such as spatial-temporal positions, dynamic postures,\ninteraction states and dexterous transitions are available almost for free. In\nthis work, we propose the YOTO (You Only Teach Once), which can extract and\nthen inject patterns of bimanual actions from as few as a single binocular\nobservation of hand movements, and teach dual robot arms various complex tasks.\nFurthermore, based on keyframes-based motion trajectories, we devise a subtle\nsolution for rapidly generating training demonstrations with diverse variations\nof manipulated objects and their locations. These data can then be used to\nlearn a customized bimanual diffusion policy (BiDP) across diverse scenes. In\nexperiments, YOTO achieves impressive performance in mimicking 5 intricate\nlong-horizon bimanual tasks, possesses strong generalization under different\nvisual and spatial conditions, and outperforms existing visuomotor imitation\nlearning methods in accuracy and efficiency. Our project link is\nhttps://hnuzhy.github.io/projects/YOTO.\n","authors":["Huayi Zhou","Ruixiang Wang","Yunxin Tai","Yueci Deng","Guiliang Liu","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2501.14208v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2501.14151v1","updated":"2025-01-24T00:34:51Z","published":"2025-01-24T00:34:51Z","title":"RaccoonBot: An Autonomous Wire-Traversing Solar-Tracking Robot for\n  Persistent Environmental Monitoring","summary":"  Environmental monitoring is used to characterize the health and relationship\nbetween organisms and their environments. In forest ecosystems, robots can\nserve as platforms to acquire such data, even in hard-to-reach places where\nwire-traversing platforms are particularly promising due to their efficient\ndisplacement. This paper presents the RaccoonBot, which is a novel autonomous\nwire-traversing robot for persistent environmental monitoring, featuring a\nfail-safe mechanical design with a self-locking mechanism in case of electrical\nshortage. The robot also features energy-aware mobility through a novel Solar\ntracking algorithm, that allows the robot to find a position on the wire to\nhave direct contact with solar power to increase the energy harvested.\nExperimental results validate the electro-mechanical features of the\nRaccoonBot, showing that it is able to handle wire perturbations, different\ninclinations, and achieving energy autonomy.\n","authors":["Efrain Mendez-Flores","Agaton Pourshahidi","Magnus Egerstedt"],"pdf_url":"https://arxiv.org/pdf/2501.14151v1.pdf","comment":"Pre-print submitted to the 2025 IEEE International Conference on\n  Robotics & Automation (ICRA 2025)"},{"id":"http://arxiv.org/abs/2501.14147v1","updated":"2025-01-24T00:21:10Z","published":"2025-01-24T00:21:10Z","title":"HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting","summary":"  3D Gaussian Splatting offers expressive scene reconstruction, modeling a\nbroad range of visual, geometric, and semantic information. However, efficient\nreal-time map reconstruction with data streamed from multiple robots and\ndevices remains a challenge. To that end, we propose HAMMER, a server-based\ncollaborative Gaussian Splatting method that leverages widely available ROS\ncommunication infrastructure to generate 3D, metric-semantic maps from\nasynchronous robot data-streams with no prior knowledge of initial robot\npositions and varying on-device pose estimators. HAMMER consists of (i) a frame\nalignment module that transforms local SLAM poses and image data into a global\nframe and requires no prior relative pose knowledge, and (ii) an online module\nfor training semantic 3DGS maps from streaming data. HAMMER handles mixed\nperception modes, adjusts automatically for variations in image pre-processing\namong different devices, and distills CLIP semantic codes into the 3D scene for\nopen-vocabulary language queries. In our real-world experiments, HAMMER creates\nhigher-fidelity maps (2x) compared to competing baselines and is useful for\ndownstream tasks, such as semantic goal-conditioned navigation (e.g., ``go to\nthe couch\"). Accompanying content available at hammer-project.github.io.\n","authors":["Javier Yu","Timothy Chen","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2501.14147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14942v1","updated":"2025-01-24T22:01:23Z","published":"2025-01-24T22:01:23Z","title":"Force-Based Robotic Imitation Learning: A Two-Phase Approach for\n  Construction Assembly Tasks","summary":"  The drive for efficiency and safety in construction has boosted the role of\nrobotics and automation. However, complex tasks like welding and pipe insertion\npose challenges due to their need for precise adaptive force control, which\ncomplicates robotic training. This paper proposes a two-phase system to improve\nrobot learning, integrating human-derived force feedback. The first phase\ncaptures real-time data from operators using a robot arm linked with a virtual\nsimulator via ROS-Sharp. In the second phase, this feedback is converted into\nrobotic motion instructions, using a generative approach to incorporate force\nfeedback into the learning process. This method's effectiveness is demonstrated\nthrough improved task completion times and success rates. The framework\nsimulates realistic force-based interactions, enhancing the training data's\nquality for precise robotic manipulation in construction tasks.\n","authors":["Hengxu You","Yang Ye","Tianyu Zhou","Jing Du"],"pdf_url":"https://arxiv.org/pdf/2501.14942v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2501.14934v1","updated":"2025-01-24T21:47:38Z","published":"2025-01-24T21:47:38Z","title":"Temporal Binding Foundation Model for Material Property Recognition via\n  Tactile Sequence Perception","summary":"  Robots engaged in complex manipulation tasks require robust material property\nrecognition to ensure adaptability and precision. Traditionally, visual data\nhas been the primary source for object perception; however, it often proves\ninsufficient in scenarios where visibility is obstructed or detailed\nobservation is needed. This gap highlights the necessity of tactile sensing as\na complementary or primary input for material recognition. Tactile data becomes\nparticularly essential in contact-rich, small-scale manipulations where subtle\ndeformations and surface interactions cannot be accurately captured by vision\nalone. This letter presents a novel approach leveraging a temporal binding\nfoundation model for tactile sequence understanding to enhance material\nproperty recognition. By processing tactile sensor data with a temporal focus,\nthe proposed system captures the sequential nature of tactile interactions,\nsimilar to human fingertip perception. Additionally, this letter demonstrates\nthat, through tailored and specific design, the foundation model can more\neffectively capture temporal information embedded in tactile sequences,\nadvancing material property understanding. Experimental results validate the\nmodel's capability to capture these temporal patterns, confirming its utility\nfor material property recognition in visually restricted scenarios. This work\nunderscores the necessity of embedding advanced tactile data processing\nframeworks within robotic systems to achieve truly embodied and responsive\nmanipulation capabilities.\n","authors":["Hengxu You","Tianyu Zhou","Jing Du"],"pdf_url":"https://arxiv.org/pdf/2501.14934v1.pdf","comment":"4 pages,"},{"id":"http://arxiv.org/abs/2308.10966v6","updated":"2025-01-24T21:32:20Z","published":"2023-08-21T18:23:53Z","title":"Deadlock-free, Safe, and Decentralized Multi-Robot Navigation in Social\n  Mini-Games via Discrete-Time Control Barrier Functions","summary":"  We present an approach to ensure safe and deadlock-free navigation for\ndecentralized multi-robot systems operating in constrained environments,\nincluding doorways and intersections. Although many solutions have been\nproposed that ensure safety and resolve deadlocks, optimally preventing\ndeadlocks in a minimally invasive and decentralized fashion remains an open\nproblem. We first formalize the objective as a non-cooperative,\nnon-communicative, partially observable multi-robot navigation problem in\nconstrained spaces with multiple conflicting agents, which we term as social\nmini-games. Formally, we solve a discrete-time optimal receding horizon control\nproblem leveraging control barrier functions for safe long-horizon planning.\nOur approach to ensuring liveness rests on the insight that \\textit{there\nexists barrier certificates that allow each robot to preemptively perturb their\nstate in a minimally-invasive fashion onto liveness sets i.e. states where\nrobots are deadlock-free}. We evaluate our approach in simulation as well on\nphysical robots using F$1/10$ robots, a Clearpath Jackal, as well as a Boston\nDynamics Spot in a doorway, hallway, and corridor intersection scenario.\nCompared to both fully decentralized and centralized approaches with and\nwithout deadlock resolution capabilities, we demonstrate that our approach\nresults in safer, more efficient, and smoother navigation, based on a\ncomprehensive set of metrics including success rate, collision rate, stop time,\nchange in velocity, path deviation, time-to-goal, and flow rate.\n","authors":["Rohan Chandra","Vrushabh Zinage","Efstathios Bakolas","Peter Stone","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2308.10966v6.pdf","comment":"major update since last revision"},{"id":"http://arxiv.org/abs/2501.14856v1","updated":"2025-01-24T17:15:49Z","published":"2025-01-24T17:15:49Z","title":"Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative\n  Framework for Imitation Learning from Observation","summary":"  This paper introduces a new imitation learning framework based on\nenergy-based generative models capable of learning complex, physics-dependent,\nrobot motion policies through state-only expert motion trajectories. Our\nalgorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR),\nconstructs several perturbed versions of the expert's motion data distribution\nand learns smooth, and well-defined representations of the data distribution's\nenergy function using denoising score matching. We propose to use these learnt\nenergy functions as reward functions to learn imitation policies via\nreinforcement learning. We also present a strategy to gradually switch between\nthe learnt energy functions, ensuring that the learnt rewards are always\nwell-defined in the manifold of policy-generated samples. We evaluate our\nalgorithm on complex humanoid tasks such as locomotion and martial arts and\ncompare it with state-only adversarial imitation learning algorithms like\nAdversarial Motion Priors (AMP). Our framework sidesteps the optimisation\nchallenges of adversarial imitation learning techniques and produces results\ncomparable to AMP in several quantitative metrics across multiple imitation\nsettings.\n","authors":["Anish Abhijit Diwan","Julen Urain","Jens Kober","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2501.14856v1.pdf","comment":"Accepted as a conference paper at the International Conference on\n  Learning Representations (ICLR) 2025"}]},"2025-01-27T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2412.05313v4","updated":"2025-01-27T18:53:40Z","published":"2024-11-28T19:31:50Z","title":"Î»: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile\n  Manipulation Robotics","summary":"  Efficiently learning and executing long-horizon mobile manipulation (MoMa)\ntasks is crucial for advancing robotics in household and workplace settings.\nHowever, current MoMa models are data-inefficient, underscoring the need for\nimproved models that require realistic-sized benchmarks to evaluate their\nefficiency, which do not exist. To address this, we introduce the LAMBDA\n({\\lambda}) benchmark (Long-horizon Actions for Mobile-manipulation\nBenchmarking of Directed Activities), which evaluates the data efficiency of\nmodels on language-conditioned, long-horizon, multi-room, multi-floor,\npick-and-place tasks using a dataset of manageable size, more feasible for\ncollection. The benchmark includes 571 human-collected demonstrations that\nprovide realism and diversity in simulated and real-world settings. Unlike\nplanner-generated data, these trajectories offer natural variability and\nreplay-verifiability, ensuring robust learning and evaluation. We benchmark\nseveral models, including learning-based models and a neuro-symbolic modular\napproach combining foundation models with task and motion planning.\nLearning-based models show suboptimal success rates, even when leveraging\npretrained weights, underscoring significant data inefficiencies. However, the\nneuro-symbolic approach performs significantly better while being more data\nefficient. Findings highlight the need for more data-efficient learning-based\nMoMa approaches. {\\lambda} addresses this gap by serving as a key benchmark for\nevaluating the data efficiency of those future models in handling household\nrobotics tasks.\n","authors":["Ahmed Jaafar","Shreyas Sundara Raman","Yichen Wei","Sudarshan Harithas","Sofia Juliani","Anneke Wernerfelt","Benedict Quartey","Ifrah Idrees","Jason Xinyu Liu","Stefanie Tellex"],"pdf_url":"https://arxiv.org/pdf/2412.05313v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13548v4","updated":"2025-01-27T17:04:37Z","published":"2024-12-18T06:49:46Z","title":"TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness","summary":"  Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://nus-lins-lab.github.io/telepreview/.\n","authors":["Jingxiang Guo","Jiayu Luo","Zhenyu Wei","Yiwen Hou","Zhixuan Xu","Xiaoyi Lin","Chongkai Gao","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2412.13548v4.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2501.16212v1","updated":"2025-01-27T17:04:12Z","published":"2025-01-27T17:04:12Z","title":"An FPGA-Based Neuro-Fuzzy Sensor for Personalized Driving Assistance","summary":"  Advanced driving-assistance systems (ADAS) are intended to automatize driver\ntasks, as well as improve driving and vehicle safety. This work proposes an\nintelligent neuro-fuzzy sensor for driving style (DS) recognition, suitable for\nADAS enhancement. The development of the driving style intelligent sensor uses\nnaturalistic driving data from the SHRP2 study, which includes data from a CAN\nbus, inertial measurement unit, and front radar. The system has been\nsuccessfully implemented using a field-programmable gate array (FPGA) device of\nthe Xilinx Zynq programmable system-on-chip (PSoC). It can mimic the typical\ntiming parameters of a group of drivers as well as tune these typical\nparameters to model individual DSs. The neuro-fuzzy intelligent sensor provides\nhigh-speed real-time active ADAS implementation and is able to personalize its\nbehavior into safe margins without driver intervention. In particular, the\npersonalization procedure of the time headway (THW) parameter for an ACC in\nsteady car following was developed, achieving a performance of 0.53\nmicroseconds. This performance fulfilled the requirements of cutting-edge\nactive ADAS specifications.\n","authors":["Ãscar Mata-Carballeira","Jon GutiÃ©rrez-Zaballa","InÃ©s del Campo","Victoria MartÃ­nez"],"pdf_url":"https://arxiv.org/pdf/2501.16212v1.pdf","comment":"Journal Article"},{"id":"http://arxiv.org/abs/2501.14486v2","updated":"2025-01-27T15:26:49Z","published":"2025-01-24T13:40:33Z","title":"Visual-Lidar Map Alignment for Infrastructure Inspections","summary":"  Routine and repetitive infrastructure inspections present safety, efficiency,\nand consistency challenges as they are performed manually, often in challenging\nor hazardous environments. They can also introduce subjectivity and errors into\nthe process, resulting in undesirable outcomes. Simultaneous localization and\nmapping (SLAM) presents an opportunity to generate high-quality 3D maps that\ncan be used to extract accurate and objective inspection data. Yet, many SLAM\nalgorithms are limited in their ability to align 3D maps from repeated\ninspections in GPS-denied settings automatically. This limitation hinders\npractical long-term asset health assessments by requiring tedious manual\nalignment for data association across scans from previous inspections. This\npaper introduces a versatile map alignment algorithm leveraging both visual and\nlidar data for improved place recognition robustness and presents an\ninfrastructure-focused dataset tailored for consecutive inspections. By\ndetaching map alignment from SLAM, our approach enhances infrastructure\ninspection pipelines, supports monitoring asset degradation over time, and\ninvigorates SLAM research by permitting exploration beyond existing\nmulti-session SLAM algorithms.\n","authors":["Jake McLaughlin","Nicholas Charron","Sriram Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2501.14486v2.pdf","comment":"8 pages, 8 figures, for associated code see\n  https://github.com/jakemclaughlin6/vlma"},{"id":"http://arxiv.org/abs/2501.16101v1","updated":"2025-01-27T14:50:19Z","published":"2025-01-27T14:50:19Z","title":"3D Reconstruction of non-visible surfaces of objects from a Single Depth\n  View -- Comparative Study","summary":"  Scene and object reconstruction is an important problem in robotics, in\nparticular in planning collision-free trajectories or in object manipulation.\nThis paper compares two strategies for the reconstruction of nonvisible parts\nof the object surface from a single RGB-D camera view. The first method, named\nDeepSDF predicts the Signed Distance Transform to the object surface for a\ngiven point in 3D space. The second method, named MirrorNet reconstructs the\noccluded objects' parts by generating images from the other side of the\nobserved object. Experiments performed with objects from the ShapeNet dataset,\nshow that the view-dependent MirrorNet is faster and has smaller reconstruction\nerrors in most categories.\n","authors":["RafaÅ Staszak","Piotr MichaÅek","Jakub ChudziÅski","Marek Kopicki","Dominik Belter"],"pdf_url":"https://arxiv.org/pdf/2501.16101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09372v2","updated":"2025-01-27T13:30:20Z","published":"2024-07-12T15:53:15Z","title":"Segmentation Dataset for Reinforced Concrete Construction","summary":"  This paper provides a dataset of 14,805 RGB images with segmentation labels\nfor autonomous robotic inspection of reinforced concrete defects. Baselines for\nthe YOLOv8L-seg, DeepLabV3, and U-Net segmentation models are established.\nLabelling inconsistencies are addressed statistically, and their influence on\nmodel performance is analyzed. An error identification tool is employed to\nexamine the error modes of the models. The paper demonstrates that YOLOv8L-seg\nperforms best, achieving a validation mIOU score of up to 0.59. Label\ninconsistencies were found to have a negligible effect on model performance,\nwhile the inclusion of more data improved the performance. False negatives were\nidentified as the primary failure mode. The results highlight the importance of\ndata availability for the performance of deep learning-based models. The lack\nof publicly available data is identified as a significant contributor to false\nnegatives. To address this, the paper advocates for an increased open-source\napproach within the construction community.\n","authors":["Patrick Schmidt","Lazaros Nalpantidis"],"pdf_url":"https://arxiv.org/pdf/2407.09372v2.pdf","comment":"The ConRebSeg Dataset can be found under the following DOI:\n  https://doi.org/10.11583/DTU.26213762 Corresponding code to download\n  additional data and initialize the dataset under\n  https://github.com/DTU-PAS/ConRebSeg This work is an accepted manuscript up\n  for publication in the Elsevier journal \"Automation in Construction\""},{"id":"http://arxiv.org/abs/2501.16006v1","updated":"2025-01-27T12:44:19Z","published":"2025-01-27T12:44:19Z","title":"Underactuated dexterous robotic grasping with reconfigurable passive\n  joints","summary":"  We introduce a novel reconfigurable passive joint (RP-joint), which has been\nimplemented and tested on an underactuated three-finger robotic gripper.\nRP-joint has no actuation, but instead it is lightweight and compact. It can be\neasily reconfigured by applying external forces and locked to perform complex\ndexterous manipulation tasks, but only after tension is applied to the\nconnected tendon. Additionally, we present an approach that allows learning\ndexterous grasps from single examples with underactuated grippers and\nautomatically configures the RP-joints for dexterous manipulation. This is\nenhanced by integrating kinaesthetic contact optimization, which improves grasp\nperformance even further. The proposed RP-joint gripper and grasp planner have\nbeen tested on over 370 grasps executed on 42 IKEA objects and on the YCB\nobject dataset, achieving grasping success rates of 80% and 87%, on IKEA and\nYCB, respectively.\n","authors":["Marek Kopicki","Sainul Islam Ansary","Simone Tolomei","Franco Angelini","Manolo Garabini","Piotr SkrzypczyÅski"],"pdf_url":"https://arxiv.org/pdf/2501.16006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15901v1","updated":"2025-01-27T09:51:48Z","published":"2025-01-27T09:51:48Z","title":"Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint\n  Generation","summary":"  Mobile robot path planning in complex environments remains a significant\nchallenge, especially in achieving efficient, safe and robust paths. The\ntraditional path planning techniques like DRL models typically trained for a\ngiven configuration of the starting point and target positions, these models\nonly perform well when these conditions are satisfied. In this paper, we\nproposed a novel path planning framework that embeds Large Language Models to\nempower mobile robots with the capability of dynamically interpreting natural\nlanguage commands and autonomously generating efficient, collision-free\nnavigation paths. The proposed framework uses LLMs to translate high-level user\ninputs into actionable waypoints while dynamically adjusting paths in response\nto obstacles. We experimentally evaluated our proposed LLM-based approach\nacross three different environments of progressive complexity, showing the\nrobustness of our approach with llama3.1 model that outperformed other LLM\nmodels in path planning time, waypoint generation success rate, and collision\navoidance. This underlines the promising contribution of LLMs for enhancing the\ncapability of mobile robots, especially when their operation involves complex\ndecisions in large and complex environments. Our framework has provided safer,\nmore reliable navigation systems and opened a new direction for the future\nresearch. The source code of this work is publicly available on GitHub.\n","authors":["Muhammad Taha Tariq","Congqing Wang","Yasir Hussain"],"pdf_url":"https://arxiv.org/pdf/2501.15901v1.pdf","comment":"18 pages, 6 figures, submitted in Journal Expert Systems with\n  Applications"},{"id":"http://arxiv.org/abs/2411.12308v2","updated":"2025-01-27T09:51:05Z","published":"2024-11-19T07:49:22Z","title":"SNN-Based Online Learning of Concepts and Action Laws in an Open World","summary":"  We present the architecture of a fully autonomous, bio-inspired cognitive\nagent built around a spiking neural network (SNN) implementing the agent's\nsemantic memory. The agent explores its universe and learns concepts of\nobjects/situations and of its own actions in a one-shot manner. While\nobject/situation concepts are unary, action concepts are triples made up of an\ninitial situation, a motor activity, and an outcome. They embody the agent's\nknowledge of its universe's actions laws. Both kinds of concepts have different\ndegrees of generality. To make decisions the agent queries its semantic memory\nfor the expected outcomes of envisaged actions and chooses the action to take\non the basis of these predictions. Our experiments show that the agent handles\nnew situations by appealing to previously learned general concepts and rapidly\nmodifies its concepts to adapt to environment changes.\n","authors":["Christel Grimaud","Dominique Longin","Andreas Herzig"],"pdf_url":"https://arxiv.org/pdf/2411.12308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15850v1","updated":"2025-01-27T08:18:52Z","published":"2025-01-27T08:18:52Z","title":"LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for\n  Autonomous Driving with Large Language Models","summary":"  Ensuring and improving the safety of autonomous driving systems (ADS) is\ncrucial for the deployment of highly automated vehicles, especially in\nsafety-critical events. To address the rarity issue, adversarial scenario\ngeneration methods are developed, in which behaviors of traffic participants\nare manipulated to induce safety-critical events. However, existing methods\nstill face two limitations. First, identification of the adversarial\nparticipant directly impacts the effectiveness of the generation. However, the\ncomplexity of real-world scenarios, with numerous participants and diverse\nbehaviors, makes identification challenging. Second, the potential of generated\nsafety-critical scenarios to continuously improve ADS performance remains\nunderexplored. To address these issues, we propose LLM-attacker: a closed-loop\nadversarial scenario generation framework leveraging large language models\n(LLMs). Specifically, multiple LLM agents are designed and coordinated to\nidentify optimal attackers. Then, the trajectories of the attackers are\noptimized to generate adversarial scenarios. These scenarios are iteratively\nrefined based on the performance of ADS, forming a feedback loop to improve\nADS. Experimental results show that LLM-attacker can create more dangerous\nscenarios than other methods, and the ADS trained with it achieves a collision\nrate half that of training with normal scenarios. This indicates the ability of\nLLM-attacker to test and enhance the safety and robustness of ADS. Video\ndemonstrations are provided at:\nhttps://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view.\n","authors":["Yuewen Mei","Tong Nie","Jian Sun","Ye Tian"],"pdf_url":"https://arxiv.org/pdf/2501.15850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15830v1","updated":"2025-01-27T07:34:33Z","published":"2025-01-27T07:34:33Z","title":"SpatialVLA: Exploring Spatial Representations for Visual-Language-Action\n  Model","summary":"  In this paper, we claim that spatial understanding is the keypoint in robot\nmanipulation, and propose SpatialVLA to explore effective spatial\nrepresentations for the robot foundation model. Specifically, we introduce\nEgo3D Position Encoding to inject 3D information into the input observations of\nthe visual-language-action model, and propose Adaptive Action Grids to\nrepresent spatial robot movement actions with adaptive discretized action\ngrids, facilitating learning generalizable and transferrable spatial action\nknowledge for cross-robot control. SpatialVLA is first pre-trained on top of a\nvision-language model with 1.1 Million real-world robot episodes, to learn a\ngeneralist manipulation policy across multiple robot environments and tasks.\nAfter pre-training, SpatialVLA is directly applied to perform numerous tasks in\na zero-shot manner. The superior results in both simulation and real-world\nrobots demonstrate its advantage of inferring complex robot motion trajectories\nand its strong in-domain multi-task generalization ability. We further show the\nproposed Adaptive Action Grids offer a new and effective way to fine-tune the\npre-trained SpatialVLA model for new simulation and real-world setups, where\nthe pre-learned action grids are re-discretized to capture robot-specific\nspatial action movements of new setups. The superior results from extensive\nevaluations demonstrate the exceptional in-distribution generalization and\nout-of-distribution adaptation capability, highlighting the crucial benefit of\nthe proposed spatial-aware representations for generalist robot policy\nlearning. All the details and codes will be open-sourced.\n","authors":["Delin Qu","Haoming Song","Qizhi Chen","Yuanqi Yao","Xinyi Ye","Yan Ding","Zhigang Wang","JiaYuan Gu","Bin Zhao","Dong Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2501.15830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15806v1","updated":"2025-01-27T06:28:29Z","published":"2025-01-27T06:28:29Z","title":"Autonomous Horizon-based Asteroid Navigation With\n  Observability-constrained Maneuvers","summary":"  Asteroid exploration is a pertinent challenge due to the varying complexity\nof their dynamical environments, shape and communication delays due to\ndistance. Thus, autonomous navigation methods are continually being developed\nand improved in current research to enable their safe exploration. These\nmethods often involve using horizon-based Optical Navigation (OpNav) to\ndetermine the spacecraft's location, which is reliant on the visibility of the\nhorizon. It is critical to ensure the reliability of this measurement such that\nthe spacecraft may maintain an accurate state estimate throughout its mission.\nThis paper presents an algorithm that generates control maneuvers for\nspacecraft to follow trajectories that allow continuously usable optical\nmeasurements to maintain system observability for safe navigation. This\nalgorithm improves upon existing asteroid navigation capabilities by allowing\nthe safe and robust autonomous targeting of various trajectories and orbits at\na wide range of distances within optical measurement range. It is adaptable to\ndifferent asteroid scenarios. Overall, the approach develops an\nall-encompassing system that simulates the asteroid dynamics, synthetic image\ngeneration, edge detection, horizon-based OpNav, filtering and\nobservability-enhancing control.\n","authors":["Aditya Arjun Anibha","Kenshiro Oguri"],"pdf_url":"https://arxiv.org/pdf/2501.15806v1.pdf","comment":"38 pages, 16 figures, preprint under journal review"},{"id":"http://arxiv.org/abs/2211.15136v4","updated":"2025-01-27T06:12:13Z","published":"2022-11-28T08:48:58Z","title":"Collective Intelligence for 2D Push Manipulations with Mobile Robots","summary":"  While natural systems often present collective intelligence that allows them\nto self-organize and adapt to changes, the equivalent is missing in most\nartificial systems. We explore the possibility of such a system in the context\nof cooperative 2D push manipulations using mobile robots. Although conventional\nworks demonstrate potential solutions for the problem in restricted settings,\nthey have computational and learning difficulties. More importantly, these\nsystems do not possess the ability to adapt when facing environmental changes.\nIn this work, we show that by distilling a planner derived from a\ndifferentiable soft-body physics simulator into an attention-based neural\nnetwork, our multi-robot push manipulation system achieves better performance\nthan baselines. In addition, our system also generalizes to configurations not\nseen during training and is able to adapt toward task completions when external\nturbulence and environmental changes are applied. Supplementary videos can be\nfound on our project website: https://sites.google.com/view/ciom/home\n","authors":["So Kuroki","Tatsuya Matsushima","Jumpei Arima","Hiroki Furuta","Yutaka Matsuo","Shixiang Shane Gu","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2211.15136v4.pdf","comment":"Published in IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2306.09872v3","updated":"2025-01-27T06:11:58Z","published":"2023-06-14T03:37:55Z","title":"GenORM: Generalizable One-shot Rope Manipulation with Parameter-Aware\n  Policy","summary":"  Due to the inherent uncertainty in their deformability during motion,\nprevious methods in rope manipulation often require hundreds of real-world\ndemonstrations to train a manipulation policy for each rope, even for simple\ntasks such as rope goal reaching, which hinder their applications in our\never-changing world. To address this issue, we introduce GenORM, a framework\nthat allows the manipulation policy to handle different deformable ropes with a\nsingle real-world demonstration. To achieve this, we augment the policy by\nconditioning it on deformable rope parameters and training it with a diverse\nrange of simulated deformable ropes so that the policy can adjust actions based\non different rope parameters. At the time of inference, given a new rope,\nGenORM estimates the deformable rope parameters by minimizing the disparity\nbetween the grid density of point clouds of real-world demonstrations and\nsimulations. With the help of a differentiable physics simulator, we require\nonly a single real-world demonstration. Empirical validations on both simulated\nand real-world rope manipulation setups clearly show that our method can\nmanipulate different ropes with a single demonstration and significantly\noutperforms the baseline in both environments (62% improvement in in-domain\nropes, and 15% improvement in out-of-distribution ropes in simulation, 26%\nimprovement in real-world), demonstrating the effectiveness of our approach in\none-shot rope manipulation.\n","authors":["So Kuroki","Jiaxian Guo","Tatsuya Matsushima","Takuya Okubo","Masato Kobayashi","Yuya Ikeda","Ryosuke Takanami","Paul Yoo","Yutaka Matsuo","Yusuke Iwasawa"],"pdf_url":"https://arxiv.org/pdf/2306.09872v3.pdf","comment":"The extended version of this paper, GenDOM, was published in the 2024\n  IEEE International Conference on Robotics and Automation (ICRA 2024),\n  arXiv:2309.09051"},{"id":"http://arxiv.org/abs/2309.09051v4","updated":"2025-01-27T06:08:19Z","published":"2023-09-16T17:18:23Z","title":"GenDOM: Generalizable One-shot Deformable Object Manipulation with\n  Parameter-Aware Policy","summary":"  Due to the inherent uncertainty in their deformability during motion,\nprevious methods in deformable object manipulation, such as rope and cloth,\noften required hundreds of real-world demonstrations to train a manipulation\npolicy for each object, which hinders their applications in our ever-changing\nworld. To address this issue, we introduce GenDOM, a framework that allows the\nmanipulation policy to handle different deformable objects with only a single\nreal-world demonstration. To achieve this, we augment the policy by\nconditioning it on deformable object parameters and training it with a diverse\nrange of simulated deformable objects so that the policy can adjust actions\nbased on different object parameters. At the time of inference, given a new\nobject, GenDOM can estimate the deformable object parameters with only a single\nreal-world demonstration by minimizing the disparity between the grid density\nof point clouds of real-world demonstrations and simulations in a\ndifferentiable physics simulator. Empirical validations on both simulated and\nreal-world object manipulation setups clearly show that our method can\nmanipulate different objects with a single demonstration and significantly\noutperforms the baseline in both environments (a 62% improvement for in-domain\nropes and a 15% improvement for out-of-distribution ropes in simulation, as\nwell as a 26% improvement for ropes and a 50% improvement for cloths in the\nreal world), demonstrating the effectiveness of our approach in one-shot\ndeformable object manipulation.\n","authors":["So Kuroki","Jiaxian Guo","Tatsuya Matsushima","Takuya Okubo","Masato Kobayashi","Yuya Ikeda","Ryosuke Takanami","Paul Yoo","Yutaka Matsuo","Yusuke Iwasawa"],"pdf_url":"https://arxiv.org/pdf/2309.09051v4.pdf","comment":"Published in the 2024 IEEE International Conference on Robotics and\n  Automation (ICRA 2024). arXiv admin note: substantial text overlap with\n  arXiv:2306.09872"},{"id":"http://arxiv.org/abs/2312.02008v4","updated":"2025-01-27T06:06:17Z","published":"2023-12-04T16:30:19Z","title":"Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for\n  Cooperative Push Manipulation by Mobile Robots","summary":"  Due to the complex interactions between agents, learning multi-agent control\npolicy often requires a prohibited amount of data. This paper aims to enable\nmulti-agent systems to effectively utilize past memories to adapt to novel\ncollaborative tasks in a data-efficient fashion. We propose the Multi-Agent\nCoordination Skill Database, a repository for storing a collection of\ncoordinated behaviors associated with key vectors distinctive to them. Our\nTransformer-based skill encoder effectively captures spatio-temporal\ninteractions that contribute to coordination and provides a unique skill\nrepresentation for each coordinated behavior. By leveraging only a small number\nof demonstrations of the target task, the database enables us to train the\npolicy using a dataset augmented with the retrieved demonstrations.\nExperimental evaluations demonstrate that our method achieves a significantly\nhigher success rate in push manipulation tasks compared with baseline methods\nlike few-shot imitation learning. Furthermore, we validate the effectiveness of\nour retrieve-and-learn framework in a real environment using a team of wheeled\nrobots.\n","authors":["So Kuroki","Mai Nishimura","Tadashi Kozuno"],"pdf_url":"https://arxiv.org/pdf/2312.02008v4.pdf","comment":"Published in the 2024 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2501.14616v2","updated":"2025-01-27T05:44:42Z","published":"2025-01-24T16:33:56Z","title":"QuIP: Experimental design for expensive simulators with many Qualitative\n  factors via Integer Programming","summary":"  The need to explore and/or optimize expensive simulators with many\nqualitative factors arises in broad scientific and engineering problems. Our\nmotivating application lies in path planning - the exploration of feasible\npaths for navigation, which plays an important role in robotics, surgical\nplanning and assembly planning. Here, the feasibility of a path is evaluated\nvia expensive virtual experiments, and its parameter space is typically\ndiscrete and high-dimensional. A carefully selected experimental design is thus\nessential for timely decision-making. We propose here a novel framework, called\nQuIP, for experimental design of Qualitative factors via Integer Programming\nunder a Gaussian process surrogate model with an exchangeable covariance\nfunction. For initial design, we show that its asymptotic D-optimal design can\nbe formulated as a variant of the well-known assignment problem in operations\nresearch, which can be efficiently solved to global optimality using\nstate-of-the-art integer programming solvers. For sequential design\n(specifically, for active learning or black-box optimization), we show that its\ndesign criterion can similarly be formulated as an assignment problem, thus\nenabling efficient and reliable optimization with existing solvers. We then\ndemonstrate the effectiveness of QuIP over existing methods in a suite of path\nplanning experiments and an application to rover trajectory optimization.\n","authors":["Yen-Chun Liu","Simon Mak"],"pdf_url":"https://arxiv.org/pdf/2501.14616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15768v1","updated":"2025-01-27T04:34:37Z","published":"2025-01-27T04:34:37Z","title":"Error-State LQR Formulation for Quadrotor UAV Trajectory Tracking","summary":"  This article presents an error-state Linear Quadratic Regulator (LQR)\nformulation for robust trajectory tracking in quadrotor Unmanned Aerial\nVehicles (UAVs). The proposed approach leverages error-state dynamics and\nemploys exponential coordinates to represent orientation errors, enabling a\nlinearized system representation for real-time control. The control strategy\nintegrates an LQR-based full-state feedback controller for trajectory tracking,\ncombined with a cascaded bodyrate controller to handle actuator dynamics.\nDetailed derivations of the error-state dynamics, the linearization process,\nand the controller design are provided, highlighting the applicability of the\nmethod for precise and stable quadrotor control in dynamic environments.\n","authors":["Micah Reich"],"pdf_url":"https://arxiv.org/pdf/2501.15768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16539v1","updated":"2025-01-27T22:20:48Z","published":"2025-01-27T22:20:48Z","title":"Generalized Mission Planning for Heterogeneous Multi-Robot Teams via\n  LLM-constructed Hierarchical Trees","summary":"  We present a novel mission-planning strategy for heterogeneous multi-robot\nteams, taking into account the specific constraints and capabilities of each\nrobot. Our approach employs hierarchical trees to systematically break down\ncomplex missions into manageable sub-tasks. We develop specialized APIs and\ntools, which are utilized by Large Language Models (LLMs) to efficiently\nconstruct these hierarchical trees. Once the hierarchical tree is generated, it\nis further decomposed to create optimized schedules for each robot, ensuring\nadherence to their individual constraints and capabilities. We demonstrate the\neffectiveness of our framework through detailed examples covering a wide range\nof missions, showcasing its flexibility and scalability.\n","authors":["Piyush Gupta","David Isele","Enna Sachdeva","Pin-Hao Huang","Behzad Dariush","Kwonjoon Lee","Sangjae Bae"],"pdf_url":"https://arxiv.org/pdf/2501.16539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14554v3","updated":"2025-01-27T21:07:49Z","published":"2024-01-25T22:49:13Z","title":"GCBF+: A Neural Graph Control Barrier Function Framework for Distributed\n  Safe Multi-Agent Control","summary":"  Distributed, scalable, and safe control of large-scale multi-agent systems is\na challenging problem. In this paper, we design a distributed framework for\nsafe multi-agent control in large-scale environments with obstacles, where a\nlarge number of agents are required to maintain safety using only local\ninformation and reach their goal locations. We introduce a new class of\ncertificates, termed graph control barrier function (GCBF), which are based on\nthe well-established control barrier function theory for safety guarantees and\nutilize a graph structure for scalable and generalizable distributed control of\nMAS. We develop a novel theoretical framework to prove the safety of an\narbitrary-sized MAS with a single GCBF. We propose a new training framework\nGCBF+ that uses graph neural networks to parameterize a candidate GCBF and a\ndistributed control policy. The proposed framework is distributed and is\ncapable of taking point clouds from LiDAR, instead of actual state information,\nfor real-world robotic applications. We illustrate the efficacy of the proposed\nmethod through various hardware experiments on a swarm of drones with\nobjectives ranging from exchanging positions to docking on a moving target\nwithout collision. Additionally, we perform extensive numerical experiments,\nwhere the number and density of agents, as well as the number of obstacles,\nincrease. Empirical results show that in complex environments with agents with\nnonlinear dynamics (e.g., Crazyflie drones), GCBF+ outperforms the hand-crafted\nCBF-based method with the best performance by up to 20% for relatively\nsmall-scale MAS with up to 256 agents, and leading reinforcement learning (RL)\nmethods by up to 40% for MAS with 1024 agents. Furthermore, the proposed method\ndoes not compromise on the performance, in terms of goal reaching, for\nachieving high safety rates, which is a common trade-off in RL-based methods.\n","authors":["Songyuan Zhang","Oswin So","Kunal Garg","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2401.14554v3.pdf","comment":"20 pages, 15 figures; Accepted by IEEE Transactions on Robotics\n  (T-RO)"},{"id":"http://arxiv.org/abs/2501.16485v1","updated":"2025-01-27T20:41:38Z","published":"2025-01-27T20:41:38Z","title":"Enhanced Position Estimation in Tactile Internet-Enabled Remote Robotic\n  Surgery Using MOESP-Based Kalman Filter","summary":"  Accurately estimating the position of a patient's side robotic arm in real\ntime during remote surgery is a significant challenge, especially within\nTactile Internet (TI) environments. This paper presents a new and efficient\nmethod for position estimation using a Kalman Filter (KF) combined with the\nMultivariable Output-Error State Space (MOESP) method for system\nidentification. Unlike traditional approaches that require prior knowledge of\nthe system's dynamics, this study uses the JIGSAW dataset, a comprehensive\ncollection of robotic surgical data, along with input from the Master Tool\nManipulator (MTM) to derive the state-space model directly. The MOESP method\nallows accurate modeling of the Patient Side Manipulator (PSM) dynamics without\nprior system models, improving the KF's performance under simulated network\nconditions, including delays, jitter, and packet loss. These conditions mimic\nreal-world challenges in Tactile Internet applications. The findings\ndemonstrate the KF's improved resilience and accuracy in state estimation,\nachieving over 95 percent accuracy despite network-induced uncertainties.\n","authors":["Muhammad Hanif Lashari","Wafa Batayneh","Ashfaq Khokhar","Shakil Ahmed"],"pdf_url":"https://arxiv.org/pdf/2501.16485v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.04503"},{"id":"http://arxiv.org/abs/2501.16480v1","updated":"2025-01-27T20:21:18Z","published":"2025-01-27T20:21:18Z","title":"Modular Framework for Uncertainty Prediction in Autonomous Vehicle\n  Motion Forecasting within Complex Traffic Scenarios","summary":"  We propose a modular modeling framework designed to enhance the capture and\nvalidation of uncertainty in autonomous vehicle (AV) trajectory prediction.\nDeparting from traditional deterministic methods, our approach employs a\nflexible, end-to-end differentiable probabilistic encoder-decoder architecture.\nThis modular design allows the encoder and decoder to be trained independently,\nenabling seamless adaptation to diverse traffic scenarios without retraining\nthe entire system. Our key contributions include: (1) a probabilistic heatmap\npredictor that generates context-aware occupancy grids for dynamic forecasting,\n(2) a modular training approach that supports independent component training\nand flexible adaptation, and (3) a structured validation scheme leveraging\nuncertainty metrics to evaluate robustness under high-risk conditions. To\nhighlight the benefits of our framework, we benchmark it against an end-to-end\nbaseline, demonstrating faster convergence, improved stability, and\nflexibility. Experimental results validate these advantages, showcasing the\ncapacity of the framework to efficiently handle complex scenarios while\nensuring reliable predictions and robust uncertainty representation. This\nmodular design offers significant practical utility and scalability for\nreal-world autonomous driving applications.\n","authors":["Han Wang","Yuneil Yeo","Antonio R. Paiva","Jean Utke","Maria Laura Delle Monache"],"pdf_url":"https://arxiv.org/pdf/2501.16480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16445v2","updated":"2025-01-27T20:15:08Z","published":"2024-10-21T19:15:41Z","title":"Automated Planning Domain Inference for Task and Motion Planning","summary":"  Task and motion planning (TAMP) frameworks address long and complex planning\nproblems by integrating high-level task planners with low-level motion\nplanners. However, existing TAMP methods rely heavily on the manual design of\nplanning domains that specify the preconditions and postconditions of all\nhigh-level actions. This paper proposes a method to automate planning domain\ninference from a handful of test-time trajectory demonstrations, reducing the\nreliance on human design. Our approach incorporates a deep learning-based\nestimator that predicts the appropriate components of a domain for a new task\nand a search algorithm that refines this prediction, reducing the size and\nensuring the utility of the inferred domain. Our method is able to generate new\ndomains from minimal demonstrations at test time, enabling robots to handle\ncomplex tasks more efficiently. We demonstrate that our approach outperforms\nbehavior cloning baselines, which directly imitate planner behavior, in terms\nof planning performance and generalization across a variety of tasks.\nAdditionally, our method reduces computational costs and data amount\nrequirements at test time for inferring new planning domains.\n","authors":["Jinbang Huang","Allen Tao","Rozilyn Marco","Miroslav Bogdanovic","Jonathan Kelly","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2410.16445v2.pdf","comment":"Accepted to 2025 International Conference on Robotics and\n  Automation(ICRA) 8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.16458v1","updated":"2025-01-27T19:37:18Z","published":"2025-01-27T19:37:18Z","title":"BiFold: Bimanual Cloth Folding with Language Guidance","summary":"  Cloth folding is a complex task due to the inevitable self-occlusions of\nclothes, their complicated dynamics, and the disparate materials, geometries,\nand textures that garments can have. In this work, we learn folding actions\nconditioned on text commands. Translating high-level, abstract instructions\ninto precise robotic actions requires sophisticated language understanding and\nmanipulation capabilities. To do that, we leverage a pre-trained\nvision-language model and repurpose it to predict manipulation actions. Our\nmodel, BiFold, can take context into account and achieves state-of-the-art\nperformance on an existing language-conditioned folding benchmark. Given the\nlack of annotated bimanual folding data, we devise a procedure to automatically\nparse actions of a simulated dataset and tag them with aligned text\ninstructions. BiFold attains the best performance on our dataset and can\ntransfer to new instructions, garments, and environments.\n","authors":["Oriol Barbany","AdriÃ  ColomÃ©","Carme Torras"],"pdf_url":"https://arxiv.org/pdf/2501.16458v1.pdf","comment":"Accepted at ICRA 2025"},{"id":"http://arxiv.org/abs/2501.16411v1","updated":"2025-01-27T18:59:58Z","published":"2025-01-27T18:59:58Z","title":"PhysBench: Benchmarking and Enhancing Vision-Language Models for\n  Physical World Understanding","summary":"  Understanding the physical world is a fundamental challenge in embodied AI,\ncritical for enabling agents to perform complex tasks and operate safely in\nreal-world environments. While Vision-Language Models (VLMs) have shown great\npromise in reasoning and task planning for embodied agents, their ability to\ncomprehend physical phenomena remains extremely limited. To close this gap, we\nintroduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'\nphysical world understanding capability across a diverse set of tasks.\nPhysBench contains 100,000 entries of interleaved video-image-text data,\ncategorized into four major domains: physical object properties, physical\nobject relationships, physical scene understanding, and physics-based dynamics,\nfurther divided into 19 subclasses and 8 distinct capability dimensions. Our\nextensive experiments, conducted on 75 representative VLMs, reveal that while\nthese models excel in common-sense reasoning, they struggle with understanding\nthe physical world -- likely due to the absence of physical knowledge in their\ntraining data and the lack of embedded physical priors. To tackle the\nshortfall, we introduce PhysAgent, a novel framework that combines the\ngeneralization strengths of VLMs with the specialized expertise of vision\nmodels, significantly enhancing VLMs' physical understanding across a variety\nof tasks, including an 18.4\\% improvement on GPT-4o. Furthermore, our results\ndemonstrate that enhancing VLMs' physical world understanding capabilities can\nhelp embodied agents such as MOKA. We believe that PhysBench and PhysAgent\noffer valuable insights and contribute to bridging the gap between VLMs and\nphysical world understanding.\n","authors":["Wei Chow","Jiageng Mao","Boyi Li","Daniel Seita","Vitor Guizilini","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2501.16411v1.pdf","comment":"ICLR 2025. Project page: https://physbench.github.io/; Dataset:\n  https://huggingface.co/datasets/USC-GVL/PhysBench;"},{"id":"http://arxiv.org/abs/2011.04820v4","updated":"2025-01-27T18:56:16Z","published":"2020-11-09T23:15:31Z","title":"Decentralized Structural-RNN for Robot Crowd Navigation with Deep\n  Reinforcement Learning","summary":"  Safe and efficient navigation through human crowds is an essential capability\nfor mobile robots. Previous work on robot crowd navigation assumes that the\ndynamics of all agents are known and well-defined. In addition, the performance\nof previous methods deteriorates in partially observable environments and\nenvironments with dense crowds. To tackle these problems, we propose\ndecentralized structural-Recurrent Neural Network (DS-RNN), a novel network\nthat reasons about spatial and temporal relationships for robot decision making\nin crowd navigation. We train our network with model-free deep reinforcement\nlearning without any expert supervision. We demonstrate that our model\noutperforms previous methods in challenging crowd navigation scenarios. We\nsuccessfully transfer the policy learned in the simulator to a real-world\nTurtleBot 2i. For more information, please visit the project website at\nhttps://sites.google.com/view/crowdnav-ds-rnn/home.\n","authors":["Shuijing Liu","Peixin Chang","Weihang Liang","Neeloy Chakraborty","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2011.04820v4.pdf","comment":"Published as a conference paper in IEEE International Conference on\n  Robotics and Automation (ICRA), 2021"}]},"2025-01-26T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2501.15659v1","updated":"2025-01-26T19:43:41Z","published":"2025-01-26T19:43:41Z","title":"AirIO: Learning Inertial Odometry with Enhanced IMU Feature\n  Observability","summary":"  Inertial odometry (IO) using only Inertial Measurement Units (IMUs) offers a\nlightweight and cost-effective solution for Unmanned Aerial Vehicle (UAV)\napplications, yet existing learning-based IO models often fail to generalize to\nUAVs due to the highly dynamic and non-linear-flight patterns that differ from\npedestrian motion. In this work, we identify that the conventional practice of\ntransforming raw IMU data to global coordinates undermines the observability of\ncritical kinematic information in UAVs. By preserving the body-frame\nrepresentation, our method achieves substantial performance improvements, with\na 66.7% average increase in accuracy across three datasets. Furthermore,\nexplicitly encoding attitude information into the motion network results in an\nadditional 23.8% improvement over prior results. Combined with a data-driven\nIMU correction model (AirIMU) and an uncertainty-aware Extended Kalman Filter\n(EKF), our approach ensures robust state estimation under aggressive UAV\nmaneuvers without relying on external sensors or control inputs. Notably, our\nmethod also demonstrates strong generalizability to unseen data not included in\nthe training set, underscoring its potential for real-world UAV applications.\n","authors":["Yuheng Qiu","Can Xu","Yutian Chen","Shibo Zhao","Junyi Geng","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2501.15659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15618v1","updated":"2025-01-26T17:54:43Z","published":"2025-01-26T17:54:43Z","title":"Your Learned Constraint is Secretly a Backward Reachable Tube","summary":"  Inverse Constraint Learning (ICL) is the problem of inferring constraints\nfrom safe (i.e., constraint-satisfying) demonstrations. The hope is that these\ninferred constraints can then be used downstream to search for safe policies\nfor new tasks and, potentially, under different dynamics. Our paper explores\nthe question of what mathematical entity ICL recovers. Somewhat surprisingly,\nwe show that both in theory and in practice, ICL recovers the set of states\nwhere failure is inevitable, rather than the set of states where failure has\nalready happened. In the language of safe control, this means we recover a\nbackwards reachable tube (BRT) rather than a failure set. In contrast to the\nfailure set, the BRT depends on the dynamics of the data collection system. We\ndiscuss the implications of the dynamics-conditionedness of the recovered\nconstraint on both the sample-efficiency of policy search and the\ntransferability of learned constraints.\n","authors":["Mohamad Qadri","Gokul Swamy","Jonathan Francis","Michael Kaess","Andrea Bajcsy"],"pdf_url":"https://arxiv.org/pdf/2501.15618v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.15564v1","updated":"2025-01-26T15:49:50Z","published":"2025-01-26T15:49:50Z","title":"Diffusion-Based Planning for Autonomous Driving with Flexible Guidance","summary":"  Achieving human-like driving behaviors in complex open-world environments is\na critical challenge in autonomous driving. Contemporary learning-based\nplanning approaches such as imitation learning methods often struggle to\nbalance competing objectives and lack of safety assurance,due to limited\nadaptability and inadequacy in learning complex multi-modal behaviors commonly\nexhibited in human planning, not to mention their strong reliance on the\nfallback strategy with predefined rules. We propose a novel transformer-based\nDiffusion Planner for closed-loop planning, which can effectively model\nmulti-modal driving behavior and ensure trajectory quality without any\nrule-based refinement. Our model supports joint modeling of both prediction and\nplanning tasks under the same architecture, enabling cooperative behaviors\nbetween vehicles. Moreover, by learning the gradient of the trajectory score\nfunction and employing a flexible classifier guidance mechanism, Diffusion\nPlanner effectively achieves safe and adaptable planning behaviors. Evaluations\non the large-scale real-world autonomous planning benchmark nuPlan and our\nnewly collected 200-hour delivery-vehicle driving dataset demonstrate that\nDiffusion Planner achieves state-of-the-art closed-loop performance with robust\ntransferability in diverse driving styles.\n","authors":["Yinan Zheng","Ruiming Liang","Kexin Zheng","Jinliang Zheng","Liyuan Mao","Jianxiong Li","Weihao Gu","Rui Ai","Shengbo Eben Li","Xianyuan Zhan","Jingjing Liu"],"pdf_url":"https://arxiv.org/pdf/2501.15564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16931v2","updated":"2025-01-26T15:27:59Z","published":"2024-11-25T21:00:46Z","title":"Performance Assessment of Lidar Odometry Frameworks: A Case Study at the\n  Australian Botanic Garden Mount Annan","summary":"  Autonomous vehicles are being tested in diverse environments worldwide.\nHowever, a notable gap exists in evaluating datasets representing natural,\nunstructured environments such as forests or gardens. To address this, we\npresent a study on localisation at the Australian Botanic Garden Mount Annan.\nThis area encompasses open grassy areas, paved pathways, and densely vegetated\nsections with trees and other objects. The dataset was recorded using a\n128-beam LiDAR sensor and GPS and IMU readings to track the ego-vehicle. This\npaper evaluates the performance of two state-of-the-art LiDARinertial odometry\nframeworks, COIN-LIO and LIO-SAM, on this dataset. We analyse trajectory\nestimates in both horizontal and vertical dimensions and assess relative\ntranslation and yaw errors over varying distances. Our findings reveal that\nwhile both frameworks perform adequately in the vertical plane, COINLIO\ndemonstrates superior accuracy in the horizontal plane, particularly over\nextended trajectories. In contrast, LIO-SAM shows increased drift and yaw\nerrors over longer distances.\n","authors":["Mohamed Mourad Ouazghire","Julie Stephany Berrio","Mao Shan","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2411.16931v2.pdf","comment":"The 2024 Australasian Conference on Robotics and Automation (ACRA\n  2024)"},{"id":"http://arxiv.org/abs/2311.01248v5","updated":"2025-01-26T15:03:06Z","published":"2023-11-02T14:02:42Z","title":"Multimodal and Force-Matched Imitation Learning with a See-Through\n  Visuotactile Sensor","summary":"  Contact-rich tasks continue to present many challenges for robotic\nmanipulation. In this work, we leverage a multimodal visuotactile sensor within\nthe framework of imitation learning (IL) to perform contact-rich tasks that\ninvolve relative motion (e.g., slipping and sliding) between the end-effector\nand the manipulated object. We introduce two algorithmic contributions, tactile\nforce matching and learned mode switching, as complimentary methods for\nimproving IL. Tactile force matching enhances kinesthetic teaching by reading\napproximate forces during the demonstration and generating an adapted robot\ntrajectory that recreates the recorded forces. Learned mode switching uses IL\nto couple visual and tactile sensor modes with the learned motion policy,\nsimplifying the transition from reaching to contacting. We perform robotic\nmanipulation experiments on four door-opening tasks with a variety of\nobservation and algorithm configurations to study the utility of multimodal\nvisuotactile sensing and our proposed improvements. Our results show that the\ninclusion of force matching raises average policy success rates by 62.5%,\nvisuotactile mode switching by 30.3%, and visuotactile data as a policy input\nby 42.5%, emphasizing the value of see-through tactile sensing for IL, both for\ndata collection to allow force matching, and for policy execution to enable\naccurate task feedback. Project site: https://papers.starslab.ca/sts-il/\n","authors":["Trevor Ablett","Oliver Limoyo","Adam Sigal","Affan Jilani","Jonathan Kelly","Kaleem Siddiqi","Francois Hogan","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2311.01248v5.pdf","comment":"14 pages, 22 figures"},{"id":"http://arxiv.org/abs/2501.15505v1","updated":"2025-01-26T12:34:48Z","published":"2025-01-26T12:34:48Z","title":"Unveiling the Potential of iMarkers: Invisible Fiducial Markers for\n  Advanced Robotics","summary":"  Fiducial markers are widely used in various robotics tasks, facilitating\nenhanced navigation, object recognition, and scene understanding. Despite their\nadvantages for robots and Augmented Reality (AR) applications, they often\ndisrupt the visual aesthetics of environments because they are visible to\nhumans, making them unsuitable for non-intrusive use cases. To address this\ngap, this paper presents \"iMarkers\"-innovative, unobtrusive fiducial markers\ndetectable exclusively by robots equipped with specialized sensors. These\nmarkers offer high flexibility in production, allowing customization of their\nvisibility range and encoding algorithms to suit various demands. The paper\nalso introduces the hardware designs and software algorithms developed for\ndetecting iMarkers, highlighting their adaptability and robustness in the\ndetection and recognition stages. Various evaluations have demonstrated the\neffectiveness of iMarkers compared to conventional (printed) and blended\nfiducial markers and confirmed their applicability in diverse robotics\nscenarios.\n","authors":["Ali Tourani","Deniz Isinsu Avsar","Hriday Bavle","Jose Luis Sanchez-Lopez","Jan Lagerwall","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2501.15505v1.pdf","comment":"12 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2405.18251v3","updated":"2025-01-26T07:43:30Z","published":"2024-05-28T15:02:09Z","title":"Sensor-Based Distributionally Robust Control for Safe Robot Navigation\n  in Dynamic Environments","summary":"  We introduce a novel method for mobile robot navigation in dynamic, unknown\nenvironments, leveraging onboard sensing and distributionally robust\noptimization to impose probabilistic safety constraints. Our method introduces\na distributionally robust control barrier function (DR-CBF) that directly\nintegrates noisy sensor measurements and state estimates to define safety\nconstraints. This approach is applicable to a wide range of control-affine\ndynamics, generalizable to robots with complex geometries, and capable of\noperating at real-time control frequencies. Coupled with a control Lyapunov\nfunction (CLF) for path following, the proposed CLF-DR-CBF control synthesis\nmethod achieves safe, robust, and efficient navigation in challenging\nenvironments. We demonstrate the effectiveness and robustness of our approach\nfor safe autonomous navigation under uncertainty in simulations and real-world\nexperiments with differential-drive robots.\n","authors":["Kehan Long","Yinzhuang Yi","Zhirui Dai","Sylvia Herbert","Jorge CortÃ©s","Nikolay Atanasov"],"pdf_url":"https://arxiv.org/pdf/2405.18251v3.pdf","comment":"Project page: https://existentialrobotics.org/DRO_Safe_Navigation"},{"id":"http://arxiv.org/abs/2501.15426v1","updated":"2025-01-26T07:06:31Z","published":"2025-01-26T07:06:31Z","title":"FAVbot: An Autonomous Target Tracking Micro-Robot with Frequency\n  Actuation Control","summary":"  Robotic autonomy at centimeter scale requires compact and\nminiaturization-friendly actuation integrated with sensing and neural network\nprocessing assembly within a tiny form factor. Applications of such systems\nhave witnessed significant advancements in recent years in fields such as\nhealthcare, manufacturing, and post-disaster rescue. The system design at this\nscale puts stringent constraints on power consumption for both the sensory\nfront-end and actuation back-end and the weight of the electronic assembly for\nrobust operation. In this paper, we introduce FAVbot, the first autonomous\nmobile micro-robotic system integrated with a novel actuation mechanism and\nconvolutional neural network (CNN) based computer vision - all integrated\nwithin a compact 3-cm form factor. The novel actuation mechanism utilizes\nmechanical resonance phenomenon to achieve frequency-controlled steering with a\nsingle piezoelectric actuator. Experimental results demonstrate the\neffectiveness of FAVbot's frequency-controlled actuation, which offers a\ndiverse selection of resonance modes with different motion characteristics. The\nactuation system is complemented with the vision front-end where a camera along\nwith a microcontroller supports object detection for closed-loop control and\nautonomous target tracking. This enables adaptive navigation in dynamic\nenvironments. This work contributes to the evolving landscape of neural\nnetwork-enabled micro-robotic systems showing the smallest autonomous robot\nbuilt using controllable multi-directional single-actuator mechanism.\n","authors":["Zhijian Hao","Ashwin Lele","Yan Fang","Arijit Raychowdhury","Azadeh Ansari"],"pdf_url":"https://arxiv.org/pdf/2501.15426v1.pdf","comment":"This paper is under consideration for journal publication. Authors\n  reserve the right to transfer copyright without notice"},{"id":"http://arxiv.org/abs/2212.00398v2","updated":"2025-01-26T02:02:40Z","published":"2022-12-01T09:59:53Z","title":"Distributed Model Predictive Covariance Steering","summary":"  This paper proposes Distributed Model Predictive Covariance Steering (DiMPCS)\nfor multi-agent control under stochastic uncertainty. The scope of our approach\nis to blend covariance steering theory, distributed optimization and model\npredictive control (MPC) into a single framework that is safe, scalable and\ndecentralized. Initially, we pose a problem formulation that uses the\nWasserstein distance to steer the state distributions of a multi-agent system\nto desired targets, and probabilistic constraints to ensure safety. We then\ntransform this problem into a finite-dimensional optimization one by utilizing\na disturbance feedback policy parametrization for covariance steering and a\ntractable approximation of the safety constraints. To solve the latter problem,\nwe derive a decentralized consensus-based algorithm using the Alternating\nDirection Method of Multipliers. This method is then extended to a receding\nhorizon form, which yields the proposed DiMPCS algorithm. Simulation\nexperiments on a variety of multi-robot tasks with up to hundreds of robots\ndemonstrate the effectiveness of DiMPCS. The superior scalability and\nperformance of the proposed method is also highlighted through a comparison\nagainst related stochastic MPC approaches. Finally, hardware results on a\nmulti-robot platform also verify the applicability of DiMPCS on real systems. A\nvideo with all results is available in https://youtu.be/tzWqOzuj2kQ.\n","authors":["Augustinos D. Saravanos","Isin M. Balci","Efstathios Bakolas","Evangelos A. Theodorou"],"pdf_url":"https://arxiv.org/pdf/2212.00398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16389v1","updated":"2025-01-26T00:27:04Z","published":"2025-01-26T00:27:04Z","title":"Bridging the Sim2Real Gap: Vision Encoder Pre-Training for Visuomotor\n  Policy Transfer","summary":"  Simulation offers a scalable and efficient alternative to real-world data\ncollection for learning visuomotor robotic policies. However, the\nsimulation-to-reality, or \"Sim2Real\" distribution shift -- introduced by\nemploying simulation-trained policies in real-world environments -- frequently\nprevents successful policy transfer. This study explores the potential of using\nlarge-scale pre-training of vision encoders to address the Sim2Real gap. We\nexamine a diverse collection of encoders, evaluating their ability to (1)\nextract features necessary for robot control while (2) remaining invariant to\ntask-irrelevant environmental variations. We quantitatively measure the\nencoder's feature extraction capabilities through linear probing and its domain\ninvariance by computing distances between simulation and real-world embedding\ncentroids. Additional qualitative insights are provided through t-SNE plots and\nGradCAM saliency maps. Findings suggest that encoders pre-trained on\nmanipulation-specific datasets generally outperform those trained on generic\ndatasets in bridging the Sim2Real gap.\nhttps://github.com/yyardi/Bridging-the-Sim2Real-Gap\n","authors":["Samuel Biruduganti","Yash Yardi","Lars Ankile"],"pdf_url":"https://arxiv.org/pdf/2501.16389v1.pdf","comment":"9 pages, 10 figures, view GitHub for all appendix figures from the\n  study"}]},"2025-01-25T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2410.14164v2","updated":"2025-01-25T22:27:30Z","published":"2024-10-18T04:04:58Z","title":"Optimal DLT-based Solutions for the Perspective-n-Point","summary":"  We propose a modified normalized direct linear transform (DLT) algorithm for\nsolving the perspective-n-point (PnP) problem with much better behavior than\nthe conventional DLT. The modification consists of analytically weighting the\ndifferent measurements in the linear system with a negligible increase in\ncomputational load. Our approach exhibits clear improvements -- in both\nperformance and runtime -- when compared to popular methods such as EPnP, CPnP,\nRPnP, and OPnP. Our new non-iterative solution approaches that of the true\noptimal found via Gauss-Newton optimization, but at a fraction of the\ncomputational cost. Our optimal DLT (oDLT) implementation, as well as the\nexperiments, are released in open source.\n","authors":["SÃ©bastien Henry","John A. Christian"],"pdf_url":"https://arxiv.org/pdf/2410.14164v2.pdf","comment":"8 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2501.15272v1","updated":"2025-01-25T16:49:54Z","published":"2025-01-25T16:49:54Z","title":"Safe and Agile Transportation of Cable-Suspended Payload via Multiple\n  Aerial Robots","summary":"  Transporting a heavy payload using multiple aerial robots (MARs) is an\nefficient manner to extend the load capacity of a single aerial robot. However,\nexisting schemes for the multiple aerial robots transportation system (MARTS)\nstill lack the capability to generate a collision-free and dynamically feasible\ntrajectory in real-time and further track an agile trajectory especially when\nthere are no sensors available to measure the states of payload and cable.\nTherefore, they are limited to low-agility transportation in simple\nenvironments. To bridge the gap, we propose complete planning and control\nschemes for the MARTS, achieving safe and agile aerial transportation (SAAT) of\na cable-suspended payload in complex environments. Flatness maps for the aerial\nrobot considering the complete kinematical constraint and the dynamical\ncoupling between each aerial robot and payload are derived. To improve the\nresponsiveness for the generation of the safe, dynamically feasible, and agile\ntrajectory in complex environments, a real-time spatio-temporal trajectory\nplanning scheme is proposed for the MARTS. Besides, we break away from the\nreliance on the state measurement for both the payload and cable, as well as\nthe closed-loop control for the payload, and propose a fully distributed\ncontrol scheme to track the agile trajectory that is robust against imprecise\npayload mass and non-point mass payload. The proposed schemes are extensively\nvalidated through benchmark comparisons, ablation studies, and simulations.\nFinally, extensive real-world experiments are conducted on a MARTS integrated\nby three aerial robots with onboard computers and sensors. The result validates\nthe efficiency and robustness of our proposed schemes for SAAT in complex\nenvironments.\n","authors":["Yongchao Wang","Junjie Wang","Xiaobin Zhou","Tiankai Yang","Chao Xu","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2501.15272v1.pdf","comment":"20 pages, 14 figures, submitted to IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2501.15214v1","updated":"2025-01-25T13:33:22Z","published":"2025-01-25T13:33:22Z","title":"Zero-shot Robotic Manipulation with Language-guided Instruction and\n  Formal Task Planning","summary":"  Robotic manipulation is often challenging due to the long-horizon tasks and\nthe complex object relationships. A common solution is to develop a task and\nmotion planning framework that integrates planning for high-level task and\nlow-level motion. Recently, inspired by the powerful reasoning ability of Large\nLanguage Models (LLMs), LLM-based planning approaches have achieved remarkable\nprogress. However, these methods still heavily rely on expert-specific\nknowledge, often generating invalid plans for unseen and unfamiliar tasks. To\naddress this issue, we propose an innovative language-guided symbolic task\nplanning (LM-SymOpt) framework with optimization. It is the first expert-free\nplanning framework since we combine the world knowledge from LLMs with formal\nreasoning, resulting in improved generalization capability to new tasks.\nSpecifically, differ to most existing work, our LM-SymOpt employs LLMs to\ntranslate natural language instructions into symbolic representations, thereby\nrepresenting actions as high-level symbols and reducing the search space for\nplanning. Next, after evaluating the action probability of completing the task\nusing LLMs, a weighted random sampling method is introduced to generate\ncandidate plans. Their feasibility is assessed through symbolic reasoning and\ntheir cost efficiency is then evaluated using trajectory optimization for\nselecting the optimal planning. Our experimental results show that LM-SymOpt\noutperforms existing LLM-based planning approaches.\n","authors":["Junfeng Tang","Zihan Ye","Yuping Yan","Ziqi Zheng","Ting Gao","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2501.15214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15198v1","updated":"2025-01-25T12:32:52Z","published":"2025-01-25T12:32:52Z","title":"Towards Conscious Service Robots","summary":"  Deep learning's success in perception, natural language processing, etc.\ninspires hopes for advancements in autonomous robotics. However, real-world\nrobotics face challenges like variability, high-dimensional state spaces,\nnon-linear dependencies, and partial observability. A key issue is\nnon-stationarity of robots, environments, and tasks, leading to performance\ndrops with out-of-distribution data. Unlike current machine learning models,\nhumans adapt quickly to changes and new tasks due to a cognitive architecture\nthat enables systematic generalization and meta-cognition. Human brain's System\n1 handles routine tasks unconsciously, while System 2 manages complex tasks\nconsciously, facilitating flexible problem-solving and self-monitoring. For\nrobots to achieve human-like learning and reasoning, they need to integrate\ncausal models, working memory, planning, and metacognitive processing. By\nincorporating human cognition insights, the next generation of service robots\nwill handle novel situations and monitor themselves to avoid risks and mitigate\nerrors.\n","authors":["Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2501.15198v1.pdf","comment":"In: Science for a Better Tomorrow: Curious 2024 Insights Actions,\n  Springer 2025"},{"id":"http://arxiv.org/abs/2501.15189v1","updated":"2025-01-25T12:01:56Z","published":"2025-01-25T12:01:56Z","title":"Extracting Forward Invariant Sets from Neural Network-Based Control\n  Barrier Functions","summary":"  Training Neural Networks (NNs) to serve as Barrier Functions (BFs) is a\npopular way to improve the safety of autonomous dynamical systems. Despite\nsignificant practical success, these methods are not generally guaranteed to\nproduce true BFs in a provable sense, which undermines their intended use as\nsafety certificates. In this paper, we consider the problem of formally\ncertifying a learned NN as a BF with respect to state avoidance for an\nautonomous system: viz. computing a region of the state space on which the\ncandidate NN is provably a BF. In particular, we propose a sound algorithm that\nefficiently produces such a certificate set for a shallow NN. Our algorithm\ncombines two novel approaches: it first uses NN reachability tools to identify\na subset of states for which the output of the NN does not increase along\nsystem trajectories; then, it uses a novel enumeration algorithm for hyperplane\narrangements to find the intersection of the NN's zero-sub-level set with the\nfirst set of states. In this way, our algorithm soundly finds a subset of\nstates on which the NN is certified as a BF. We further demonstrate the\neffectiveness of our algorithm at certifying for real-world NNs as BFs in two\ncase studies. We complemented these with scalability experiments that\ndemonstrate the efficiency of our algorithm.\n","authors":["Goli Vaisi","James Ferlez","Yasser Shoukry"],"pdf_url":"https://arxiv.org/pdf/2501.15189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09905v3","updated":"2025-01-25T10:43:04Z","published":"2025-01-17T01:32:18Z","title":"SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon\n  Visuomotor Learning","summary":"  We present a low-cost legged mobile manipulation system that solves\nlong-horizon real-world tasks, trained by reinforcement learning purely in\nsimulation. This system is made possible by 1) a hierarchical design of a\nhigh-level policy for visual-mobile manipulation following instructions and a\nlow-level policy for quadruped movement and limb control, 2) a progressive\nexploration and learning approach that leverages privileged task decomposition\ninformation to train the teacher policy for long-horizon tasks, which will\nguide an imitation-based student policy for efficient training of the\nhigh-level visuomotor policy, and 3) a suite of techniques for minimizing\nsim-to-real gaps.\n  In contrast to previous approaches that use high-end equipment, our system\ndemonstrates effective performance with more accessible hardware -\nspecifically, a Unitree Go1 quadruped, a WidowX250S arm, and a single\nwrist-mounted RGB camera - despite the increased challenges of sim-to-real\ntransfer. When fully trained in simulation, a single policy autonomously solves\nlong-horizon tasks such as search, move, grasp, and drop-into, achieving nearly\n80% success. This performance is comparable to that of expert human\nteleoperation on the same tasks but significantly more efficient, operating at\nabout 1.5x the speed. The sim-to-real transfer is fluid across diverse indoor\nand outdoor scenes under varying lighting conditions. Finally, we discuss the\nkey techniques that enable the entire pipeline, including efficient RL training\nand sim-to-real, to work effectively for legged mobile manipulation, and\npresent their ablation results.\n","authors":["Haichao Zhang","Haonan Yu","Le Zhao","Andrew Choi","Qinxun Bai","Break Yang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2501.09905v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15078v1","updated":"2025-01-25T05:13:41Z","published":"2025-01-25T05:13:41Z","title":"Impact-resistant, autonomous robots inspired by tensegrity architecture","summary":"  Future robots will navigate perilous, remote environments with resilience and\nautonomy. Researchers have proposed building robots with compliant bodies to\nenhance robustness, but this approach often sacrifices the autonomous\ncapabilities expected of rigid robots. Inspired by tensegrity architecture, we\nintroduce a tensegrity robot -- a hybrid robot made from rigid struts and\nelastic tendons -- that demonstrates the advantages of compliance and the\nautonomy necessary for task performance. This robot boasts impact resistance\nand autonomy in a field environment and additional advances in the state of the\nart, including surviving harsh impacts from drops (at least 5.7 m), accurately\nreconstructing its shape and orientation using on-board sensors, achieving high\nlocomotion speeds (18 bar lengths per minute), and climbing the steepest\nincline of any tensegrity robot (28 degrees). We characterize the robot's\nlocomotion on unstructured terrain, showcase its autonomous capabilities in\nnavigation tasks, and demonstrate its robustness by rolling it off a cliff.\n","authors":["William R. Johnson III","Xiaonan Huang","Shiyang Lu","Kun Wang","Joran W. Booth","Kostas Bekris","Rebecca Kramer-Bottiglio"],"pdf_url":"https://arxiv.org/pdf/2501.15078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15071v1","updated":"2025-01-25T04:33:43Z","published":"2025-01-25T04:33:43Z","title":"Understanding via Gaze: Gaze-based Task Decomposition for Imitation\n  Learning of Robot Manipulation","summary":"  In imitation learning for robotic manipulation, decomposing object\nmanipulation tasks into multiple semantic actions is essential. This\ndecomposition enables the reuse of learned skills in varying contexts and the\ncombination of acquired skills to perform novel tasks, rather than merely\nreplicating demonstrated motions. Gaze, an evolutionary tool for understanding\nongoing events, plays a critical role in human object manipulation, where it\nstrongly correlates with motion planning. In this study, we propose a simple\nyet robust task decomposition method based on gaze transitions. We hypothesize\nthat an imitation agent's gaze control, fixating on specific landmarks and\ntransitioning between them, naturally segments demonstrated manipulations into\nsub-tasks. Notably, our method achieves consistent task decomposition across\nall demonstrations, which is desirable in contexts such as machine learning.\nUsing teleoperation, a common modality in imitation learning for robotic\nmanipulation, we collected demonstration data for various tasks, applied our\nsegmentation method, and evaluated the characteristics and consistency of the\nresulting sub-tasks. Furthermore, through extensive testing across a wide range\nof hyperparameter variations, we demonstrated that the proposed method\npossesses the robustness necessary for application to different robotic\nsystems.\n","authors":["Ryo Takizawa","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2501.15071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15068v1","updated":"2025-01-25T04:19:33Z","published":"2025-01-25T04:19:33Z","title":"An Atomic Skill Library Construction Method for Data-Efficient Embodied\n  Manipulation","summary":"  Embodied manipulation is a fundamental ability in the realm of embodied\nartificial intelligence. Although current embodied manipulation models show\ncertain generalizations in specific settings, they struggle in new environments\nand tasks due to the complexity and diversity of real-world scenarios. The\ntraditional end-to-end data collection and training manner leads to significant\ndata demands, which we call ``data explosion''. To address the issue, we\nintroduce a three-wheeled data-driven method to build an atomic skill library.\nWe divide tasks into subtasks using the Vision-Language Planning (VLP). Then,\natomic skill definitions are formed by abstracting the subtasks. Finally, an\natomic skill library is constructed via data collection and\nVision-Language-Action (VLA) fine-tuning. As the atomic skill library expands\ndynamically with the three-wheel update strategy, the range of tasks it can\ncover grows naturally. In this way, our method shifts focus from end-to-end\ntasks to atomic skills, significantly reducing data costs while maintaining\nhigh performance and enabling efficient adaptation to new tasks. Extensive\nexperiments in real-world settings demonstrate the effectiveness and efficiency\nof our approach.\n","authors":["Dongjiang Li","Bo Peng","Chang Li","Ning Qiao","Qi Zheng","Lei Sun","Yusen Qin","Bangguo Li","Yifeng Luan","Yibing Zhan","Mingang Sun","Tong Xu","Lusong Li","Hui Shen","Xiaodong He"],"pdf_url":"https://arxiv.org/pdf/2501.15068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16087v5","updated":"2025-01-25T04:11:34Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neuro-Symbolic Learning Framework\n  for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneuro-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05585v2","updated":"2025-01-25T03:16:03Z","published":"2024-10-08T00:58:42Z","title":"Towards Robust Spacecraft Trajectory Optimization via Transformers","summary":"  Future multi-spacecraft missions require robust autonomous trajectory\noptimization capabilities to ensure safe and efficient rendezvous operations.\nThis capability hinges on solving non-convex optimal control problems in\nreal-time, although traditional iterative methods such as sequential convex\nprogramming impose significant computational challenges. To mitigate this\nburden, the Autonomous Rendezvous Transformer (ART) introduced a generative\nmodel trained to provide near-optimal initial guesses. This approach provides\nconvergence to better local optima (e.g., fuel optimality), improves\nfeasibility rates, and results in faster convergence speed of optimization\nalgorithms through warm-starting. This work extends the capabilities of ART to\naddress robust chance-constrained optimal control problems. Specifically, ART\nis applied to challenging rendezvous scenarios in Low Earth Orbit (LEO),\nensuring fault-tolerant behavior under uncertainty. Through extensive\nexperimentation, the proposed warm-starting strategy is shown to consistently\nproduce high-quality reference trajectories, achieving up to 30\\% cost\nimprovement and 50\\% reduction in infeasible cases compared to conventional\nmethods, demonstrating robust performance across multiple state\nrepresentations. Additionally, a post hoc evaluation framework is proposed to\nassess the quality of generated trajectories and mitigate runtime failures,\nmarking an initial step toward the reliable deployment of AI-driven solutions\nin safety-critical autonomous systems such as spacecraft.\n","authors":["Yuji Takubo","Tommaso Guffanti","Daniele Gammelli","Marco Pavone","Simone D'Amico"],"pdf_url":"https://arxiv.org/pdf/2410.05585v2.pdf","comment":"Submitted to the IEEE Aerospace Conference 2025. 13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.01831v2","updated":"2025-01-25T01:50:30Z","published":"2025-01-03T14:32:17Z","title":"Online Fault Tolerance Strategy for Abrupt Reachability Constraint\n  Changes","summary":"  When a system's constraints change abruptly, the system's reachability safety\ndoes no longer sustain. Thus, the system can reach a forbidden/dangerous value.\nConventional remedy practically involves online controller redesign (OCR) to\nre-establish the reachability's compliance with the new constraints, which,\nhowever, is usually too slow. There is a need for an online strategy capable of\nmanaging runtime changes in reachability constraints. However, to the best of\nthe authors' knowledge, this topic has not been addressed in the existing\nliterature. In this paper, we propose a fast fault tolerance strategy to\nrecover the system's reachability safety in runtime. Instead of redesigning the\nsystem's controller, we propose to change the system's reference state to\nmodify the system's reachability to comply with the new constraints. We frame\nthe reference state search as an optimization problem and employ the\nKarush-Kuhn-Tucker (KKT) method as well as the Interior Point Method (IPM)\nbased Newton's method (as a fallback for the KKT method) for fast solution\nderivation. The optimization also allows more future fault tolerance. Numerical\nsimulations demonstrate that our method outperforms the conventional OCR method\nin terms of computational efficiency and success rate. Specifically, the\nresults show that the proposed method finds a solution $10^{2}$ (with the IPM\nbased Newton's method) $\\sim 10^{4}$ (with the KKT method) times faster than\nthe OCR method. Additionally, the improvement rate of the success rate of our\nmethod over the OCR method is $40.81\\%$ without considering the deadline of run\ntime. The success rate remains at $49.44\\%$ for the proposed method, while it\nbecomes $0\\%$ for the OCR method when a deadline of $1.5 \\; seconds$ is\nimposed.\n","authors":["Henghua Shen","Qixin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.01831v2.pdf","comment":"9 pages, 2 figures,"},{"id":"http://arxiv.org/abs/2405.14005v2","updated":"2025-01-25T00:38:28Z","published":"2024-05-22T21:22:44Z","title":"Neural Scaling Laws in Robotics","summary":"  Neural scaling laws have driven significant advancements in machine learning,\nparticularly in domains like language modeling and computer vision. However,\nthe exploration of neural scaling laws within robotics has remained relatively\nunderexplored, despite the growing adoption of foundation models in this field.\nThis paper represents the first comprehensive study to quantify neural scaling\nlaws for Robot Foundation Models (RFMs) and Large Language Models (LLMs) in\nrobotics tasks. Through a meta-analysis of 327 research papers, we investigate\nhow data size, model size, and compute resources influence downstream\nperformance across a diverse set of robotic tasks. Consistent with previous\nscaling law research, our results reveal that the performance of robotic models\nimproves with increased resources, following a power-law relationship.\nPromisingly, the improvement in robotic task performance scales notably faster\nthan language tasks. This suggests that, while performance on downstream\nrobotic tasks today is often moderate-to-poor, increased data and compute are\nlikely to signficantly improve performance in the future. Also consistent with\nprevious scaling law research, we also observe the emergence of new robot\ncapabilities as models scale.\n","authors":["Sebastian Sartor","Neil Thompson"],"pdf_url":"https://arxiv.org/pdf/2405.14005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14992v1","updated":"2025-01-25T00:00:11Z","published":"2025-01-25T00:00:11Z","title":"Extensive Exploration in Complex Traffic Scenarios using Hierarchical\n  Reinforcement Learning","summary":"  Developing an automated driving system capable of navigating complex traffic\nenvironments remains a formidable challenge. Unlike rule-based or supervised\nlearning-based methods, Deep Reinforcement Learning (DRL) based controllers\neliminate the need for domain-specific knowledge and datasets, thus providing\nadaptability to various scenarios. Nonetheless, a common limitation of existing\nstudies on DRL-based controllers is their focus on driving scenarios with\nsimple traffic patterns, which hinders their capability to effectively handle\ncomplex driving environments with delayed, long-term rewards, thus compromising\nthe generalizability of their findings. In response to these limitations, our\nresearch introduces a pioneering hierarchical framework that efficiently\ndecomposes intricate decision-making problems into manageable and interpretable\nsubtasks. We adopt a two step training process that trains the high-level\ncontroller and low-level controller separately. The high-level controller\nexhibits an enhanced exploration potential with long-term delayed rewards, and\nthe low-level controller provides longitudinal and lateral control ability\nusing short-term instantaneous rewards. Through simulation experiments, we\ndemonstrate the superiority of our hierarchical controller in managing complex\nhighway driving situations.\n","authors":["Zhihao Zhang","Ekim Yurtsever","Keith A. Redmill"],"pdf_url":"https://arxiv.org/pdf/2501.14992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1802.03498v7","updated":"2025-01-25T15:40:02Z","published":"2018-02-10T01:54:39Z","title":"The Strange Attractor Model of Bipedal Locomotion and its Consequences\n  on Motor Control","summary":"  Despite decades of study, many unknowns exist about the mechanisms governing\nhuman locomotion. Current models and motor control theories can only partially\ncapture the phenomenon. This may be a major cause of the reduced efficacy of\nlower limb rehabilitation therapies. Recently, it has been proposed that human\nlocomotion can be planned in the task-space by taking advantage of the\ngravitational pull acting on the Centre of Mass (CoM) by modelling the\nattractor dynamics. The model proposed represents the CoM transversal\ntrajectory as a harmonic oscillator propagating on the attractor manifold.\nHowever, the vertical trajectory of the CoM, controlled through ankle\nstrategies, has not been accurately captured yet. Research Questions: Is it\npossible to improve the model accuracy by introducing a mathematical model of\nthe ankle strategies by coordinating the heel-strike and toe-off strategies\nwith the CoM movement? Our solution consists of closed-form equations that plan\nhuman-like trajectories for the CoM, the foot swing, and the ankle strategies.\nWe have tested our model by extracting the biomechanics data and postural\nduring locomotion from the motion capture trajectories of 12 healthy subjects\nat 3 self-selected speeds to generate a virtual subject using our model. Our\nvirtual subject has been based on the average of the collected data. The model\noutput shows our virtual subject has walking trajectories that have their\nfeatures consistent with our motion capture data. Additionally, it emerged from\nthe data analysis that our model regulates the stance phase of the foot as\nhumans do. The model proves that locomotion can be modelled as an attractor\ndynamics, proving the existence of a nonlinear map that our nervous system\nlearns. It can support a deeper investigation of locomotion motor control,\npotentially improving locomotion rehabilitation and assistive technologies.\n","authors":["Carlo Tiseo","Ming Jeat Foo","Kalyana C Veluvolu","Arturo Forner-Cordero","Wei Tech Ang"],"pdf_url":"https://arxiv.org/pdf/1802.03498v7.pdf","comment":null}]},"2025-01-28T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2402.18393v3","updated":"2025-01-28T17:36:51Z","published":"2024-02-28T15:13:33Z","title":"Decictor: Towards Evaluating the Robustness of Decision-Making in\n  Autonomous Driving Systems","summary":"  Autonomous Driving System (ADS) testing is crucial in ADS development, with\nthe current primary focus being on safety. However, the evaluation of\nnon-safety-critical performance, particularly the ADS's ability to make optimal\ndecisions and produce optimal paths for autonomous vehicles (AVs), is also\nvital to ensure the intelligence and reduce risks of AVs. Currently, there is\nlittle work dedicated to assessing the robustness of ADSs' path-planning\ndecisions (PPDs), i.e., whether an ADS can maintain the optimal PPD after an\ninsignificant change in the environment. The key challenges include the lack of\nclear oracles for assessing PPD optimality and the difficulty in searching for\nscenarios that lead to non-optimal PPDs. To fill this gap, in this paper, we\nfocus on evaluating the robustness of ADSs' PPDs and propose the first method,\nDecictor, for generating non-optimal decision scenarios (NoDSs), where the ADS\ndoes not plan optimal paths for AVs. Decictor comprises three main components:\nNon-invasive Mutation, Consistency Check, and Feedback. To overcome the oracle\nchallenge, Non-invasive Mutation is devised to implement conservative\nmodifications, ensuring the preservation of the original optimal path in the\nmutated scenarios. Subsequently, the Consistency Check is applied to determine\nthe presence of non-optimal PPDs by comparing the driving paths in the original\nand mutated scenarios. To deal with the challenge of large environment space,\nwe design Feedback metrics that integrate spatial and temporal dimensions of\nthe AV's movement. These metrics are crucial for effectively steering the\ngeneration of NoDSs. We evaluate Decictor on Baidu Apollo, an open-source and\nproduction-grade ADS. The experimental results validate the effectiveness of\nDecictor in detecting non-optimal PPDs of ADSs.\n","authors":["Mingfei Cheng","Yuan Zhou","Xiaofei Xie","Junjie Wang","Guozhu Meng","Kairui Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18393v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07688v2","updated":"2025-01-28T16:02:30Z","published":"2024-10-10T07:54:17Z","title":"PokeFlex: A Real-World Dataset of Volumetric Deformable Objects for\n  Robotics","summary":"  Data-driven methods have shown great potential in solving challenging\nmanipulation tasks; however, their application in the domain of deformable\nobjects has been constrained, in part, by the lack of data. To address this\nlack, we propose PokeFlex, a dataset featuring real-world multimodal data that\nis paired and annotated. The modalities include 3D textured meshes, point\nclouds, RGB images, and depth maps. Such data can be leveraged for several\ndownstream tasks, such as online 3D mesh reconstruction, and it can potentially\nenable underexplored applications such as the real-world deployment of\ntraditional control methods based on mesh simulations. To deal with the\nchallenges posed by real-world 3D mesh reconstruction, we leverage a\nprofessional volumetric capture system that allows complete 360{\\deg}\nreconstruction. PokeFlex consists of 18 deformable objects with varying\nstiffness and shapes. Deformations are generated by dropping objects onto a\nflat surface or by poking the objects with a robot arm. Interaction wrenches\nand contact locations are also reported for the latter case. Using different\ndata modalities, we demonstrated a use case for our dataset training models\nthat, given the novelty of the multimodal nature of Pokeflex, constitute the\nstate-of-the-art in multi-object online template-based mesh reconstruction from\nmultimodal data, to the best of our knowledge. We refer the reader to our\nwebsite ( https://pokeflex-dataset.github.io/ ) for further demos and examples.\n","authors":["Jan Obrist","Miguel Zamora","Hehui Zheng","Ronan Hinchet","Firat Ozdemir","Juan Zarate","Robert K. Katzschmann","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2410.07688v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2501.17022v1","updated":"2025-01-28T15:39:07Z","published":"2025-01-28T15:39:07Z","title":"Mobile Manipulation Instruction Generation from Multiple Images with\n  Automatic Metric Enhancement","summary":"  We consider the problem of generating free-form mobile manipulation\ninstructions based on a target object image and receptacle image. Conventional\nimage captioning models are not able to generate appropriate instructions\nbecause their architectures are typically optimized for single-image. In this\nstudy, we propose a model that handles both the target object and receptacle to\ngenerate free-form instruction sentences for mobile manipulation tasks.\nMoreover, we introduce a novel training method that effectively incorporates\nthe scores from both learning-based and n-gram based automatic evaluation\nmetrics as rewards. This method enables the model to learn the co-occurrence\nrelationships between words and appropriate paraphrases. Results demonstrate\nthat our proposed method outperforms baseline methods including representative\nmultimodal large language models on standard automatic evaluation metrics.\nMoreover, physical experiments reveal that using our method to augment data on\nlanguage instructions improves the performance of an existing multimodal\nlanguage understanding model for mobile manipulation.\n","authors":["Kei Katsumata","Motonari Kambara","Daichi Yashima","Ryosuke Korekata","Komei Sugiura"],"pdf_url":"https://arxiv.org/pdf/2501.17022v1.pdf","comment":"Accepted for IEEE RA-L 2025"},{"id":"http://arxiv.org/abs/2501.17018v1","updated":"2025-01-28T15:32:41Z","published":"2025-01-28T15:32:41Z","title":"Six-Degree-of-Freedom Motion Emulation for Data-Driven Modeling of\n  Underwater Vehicles","summary":"  This article presents a collaborative research effort aimed at developing a\nnovel six-degree-of-freedom (6-DOF) motion platform for the empirical\ncharacterization of hydrodynamic forces crucial for the control and stability\nof surface and subsurface vehicles. Traditional experimental methods, such as\nthe Planar Motion Mechanism (PMM), are limited by the number of simultaneously\narticulated DOFs and are limited to single-frequency testing, making such\nsystems impractical for resolving frequency-dependent added mass or damping\nmatrices. The 6 DOF platform, termed a hexapod, overcomes these limitations by\noffering enhanced maneuverability and the ability to test broad-banded\nfrequency spectra in multiple degrees of freedom in a single experiment.\n","authors":["Juliana Danesi Ruiz","Michael Swafford","Austin Krebill","Rachel Vitali","Casey Harwood"],"pdf_url":"https://arxiv.org/pdf/2501.17018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17015v1","updated":"2025-01-28T15:26:25Z","published":"2025-01-28T15:26:25Z","title":"Revisit Mixture Models for Multi-Agent Simulation: Experimental Study\n  within a Unified Framework","summary":"  Simulation plays a crucial role in assessing autonomous driving systems,\nwhere the generation of realistic multi-agent behaviors is a key aspect. In\nmulti-agent simulation, the primary challenges include behavioral multimodality\nand closed-loop distributional shifts. In this study, we revisit mixture models\nfor generating multimodal agent behaviors, which can cover the mainstream\nmethods including continuous mixture models and GPT-like discrete models.\nFurthermore, we introduce a closed-loop sample generation approach tailored for\nmixture models to mitigate distributional shifts. Within the unified mixture\nmodel~(UniMM) framework, we recognize critical configurations from both model\nand data perspectives. We conduct a systematic examination of various model\nconfigurations, including positive component matching, continuous regression,\nprediction horizon, and the number of components. Moreover, our investigation\ninto the data configuration highlights the pivotal role of closed-loop samples\nin achieving realistic simulations. To extend the benefits of closed-loop\nsamples across a broader range of mixture models, we further address the\nshortcut learning and off-policy learning issues. Leveraging insights from our\nexploration, the distinct variants proposed within the UniMM framework,\nincluding discrete, anchor-free, and anchor-based models, all achieve\nstate-of-the-art performance on the WOSAC benchmark.\n","authors":["Longzhong Lin","Xuewu Lin","Kechun Xu","Haojian Lu","Lichao Huang","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2501.17015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16997v1","updated":"2025-01-28T14:52:10Z","published":"2025-01-28T14:52:10Z","title":"MAUCell: An Adaptive Multi-Attention Framework for Video Frame\n  Prediction","summary":"  Temporal sequence modeling stands as the fundamental foundation for video\nprediction systems and real-time forecasting operations as well as anomaly\ndetection applications. The achievement of accurate predictions through\nefficient resource consumption remains an ongoing issue in contemporary\ntemporal sequence modeling. We introduce the Multi-Attention Unit (MAUCell)\nwhich combines Generative Adversarial Networks (GANs) and spatio-temporal\nattention mechanisms to improve video frame prediction capabilities. Our\napproach implements three types of attention models to capture intricate motion\nsequences. A dynamic combination of these attention outputs allows the model to\nreach both advanced decision accuracy along with superior quality while\nremaining computationally efficient. The integration of GAN elements makes\ngenerated frames appear more true to life therefore the framework creates\noutput sequences which mimic real-world footage. The new design system\nmaintains equilibrium between temporal continuity and spatial accuracy to\ndeliver reliable video prediction. Through a comprehensive evaluation\nmethodology which merged the perceptual LPIPS measurement together with classic\ntests MSE, MAE, SSIM and PSNR exhibited enhancing capabilities than\ncontemporary approaches based on direct benchmark tests of Moving MNIST, KTH\nAction, and CASIA-B (Preprocessed) datasets. Our examination indicates that\nMAUCell shows promise for operational time requirements. The research findings\ndemonstrate how GANs work best with attention mechanisms to create better\napplications for predicting video sequences.\n","authors":["Shreyam Gupta","P. Agrawal","Priyam Gupta"],"pdf_url":"https://arxiv.org/pdf/2501.16997v1.pdf","comment":"This work has been submitted to the IJCAI 2025 Conference for review.\n  It contains: 11 pages, 4 figures, 7 tables, and 3 Algorithms"},{"id":"http://arxiv.org/abs/2501.16973v1","updated":"2025-01-28T14:14:02Z","published":"2025-01-28T14:14:02Z","title":"Towards Open-Source and Modular Space Systems with ATMOS","summary":"  In the near future, autonomous space systems will compose a large number of\nthe spacecraft being deployed. Their tasks will involve autonomous rendezvous\nand proximity operations with large structures, such as inspections or assembly\nof orbiting space stations and maintenance and human-assistance tasks over\nshared workspaces. To promote replicable and reliable scientific results for\nautonomous control of spacecraft, we present the design of a space systems\nlaboratory based on open-source and modular software and hardware. The\nsimulation software provides a software-in-the-loop (SITL) architecture that\nseamlessly transfers simulated results to the ATMOS platforms, developed for\ntesting of multi-agent autonomy schemes for microgravity. The manuscript\npresents the KTH space systems laboratory facilities and the ATMOS platform as\nopen-source hardware and software contributions. Preliminary results showcase\nSITL and real testing.\n","authors":["Pedro Roque","Sujet Phodapol","Elias Krantz","Jaeyoung Lim","Joris Verhagen","Frank Jiang","David Dorner","Roland Siegwart","Ivan Stenius","Gunnar Tibert","Huina Mao","Jana Tumova","Christer Fuglesang","Dimos V. Dimarogonas"],"pdf_url":"https://arxiv.org/pdf/2501.16973v1.pdf","comment":"Preliminary release, to be submitted"},{"id":"http://arxiv.org/abs/2501.16947v1","updated":"2025-01-28T13:46:01Z","published":"2025-01-28T13:46:01Z","title":"Image-based Geo-localization for Robotics: Are Black-box Vision-Language\n  Models there yet?","summary":"  The advances in Vision-Language models (VLMs) offer exciting opportunities\nfor robotic applications involving image geo-localization, the problem of\nidentifying the geo-coordinates of a place based on visual data only. Recent\nresearch works have focused on using a VLM as embeddings extractor for\ngeo-localization, however, the most sophisticated VLMs may only be available as\nblack boxes that are accessible through an API, and come with a number of\nlimitations: there is no access to training data, model features and gradients;\nretraining is not possible; the number of predictions may be limited by the\nAPI; training on model outputs is often prohibited; and queries are open-ended.\nThe utilization of a VLM as a stand-alone, zero-shot geo-localization system\nusing a single text-based prompt is largely unexplored. To bridge this gap,\nthis paper undertakes the first systematic study, to the best of our knowledge,\nto investigate the potential of some of the state-of-the-art VLMs as\nstand-alone, zero-shot geo-localization systems in a black-box setting with\nrealistic constraints. We consider three main scenarios for this thorough\ninvestigation: a) fixed text-based prompt; b) semantically-equivalent\ntext-based prompts; and c) semantically-equivalent query images. We also take\ninto account the auto-regressive and probabilistic generation process of the\nVLMs when investigating their utility for geo-localization task by using model\nconsistency as a metric in addition to traditional accuracy. Our work provides\nnew insights in the capabilities of different VLMs for the above-mentioned\nscenarios.\n","authors":["Sania Waheed","Bruno Ferrarini","Michael Milford","Sarvapali D. Ramchurn","Shoaib Ehsan"],"pdf_url":"https://arxiv.org/pdf/2501.16947v1.pdf","comment":"Submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2501.16929v1","updated":"2025-01-28T13:18:27Z","published":"2025-01-28T13:18:27Z","title":"Giving Sense to Inputs: Toward an Accessible Control Framework for\n  Shared Autonomy","summary":"  While shared autonomy offers significant potential for assistive robotics,\nkey questions remain about how to effectively map 2D control inputs to 6D robot\nmotions. An intuitive framework should allow users to input commands\neffortlessly, with the robot responding as expected, without users needing to\nanticipate the impact of their inputs. In this article, we propose a dynamic\ninput mapping framework that links joystick movements to motions on control\nframes defined along a trajectory encoded with canal surfaces. We evaluate our\nmethod in a user study with 20 participants, demonstrating that our input\nmapping framework reduces the workload and improves usability compared to a\nbaseline mapping with similar motion encoding. To prepare for deployment in\nassistive scenarios, we built on the development from the accessible gaming\ncommunity to select an accessible control interface. We then tested the system\nin an exploratory study, where three wheelchair users controlled the robot for\nboth daily living activities and a creative painting task, demonstrating its\nfeasibility for users closer to our target population.\n","authors":["Shalutha Rajapakshe","Jean-Marc Odobez","Emmanuel Senft"],"pdf_url":"https://arxiv.org/pdf/2501.16929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16899v1","updated":"2025-01-28T12:35:06Z","published":"2025-01-28T12:35:06Z","title":"RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with\n  Enhanced Contextual Awareness in Specific Domains","summary":"  Large language models (LLMs) represent a significant advancement in\nintegrating physical robots with AI-driven systems. We showcase the\ncapabilities of our framework within the context of the real-world household\ncompetition. This research introduces a framework that utilizes RDMM (Robotics\nDecision-Making Models), which possess the capacity for decision-making within\ndomain-specific contexts, as well as an awareness of their personal knowledge\nand capabilities. The framework leverages information to enhance the autonomous\ndecision-making of the system. In contrast to other approaches, our focus is on\nreal-time, on-device solutions, successfully operating on hardware with as\nlittle as 8GB of memory. Our framework incorporates visual perception models\nequipping robots with understanding of their environment. Additionally, the\nframework has integrated real-time speech recognition capabilities, thus\nenhancing the human-robot interaction experience. Experimental results\ndemonstrate that the RDMM framework can plan with an 93\\% accuracy.\nFurthermore, we introduce a new dataset consisting of 27k planning instances,\nas well as 1.3k text-image annotated samples derived from the competition. The\nframework, benchmarks, datasets, and models developed in this work are publicly\navailable on our GitHub repository at https://github.com/shadynasrat/RDMM.\n","authors":["Shady Nasrat","Myungsu Kim","Seonil Lee","Jiho Lee","Yeoncheol Jang","Seung-joon Yi"],"pdf_url":"https://arxiv.org/pdf/2501.16899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16868v1","updated":"2025-01-28T11:39:02Z","published":"2025-01-28T11:39:02Z","title":"Event-Based Adaptive Koopman Framework for Optic Flow-Guided Landing on\n  Moving Platforms","summary":"  This paper presents an optic flow-guided approach for achieving soft landings\nby resource-constrained unmanned aerial vehicles (UAVs) on dynamic platforms.\nAn offline data-driven linear model based on Koopman operator theory is\ndeveloped to describe the underlying (nonlinear) dynamics of optic flow output\nobtained from a single monocular camera that maps to vehicle acceleration as\nthe control input. Moreover, a novel adaptation scheme within the Koopman\nframework is introduced online to handle uncertainties such as unknown platform\nmotion and ground effect, which exert a significant influence during the\nterminal stage of the descent process. Further, to minimize computational\noverhead, an event-based adaptation trigger is incorporated into an\nevent-driven Model Predictive Control (MPC) strategy to regulate optic flow and\ntrack a desired reference. A detailed convergence analysis ensures global\nconvergence of the tracking error to a uniform ultimate bound while ensuring\nZeno-free behavior. Simulation results demonstrate the algorithm's robustness\nand effectiveness in landing on dynamic platforms under ground effect and\nsensor noise, which compares favorably to non-adaptive event-triggered and\ntime-triggered adaptive schemes.\n","authors":["Bazeela Banday","Chandan Kumar Sah","Jishnu Keshavan"],"pdf_url":"https://arxiv.org/pdf/2501.16868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07594v2","updated":"2025-01-28T09:32:08Z","published":"2024-04-11T09:23:44Z","title":"Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Tool\n  Segmentation in Robot-Assisted Cardiovascular Catheterization","summary":"  Robot-assisted catheterization has garnered a good attention for its\npotentials in treating cardiovascular diseases. However, advancing\nsurgeon-robot collaboration still requires further research, particularly on\ntask-specific automation. For instance, automated tool segmentation can assist\nsurgeons in visualizing and tracking of endovascular tools during cardiac\nprocedures. While learning-based models have demonstrated state-of-the-art\nsegmentation performances, generating ground-truth labels for fully-supervised\nmethods is both labor-intensive time consuming, and costly. In this study, we\npropose a weakly-supervised learning method with multi-lateral pseudo labeling\nfor tool segmentation in cardiovascular angiogram datasets. The method utilizes\na modified U-Net architecture featuring one encoder and multiple laterally\nbranched decoders. The decoders generate diverse pseudo labels under different\nperturbations, augmenting available partial labels. The pseudo labels are\nself-generated using a mixed loss function with shared consistency across the\ndecoders. The weakly-supervised model was trained end-to-end and validated\nusing partially annotated angiogram data from three cardiovascular\ncatheterization procedures. Validation results show that the model could\nperform closer to fully-supervised models. Also, the proposed weakly-supervised\nmulti-lateral method outperforms three well known methods used for\nweakly-supervised learning, offering the highest segmentation performance\nacross the three angiogram datasets. Furthermore, numerous ablation studies\nconfirmed the model's consistent performance under different parameters.\nFinally, the model was applied for tool segmentation in a robot-assisted\ncatheterization experiments. The model enhanced visualization with high\nconnectivity indices for guidewire and catheter, and a mean processing time of\n35 ms per frame.\n","authors":["Olatunji Mumini Omisore","Toluwanimi Akinyemi","Anh Nguyen","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15830v2","updated":"2025-01-28T09:25:31Z","published":"2025-01-27T07:34:33Z","title":"SpatialVLA: Exploring Spatial Representations for Visual-Language-Action\n  Model","summary":"  In this paper, we claim that spatial understanding is the keypoint in robot\nmanipulation, and propose SpatialVLA to explore effective spatial\nrepresentations for the robot foundation model. Specifically, we introduce\nEgo3D Position Encoding to inject 3D information into the input observations of\nthe visual-language-action model, and propose Adaptive Action Grids to\nrepresent spatial robot movement actions with adaptive discretized action\ngrids, facilitating learning generalizable and transferrable spatial action\nknowledge for cross-robot control. SpatialVLA is first pre-trained on top of a\nvision-language model with 1.1 Million real-world robot episodes, to learn a\ngeneralist manipulation policy across multiple robot environments and tasks.\nAfter pre-training, SpatialVLA is directly applied to perform numerous tasks in\na zero-shot manner. The superior results in both simulation and real-world\nrobots demonstrate its advantage of inferring complex robot motion trajectories\nand its strong in-domain multi-task generalization ability. We further show the\nproposed Adaptive Action Grids offer a new and effective way to fine-tune the\npre-trained SpatialVLA model for new simulation and real-world setups, where\nthe pre-learned action grids are re-discretized to capture robot-specific\nspatial action movements of new setups. The superior results from extensive\nevaluations demonstrate the exceptional in-distribution generalization and\nout-of-distribution adaptation capability, highlighting the crucial benefit of\nthe proposed spatial-aware representations for generalist robot policy\nlearning. All the details and codes will be open-sourced.\n","authors":["Delin Qu","Haoming Song","Qizhi Chen","Yuanqi Yao","Xinyi Ye","Yan Ding","Zhigang Wang","JiaYuan Gu","Bin Zhao","Dong Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2501.15830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10991v2","updated":"2025-01-28T09:21:46Z","published":"2025-01-19T09:23:57Z","title":"Front Hair Styling Robot System Using Path Planning for Root-Centric\n  Strand Adjustment","summary":"  Hair styling is a crucial aspect of personal grooming, significantly\ninfluenced by the appearance of front hair. While brushing is commonly used\nboth to detangle hair and for styling purposes, existing research primarily\nfocuses on robotic systems for detangling hair, with limited exploration into\nrobotic hair styling. This research presents a novel robotic system designed to\nautomatically adjust front hairstyles, with an emphasis on path planning for\nroot-centric strand adjustment. The system utilizes images to compare the\ncurrent hair state with the desired target state through an orientation map of\nhair strands. By concentrating on the differences in hair orientation and\nspecifically targeting adjustments at the root of each strand, the system\nperforms detailed styling tasks. The path planning approach ensures effective\nalignment of the hairstyle with the target, and a closed-loop mechanism refines\nthese adjustments to accurately evolve the hairstyle towards the desired\noutcome. Experimental results demonstrate that the proposed system achieves a\nhigh degree of similarity and consistency in front hair styling, showing\npromising results for automated, precise hairstyle adjustments.\n","authors":["Soonhyo Kim","Naoaki Kanazawa","Shun Hasegawa","Kento Kawaharazuka","Kei Okada"],"pdf_url":"https://arxiv.org/pdf/2501.10991v2.pdf","comment":"Accepted at IEEE/SICE SII2025"},{"id":"http://arxiv.org/abs/2501.16803v1","updated":"2025-01-28T09:08:31Z","published":"2025-01-28T09:08:31Z","title":"RG-Attn: Radian Glue Attention for Multi-modality Multi-agent\n  Cooperative Perception","summary":"  Cooperative perception offers an optimal solution to overcome the perception\nlimitations of single-agent systems by leveraging Vehicle-to-Everything (V2X)\ncommunication for data sharing and fusion across multiple agents. However, most\nexisting approaches focus on single-modality data exchange, limiting the\npotential of both homogeneous and heterogeneous fusion across agents. This\noverlooks the opportunity to utilize multi-modality data per agent, restricting\nthe system's performance. In the automotive industry, manufacturers adopt\ndiverse sensor configurations, resulting in heterogeneous combinations of\nsensor modalities across agents. To harness the potential of every possible\ndata source for optimal performance, we design a robust LiDAR and camera\ncross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to\nboth intra-agent cross-modality fusion and inter-agent cross-modality fusion\nscenarios, owing to the convenient coordinate conversion by transformation\nmatrix and the unified sampling/inversion mechanism. We also propose two\ndifferent architectures, named Paint-To-Puzzle (PTP) and\nCo-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception. PTP\naims for maximum precision performance and achieves smaller data packet size by\nlimiting cross-agent fusion to a single instance, but requiring all\nparticipants to be equipped with LiDAR. In contrast, CoS-CoCo supports agents\nwith any configuration-LiDAR-only, camera-only, or LiDAR-camera-both,\npresenting more generalization ability. Our approach achieves state-of-the-art\n(SOTA) performance on both real and simulated cooperative perception datasets.\nThe code will be released at GitHub in early 2025.\n","authors":["Lantao Li","Kang Yang","Wenqi Zhang","Xiaoxue Wang","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2501.16803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16800v1","updated":"2025-01-28T09:05:03Z","published":"2025-01-28T09:05:03Z","title":"DIRIGENt: End-To-End Robotic Imitation of Human Demonstrations Based on\n  a Diffusion Model","summary":"  There has been substantial progress in humanoid robots, with new skills\ncontinuously being taught, ranging from navigation to manipulation. While these\nabilities may seem impressive, the teaching methods often remain inefficient.\nTo enhance the process of teaching robots, we propose leveraging a mechanism\neffectively used by humans: teaching by demonstrating. In this paper, we\nintroduce DIRIGENt (DIrect Robotic Imitation GENeration model), a novel\nend-to-end diffusion approach that directly generates joint values from\nobserving human demonstrations, enabling a robot to imitate these actions\nwithout any existing mapping between it and humans. We create a dataset in\nwhich humans imitate a robot and then use this collected data to train a\ndiffusion model that enables a robot to imitate humans. The following three\naspects are the core of our contribution. First is our novel dataset with\nnatural pairs between human and robot poses, allowing our approach to imitate\nhumans accurately despite the gap between their anatomies. Second, the\ndiffusion input to our model alleviates the challenge of redundant joint\nconfigurations, limiting the search space. And finally, our end-to-end\narchitecture from perception to action leads to an improved learning\ncapability. Through our experimental analysis, we show that combining these\nthree aspects allows DIRIGENt to outperform existing state-of-the-art\napproaches in the field of generating joint values from RGB images.\n","authors":["Josua Spisak","Matthias Kerzel","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2501.16800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16754v1","updated":"2025-01-28T07:15:39Z","published":"2025-01-28T07:15:39Z","title":"SSF-PAN: Semantic Scene Flow-Based Perception for Autonomous Navigation\n  in Traffic Scenarios","summary":"  Vehicle detection and localization in complex traffic scenarios pose\nsignificant challenges due to the interference of moving objects. Traditional\nmethods often rely on outlier exclusions or semantic segmentations, which\nsuffer from low computational efficiency and accuracy. The proposed SSF-PAN can\nachieve the functionalities of LiDAR point cloud based object\ndetection/localization and SLAM (Simultaneous Localization and Mapping) with\nhigh computational efficiency and accuracy, enabling map-free navigation\nframeworks. The novelty of this work is threefold: 1) developing a neural\nnetwork which can achieve segmentation among static and dynamic objects within\nthe scene flows with different motion features, that is, semantic scene flow\n(SSF); 2) developing an iterative framework which can further optimize the\nquality of input scene flows and output segmentation results; 3) developing a\nscene flow-based navigation platform which can test the performance of the SSF\nperception system in the simulation environment. The proposed SSF-PAN method is\nvalidated using the SUScape-CARLA and the KITTI datasets, as well as on the\nCARLA simulator. Experimental results demonstrate that the proposed approach\noutperforms traditional methods in terms of scene flow computation accuracy,\nmoving object detection accuracy, computational efficiency, and autonomous\nnavigation effectiveness.\n","authors":["Yinqi Chen","Meiying Zhang","Qi Hao","Guang Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.16754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16743v1","updated":"2025-01-28T06:40:29Z","published":"2025-01-28T06:40:29Z","title":"Hierarchical Trajectory (Re)Planning for a Large Scale Swarm","summary":"  We consider the trajectory replanning problem for a large-scale swarm in a\ncluttered environment. Our path planner replans for robots by utilizing a\nhierarchical approach, dividing the workspace, and computing collision-free\npaths for robots within each cell in parallel. Distributed trajectory\noptimization generates a deadlock-free trajectory for efficient execution and\nmaintains the control feasibility even when the optimization fails. Our\nhierarchical approach combines the benefits of both centralized and\ndecentralized methods, achieving a high task success rate while providing\nreal-time replanning capability. Compared to decentralized approaches, our\napproach effectively avoids deadlocks and collisions, significantly increasing\nthe task success rate. We demonstrate the real-time performance of our\nalgorithm with up to 142 robots in simulation, and a representative 24 physical\nCrazyflie nano-quadrotor experiment.\n","authors":["Lishuo Pan","Yutong Wang","Nora Ayanian"],"pdf_url":"https://arxiv.org/pdf/2501.16743v1.pdf","comment":"13 pages, 14 figures. arXiv admin note: substantial text overlap with\n  arXiv:2407.02777"},{"id":"http://arxiv.org/abs/2501.16733v1","updated":"2025-01-28T06:18:29Z","published":"2025-01-28T06:18:29Z","title":"Dream to Drive with Predictive Individual World Model","summary":"  It is still a challenging topic to make reactive driving behaviors in complex\nurban environments as road users' intentions are unknown. Model-based\nreinforcement learning (MBRL) offers great potential to learn a reactive policy\nby constructing a world model that can provide informative states and\nimagination training. However, a critical limitation in relevant research lies\nin the scene-level reconstruction representation learning, which may overlook\nkey interactive vehicles and hardly model the interactive features among\nvehicles and their long-term intentions. Therefore, this paper presents a novel\nMBRL method with a predictive individual world model (PIWM) for autonomous\ndriving. PIWM describes the driving environment from an individual-level\nperspective and captures vehicles' interactive relations and their intentions\nvia trajectory prediction task. Meanwhile, a behavior policy is learned jointly\nwith PIWM. It is trained in PIWM's imagination and effectively navigates in the\nurban driving scenes leveraging intention-aware latent states. The proposed\nmethod is trained and evaluated on simulation environments built upon\nreal-world challenging interactive scenarios. Compared with popular model-free\nand state-of-the-art model-based reinforcement learning methods, experimental\nresults show that the proposed method achieves the best performance in terms of\nsafety and efficiency.\n","authors":["Yinfeng Gao","Qichao Zhang","Da-wei Ding","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.16733v1.pdf","comment":"Codes: https://github.com/gaoyinfeng/PIWM"},{"id":"http://arxiv.org/abs/2501.16728v1","updated":"2025-01-28T06:12:24Z","published":"2025-01-28T06:12:24Z","title":"Optimizing Efficiency of Mixed Traffic through Reinforcement Learning: A\n  Topology-Independent Approach and Benchmark","summary":"  This paper presents a mixed traffic control policy designed to optimize\ntraffic efficiency across diverse road topologies, addressing issues of\ncongestion prevalent in urban environments. A model-free reinforcement learning\n(RL) approach is developed to manage large-scale traffic flow, using data\ncollected by autonomous vehicles to influence human-driven vehicles. A\nreal-world mixed traffic control benchmark is also released, which includes 444\nscenarios from 20 countries, representing a wide geographic distribution and\ncovering a variety of scenarios and road topologies. This benchmark serves as a\nfoundation for future research, providing a realistic simulation environment\nfor the development of effective policies. Comprehensive experiments\ndemonstrate the effectiveness and adaptability of the proposed method,\nachieving better performance than existing traffic control methods in both\nintersection and roundabout scenarios. To the best of our knowledge, this is\nthe first project to introduce a real-world complex scenarios mixed traffic\ncontrol benchmark. Videos and code of our work are available at\nhttps://sites.google.com/berkeley.edu/mixedtrafficplus/home\n","authors":["Chuyang Xiao","Dawei Wang","Xinzheng Tang","Jia Pan","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2501.16728v1.pdf","comment":"accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2501.16719v1","updated":"2025-01-28T05:53:29Z","published":"2025-01-28T05:53:29Z","title":"Safety-Critical Control for Aerial Physical Interaction in Uncertain\n  Environment","summary":"  Aerial manipulation for safe physical interaction with their environments is\ngaining significant momentum in robotics research. In this paper, we present a\ndisturbance-observer-based safety-critical control for a fully actuated aerial\nmanipulator interacting with both static and dynamic structures. Our approach\ncenters on a safety filter that dynamically adjusts the desired trajectory of\nthe vehicle's pose, accounting for the aerial manipulator's dynamics, the\ndisturbance observer's structure, and motor thrust limits. We provide rigorous\nproof that the proposed safety filter ensures the forward invariance of the\nsafety set - representing motor thrust limits - even in the presence of\ndisturbance estimation errors. To demonstrate the superiority of our method\nover existing control strategies for aerial physical interaction, we perform\ncomparative experiments involving complex tasks, such as pushing against a\nstatic structure and pulling a plug firmly attached to an electric socket.\nFurthermore, to highlight its repeatability in scenarios with sudden dynamic\nchanges, we perform repeated tests of pushing a movable cart and extracting a\nplug from a socket. These experiments confirm that our method not only\noutperforms existing methods but also excels in handling tasks with rapid\ndynamic variations.\n","authors":["Jeonghyun Byun","Yeonjoon Kim","Dongjae Lee","H. Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2501.16719v1.pdf","comment":"to be presented in 2025 IEEE International Conference on Robotics and\n  Automation (ICRA), Atlanta, USA, 2025"},{"id":"http://arxiv.org/abs/2501.16717v1","updated":"2025-01-28T05:49:35Z","published":"2025-01-28T05:49:35Z","title":"Strawberry Robotic Operation Interface: An Open-Source Device for\n  Collecting Dexterous Manipulation Data in Robotic Strawberry Farming","summary":"  The strawberry farming is labor-intensive, particularly in tasks requiring\ndexterous manipulation such as picking occluded strawberries. To address this\nchallenge, we present the Strawberry Robotic Operation Interface (SROI), an\nopen-source device designed for collecting dexterous manipulation data in\nrobotic strawberry farming. The SROI features a handheld unit with a modular\nend effector, a stereo robotic camera, enabling the easy collection of\ndemonstration data in field environments. A data post-processing pipeline is\nintroduced to extract spatial trajectories and gripper states from the\ncollected data. Additionally, we release an open-source dataset of strawberry\npicking demonstrations to facilitate research in dexterous robotic\nmanipulation. The SROI represents a step toward automating complex strawberry\nfarming tasks, reducing reliance on manual labor.\n","authors":["Linsheng Hou","Wenwu Lu","Yanan Wang","Chen Peng","Zhenghao Fei"],"pdf_url":"https://arxiv.org/pdf/2501.16717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16698v1","updated":"2025-01-28T04:31:19Z","published":"2025-01-28T04:31:19Z","title":"3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose\n  Diffusion via Rectified Flow","summary":"  3D vision and spatial reasoning have long been recognized as preferable for\naccurately perceiving our three-dimensional world, especially when compared\nwith traditional visual reasoning based on 2D images. Due to the difficulties\nin collecting high-quality 3D data, research in this area has only recently\ngained momentum. With the advent of powerful large language models (LLMs),\nmulti-modal LLMs for 3D vision have been developed over the past few years.\nHowever, most of these models focus primarily on the vision encoder for 3D\ndata. In this paper, we propose converting existing densely activated LLMs into\nmixture-of-experts (MoE) models, which have proven effective for multi-modal\ndata processing. In addition to leveraging these models' instruction-following\ncapabilities, we further enable embodied task planning by attaching a diffusion\nhead, Pose-DiT, that employs a novel rectified flow diffusion scheduler.\nExperimental results on 3D question answering and task-planning tasks\ndemonstrate that our 3D-MoE framework achieves improved performance with fewer\nactivated parameters.\n","authors":["Yueen Ma","Yuzheng Zhuang","Jianye Hao","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2501.16698v1.pdf","comment":"Preprint. Work in progress"},{"id":"http://arxiv.org/abs/2501.16664v1","updated":"2025-01-28T02:53:48Z","published":"2025-01-28T02:53:48Z","title":"Improving Vision-Language-Action Model with Online Reinforcement\n  Learning","summary":"  Recent studies have successfully integrated large vision-language models\n(VLMs) into low-level robotic control by supervised fine-tuning (SFT) with\nexpert robotic datasets, resulting in what we term vision-language-action (VLA)\nmodels. Although the VLA models are powerful, how to improve these large models\nduring interaction with environments remains an open question. In this paper,\nwe explore how to further improve these VLA models via Reinforcement Learning\n(RL), a commonly used fine-tuning technique for large models. However, we find\nthat directly applying online RL to large VLA models presents significant\nchallenges, including training instability that severely impacts the\nperformance of large models, and computing burdens that exceed the capabilities\nof most local machines. To address these challenges, we propose iRe-VLA\nframework, which iterates between Reinforcement Learning and Supervised\nLearning to effectively improve VLA models, leveraging the exploratory benefits\nof RL while maintaining the stability of supervised learning. Experiments in\ntwo simulated benchmarks and a real-world manipulation suite validate the\neffectiveness of our method.\n","authors":["Yanjiang Guo","Jianke Zhang","Xiaoyu Chen","Xiang Ji","Yen-Jen Wang","Yucheng Hu","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2501.16664v1.pdf","comment":"Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2403.02508v3","updated":"2025-01-28T02:12:38Z","published":"2024-03-04T21:54:51Z","title":"Collision Avoidance and Geofencing for Fixed-wing Aircraft with Control\n  Barrier Functions","summary":"  Safety-critical failures often have fatal consequences in aerospace control.\nControl systems on aircraft, therefore, must ensure the strict satisfaction of\nsafety constraints, preferably with formal guarantees of safe behavior. This\npaper establishes the safety-critical control of fixed-wing aircraft in\ncollision avoidance and geofencing tasks. A control framework is developed\nwherein a run-time assurance (RTA) system modulates the nominal flight\ncontroller of the aircraft whenever necessary to prevent it from colliding with\nother aircraft or crossing a boundary (geofence) in space. The RTA is\nformulated as a safety filter using control barrier functions (CBFs) with\nformal guarantees of safe behavior. CBFs are constructed and compared for a\nnonlinear kinematic fixed-wing aircraft model. The proposed CBF-based\ncontrollers showcase the capability of safely executing simultaneous collision\navoidance and geofencing, as demonstrated by simulations on the kinematic model\nand a high-fidelity dynamical model.\n","authors":["Tamas G. Molnar","Suresh K. Kannan","James Cunningham","Kyle Dunlap","Kerianne L. Hobbs","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2403.02508v3.pdf","comment":"Accepted to the IEEE Transactions on Control System Technology. 15\n  pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17524v3","updated":"2025-01-28T01:46:20Z","published":"2024-10-23T03:01:43Z","title":"Mechanisms and Computational Design of Multi-Modal End-Effector with\n  Force Sensing using Gated Networks","summary":"  In limbed robotics, end-effectors must serve dual functions, such as both\nfeet for locomotion and grippers for grasping, which presents design\nchallenges. This paper introduces a multi-modal end-effector capable of\ntransitioning between flat and line foot configurations while providing\ngrasping capabilities. MAGPIE integrates 8-axis force sensing using proposed\nmechanisms with hall effect sensors, enabling both contact and tactile force\nmeasurements. We present a computational design framework for our sensing\nmechanism that accounts for noise and interference, allowing for desired\nsensitivity and force ranges and generating ideal inverse models. The hardware\nimplementation of MAGPIE is validated through experiments, demonstrating its\ncapability as a foot and verifying the performance of the sensing mechanisms,\nideal models, and gated network-based models.\n","authors":["Yusuke Tanaka","Alvin Zhu","Richard Lin","Ankur Mehta","Dennis Hong"],"pdf_url":"https://arxiv.org/pdf/2410.17524v3.pdf","comment":"Proceeding to 2025 IEEE International Conference on Robotics and\n  Automation (ICRA25)"},{"id":"http://arxiv.org/abs/2501.16590v1","updated":"2025-01-28T00:07:28Z","published":"2025-01-28T00:07:28Z","title":"Benchmarking Model Predictive Control and Reinforcement Learning Based\n  Control for Legged Robot Locomotion in MuJoCo Simulation","summary":"  Model Predictive Control (MPC) and Reinforcement Learning (RL) are two\nprominent strategies for controlling legged robots, each with unique strengths.\nRL learns control policies through system interaction, adapting to various\nscenarios, whereas MPC relies on a predefined mathematical model to solve\noptimization problems in real-time. Despite their widespread use, there is a\nlack of direct comparative analysis under standardized conditions. This work\naddresses this gap by benchmarking MPC and RL controllers on a Unitree Go1\nquadruped robot within the MuJoCo simulation environment, focusing on a\nstandardized task-straight walking at a constant velocity. Performance is\nevaluated based on disturbance rejection, energy efficiency, and terrain\nadaptability. The results show that RL excels in handling disturbances and\nmaintaining energy efficiency but struggles with generalization to new terrains\ndue to its dependence on learned policies tailored to specific environments. In\ncontrast, MPC shows enhanced recovery capabilities from larger perturbations by\nleveraging its optimization-based approach, allowing for a balanced\ndistribution of control efforts across the robot's joints. The results provide\na clear understanding of the advantages and limitations of both RL and MPC,\noffering insights into selecting an appropriate control strategy for legged\nrobotic applications.\n","authors":["Shivayogi Akki","Tan Chen"],"pdf_url":"https://arxiv.org/pdf/2501.16590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17349v1","updated":"2025-01-28T23:51:44Z","published":"2025-01-28T23:51:44Z","title":"An Efficient Numerical Function Optimization Framework for Constrained\n  Nonlinear Robotic Problems","summary":"  This paper presents a numerical function optimization framework designed for\nconstrained optimization problems in robotics. The tool is designed with\nreal-time considerations and is suitable for online trajectory and control\ninput optimization problems. The proposed framework does not require any\nanalytical representation of the problem and works with constrained block-box\noptimization functions. The method combines first-order gradient-based line\nsearch algorithms with constraint prioritization through nullspace projections\nonto constraint Jacobian space. The tool is implemented in C++ and provided\nonline for community use, along with some numerical and robotic example\nimplementations presented in this paper.\n","authors":["Sait Sovukluk","Christian Ott"],"pdf_url":"https://arxiv.org/pdf/2501.17349v1.pdf","comment":"This work has been submitted to IFAC for possible publication"},{"id":"http://arxiv.org/abs/2411.15371v2","updated":"2025-01-28T22:26:37Z","published":"2024-11-22T22:59:34Z","title":"Safe and Trustworthy Robot Pathfinding with BIM, MHA*, and NLP","summary":"  Construction robots have gained significant traction in recent years in\nresearch and development. However, the application of industrial robots has\nunique challenges. Dynamic environments, domain-specific tasks, and complex\nlocalization and mapping are significant obstacles in their development. In\nconstruction job sites, moving objects and complex machinery can make\npathfinding a difficult task due to the possibility of object collisions.\nExisting methods such as simultaneous localization and mapping are viable\nsolutions to this problem, however, due to the precision and data quality\nrequired by the sensors and the processing of the information, they can be very\ncomputationally expensive. We propose using spatial and semantic information in\nbuilding information modeling (BIM) to develop domain-specific pathfinding\nstrategies. In this work, we integrate a multi-heuristic A* (MHA*) algorithm\nusing APFs from the BIM spatial information and process textual information\nfrom the BIM using large language models (LLMs) to adjust the algorithm for\ndynamic object avoidance. We show increased robot object proximity by 80% while\nmaintaining similar path lengths.\n","authors":["Mani Amani","Reza Akhavian"],"pdf_url":"https://arxiv.org/pdf/2411.15371v2.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2501.17313v1","updated":"2025-01-28T21:50:43Z","published":"2025-01-28T21:50:43Z","title":"Surena-V: A Humanoid Robot for Human-Robot Collaboration with\n  Optimization-based Control Architecture","summary":"  This paper presents Surena-V, a humanoid robot designed to enhance\nhuman-robot collaboration capabilities. The robot features a range of sensors,\nincluding barometric tactile sensors in its hands, to facilitate precise\nenvironmental interaction. This is demonstrated through an experiment\nshowcasing the robot's ability to control a medical needle's movement through\nsoft material. Surena-V's operational framework emphasizes stability and\ncollaboration, employing various optimization-based control strategies such as\nZero Moment Point (ZMP) modification through upper body movement and stepping.\nNotably, the robot's interaction with the environment is improved by detecting\nand interpreting external forces at their point of effect, allowing for more\nagile responses compared to methods that control overall balance based on\nexternal forces. The efficacy of this architecture is substantiated through an\nexperiment illustrating the robot's collaboration with a human in moving a bar.\nThis work contributes to the field of humanoid robotics by presenting a\ncomprehensive system design and control architecture focused on human-robot\ncollaboration and environmental adaptability.\n","authors":["Mohammad Ali Bazrafshani","Aghil Yousefi-Koma","Amin Amani","Behnam Maleki","Shahab Batmani","Arezoo Dehestani Ardakani","Sajedeh Taheri","Parsa Yazdankhah","Mahdi Nozari","Amin Mozayyan","Alireza Naeini","Milad Shafiee","Amirhosein Vedadi"],"pdf_url":"https://arxiv.org/pdf/2501.17313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17311v1","updated":"2025-01-28T21:48:18Z","published":"2025-01-28T21:48:18Z","title":"RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on\n  Scaled Platforms","summary":"  Autonomous racing presents a complex environment requiring robust controllers\ncapable of making rapid decisions under dynamic conditions. While traditional\ncontrollers based on tire models are reliable, they often demand extensive\ntuning or system identification. RL methods offer significant potential due to\ntheir ability to learn directly from interaction, yet they typically suffer\nfrom the Sim-to-Reall gap, where policies trained in simulation fail to perform\neffectively in the real world. In this paper, we propose RLPP, a residual RL\nframework that enhances a PP controller with an RL-based residual. This hybrid\napproach leverages the reliability and interpretability of PP while using RL to\nfine-tune the controller's performance in real-world scenarios. Extensive\ntesting on the F1TENTH platform demonstrates that RLPP improves lap times by up\nto 6.37 %, closing the gap to the SotA methods by more than 52 % and providing\nreliable performance in zero-shot real-world deployment, overcoming key\nchallenges associated with the Sim-to-Real transfer and reducing the\nperformance gap from simulation to reality by more than 8-fold when compared to\nthe baseline RL controller. The RLPP framework is made available as an\nopen-source tool, encouraging further exploration and advancement in autonomous\nracing research. The code is available at: www.github.com/forzaeth/rlpp.\n","authors":["Edoardo Ghignone","Nicolas Baumann","Cheng Hu","Jonathan Wang","Lei Xie","Andrea Carron","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2501.17311v1.pdf","comment":"This paper has been accepted for publication at the IEEE\n  International Conference on Robotics and Automation (ICRA), Atlanta 2025. The\n  code is available at: www.github.com/forzaeth/rlpp"},{"id":"http://arxiv.org/abs/2409.06111v4","updated":"2025-01-28T21:14:37Z","published":"2024-09-09T23:34:24Z","title":"Competency-Aware Planning for Probabilistically Safe Navigation Under\n  Perception Uncertainty","summary":"  Perception-based navigation systems are useful for unmanned ground vehicle\n(UGV) navigation in complex terrains, where traditional depth-based navigation\nschemes are insufficient. However, these data-driven methods are highly\ndependent on their training data and can fail in surprising and dramatic ways\nwith little warning. To ensure the safety of the vehicle and the surrounding\nenvironment, it is imperative that the navigation system is able to recognize\nthe predictive uncertainty of the perception model and respond safely and\neffectively in the face of uncertainty. In an effort to enable safe navigation\nunder perception uncertainty, we develop a probabilistic and\nreconstruction-based competency estimation (PaRCE) method to estimate the\nmodel's level of familiarity with an input image as a whole and with specific\nregions in the image. We find that the overall competency score can correctly\npredict correctly classified, misclassified, and out-of-distribution (OOD)\nsamples. We also confirm that the regional competency maps can accurately\ndistinguish between familiar and unfamiliar regions across images. We then use\nthis competency information to develop a planning and control scheme that\nenables effective navigation while maintaining a low probability of error. We\nfind that the competency-aware scheme greatly reduces the number of collisions\nwith unfamiliar obstacles, compared to a baseline controller with no competency\nawareness. Furthermore, the regional competency information is very valuable in\nenabling efficient navigation.\n","authors":["Sara Pohland","Claire Tomlin"],"pdf_url":"https://arxiv.org/pdf/2409.06111v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17206v1","updated":"2025-01-28T06:38:24Z","published":"2025-01-28T06:38:24Z","title":"Integrating Reinforcement Learning and AI Agents for Adaptive Robotic\n  Interaction and Assistance in Dementia Care","summary":"  This study explores a novel approach to advancing dementia care by\nintegrating socially assistive robotics, reinforcement learning (RL), large\nlanguage models (LLMs), and clinical domain expertise within a simulated\nenvironment. This integration addresses the critical challenge of limited\nexperimental data in socially assistive robotics for dementia care, providing a\ndynamic simulation environment that realistically models interactions between\npersons living with dementia (PLWDs) and robotic caregivers. The proposed\nframework introduces a probabilistic model to represent the cognitive and\nemotional states of PLWDs, combined with an LLM-based behavior simulation to\nemulate their responses. We further develop and train an adaptive RL system\nenabling humanoid robots, such as Pepper, to deliver context-aware and\npersonalized interactions and assistance based on PLWDs' cognitive and\nemotional states. The framework also generalizes to computer-based agents,\nhighlighting its versatility. Results demonstrate that the RL system, enhanced\nby LLMs, effectively interprets and responds to the complex needs of PLWDs,\nproviding tailored caregiving strategies. This research contributes to\nhuman-computer and human-robot interaction by offering a customizable AI-driven\ncaregiving platform, advancing understanding of dementia-related challenges,\nand fostering collaborative innovation in assistive technologies. The proposed\napproach has the potential to enhance the independence and quality of life for\nPLWDs while alleviating caregiver burden, underscoring the transformative role\nof interaction-focused AI systems in dementia care.\n","authors":["Fengpei Yuan","Nehal Hasnaeen","Ran Zhang","Bryce Bible","Joseph Riley Taylor","Hairong Qi","Fenghui Yao","Xiaopeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.17206v1.pdf","comment":"18 pages, 12 figures"}]},"2025-01-29T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2411.12155v2","updated":"2025-01-29T18:56:20Z","published":"2024-11-19T01:23:52Z","title":"Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot\n  Learning","summary":"  In reinforcement learning (RL), we train a value function to understand the\nlong-term consequence of executing a single action. However, the value of\ntaking each action can be ambiguous in robotics as robot movements are\ntypically the aggregate result of executing multiple small actions. Moreover,\nrobotic training data often consists of noisy trajectories, in which each\naction is noisy but executing a series of actions results in a meaningful robot\nmovement. This further makes it difficult for the value function to understand\nthe effect of individual actions. To address this, we introduce Coarse-to-fine\nQ-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that\nlearns a critic network that outputs Q-values over a sequence of actions, i.e.,\nexplicitly training the value function to learn the consequence of executing\naction sequences. We study our algorithm on 53 robotic tasks with sparse and\ndense rewards, as well as with and without demonstrations, from BiGym,\nHumanoidBench, and RLBench. We find that CQN-AS outperforms various baselines,\nin particular on humanoid control tasks.\n","authors":["Younggyo Seo","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2411.12155v2.pdf","comment":"15 Pages. Website: https://younggyo.me/cqn-as/"},{"id":"http://arxiv.org/abs/2501.17855v1","updated":"2025-01-29T18:55:07Z","published":"2025-01-29T18:55:07Z","title":"GRACE: Generalizing Robot-Assisted Caregiving with User Functionality\n  Embeddings","summary":"  Robot caregiving should be personalized to meet the diverse needs of care\nrecipients -- assisting with tasks as needed, while taking user agency in\naction into account. In physical tasks such as handover, bathing, dressing, and\nrehabilitation, a key aspect of this diversity is the functional range of\nmotion (fROM), which can vary significantly between individuals. In this work,\nwe learn to predict personalized fROM as a way to generalize robot\ndecision-making in a wide range of caregiving tasks. We propose a novel\ndata-driven method for predicting personalized fROM using functional assessment\nscores from occupational therapy. We develop a neural model that learns to\nembed functional assessment scores into a latent representation of the user's\nphysical function. The model is trained using motion capture data collected\nfrom users with emulated mobility limitations. After training, the model\npredicts personalized fROM for new users without motion capture. Through\nsimulated experiments and a real-robot user study, we show that the\npersonalized fROM predictions from our model enable the robot to provide\npersonalized and effective assistance while improving the user's agency in\naction. See our website for more visualizations:\nhttps://emprise.cs.cornell.edu/grace/.\n","authors":["Ziang Liu","Yuanchen Ju","Yu Da","Tom Silver","Pranav N. Thakkar","Jenna Li","Justin Guo","Katherine Dimitropoulou","Tapomayukh Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2501.17855v1.pdf","comment":"10 pages, 5 figures, Accepted to IEEE/ACM International Conference on\n  Human-Robot Interaction (HRI), 2025"},{"id":"http://arxiv.org/abs/2501.17851v1","updated":"2025-01-29T18:50:41Z","published":"2025-01-29T18:50:41Z","title":"UGSim: Autonomous Buoyancy-Driven Underwater Glider Simulator with LQR\n  Control Strategy and Recursive Guidance System","summary":"  This paper presents the UGSim, a simulator for buoyancy-driven gliders, with\na LQR control strategy, and a recursive guidance system. Building on the top of\nthe DAVE and the UUVsim, it is designed to address unique challenges that come\nfrom the complex hydrodynamic and hydrostatic impacts on buoyancy-driven\ngliders, which conventional robotics simulators can't deal with. Since\ndistinguishing features of the class of vehicles, general controllers and\nguidance systems developed for underwater robotics are infeasible. The\nsimulator is provided to accelerate the development and the evaluation of\nalgorithms that would otherwise require expensive and time-consuming operations\nat sea. It consists of a basic kinetic module, a LQR control module and a\nrecursive guidance module, which allows the user to concentrate on the single\nproblem rather than the whole robotics system and the software infrastructure.\nWe demonstrate the usage of the simulator through an example, loading the\nconfiguration of the buoyancy-driven glider named Petrel-II, presenting its\ndynamics simulation, performances of the control strategy and the guidance\nsystem.\n","authors":["Zhizun Xu","Yang Song","Jiabao Zhu","Weichao Shi"],"pdf_url":"https://arxiv.org/pdf/2501.17851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17842v1","updated":"2025-01-29T18:46:35Z","published":"2025-01-29T18:46:35Z","title":"From Sparse to Dense: Toddler-inspired Reward Transition in\n  Goal-Oriented Reinforcement Learning","summary":"  Reinforcement learning (RL) agents often face challenges in balancing\nexploration and exploitation, particularly in environments where sparse or\ndense rewards bias learning. Biological systems, such as human toddlers,\nnaturally navigate this balance by transitioning from free exploration with\nsparse rewards to goal-directed behavior guided by increasingly dense rewards.\nInspired by this natural progression, we investigate the Toddler-Inspired\nReward Transition in goal-oriented RL tasks. Our study focuses on transitioning\nfrom sparse to potential-based dense (S2D) rewards while preserving optimal\nstrategies. Through experiments on dynamic robotic arm manipulation and\negocentric 3D navigation tasks, we demonstrate that effective S2D reward\ntransitions significantly enhance learning performance and sample efficiency.\nAdditionally, using a Cross-Density Visualizer, we show that S2D transitions\nsmooth the policy loss landscape, resulting in wider minima that improve\ngeneralization in RL models. In addition, we reinterpret Tolman's maze\nexperiments, underscoring the critical role of early free exploratory learning\nin the context of S2D rewards.\n","authors":["Junseok Park","Hyeonseo Yang","Min Whoo Lee","Won-Seok Choi","Minsu Lee","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.17842v1.pdf","comment":"Extended version of AAAI 2024 paper: Unveiling the Significance of\n  Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning.\n  This manuscript is currently being prepared for journal submission"},{"id":"http://arxiv.org/abs/2501.17789v1","updated":"2025-01-29T17:30:34Z","published":"2025-01-29T17:30:34Z","title":"Propeller Motion of a Devil-Stick using Normal Forcing","summary":"  The problem of realizing rotary propeller motion of a devil-stick in the\nvertical plane using forces purely normal to the stick is considered. This\nproblem represents a nonprehensile manipulation task of an underactuated\nsystem. In contrast with previous approaches, the devil-stick is manipulated by\ncontrolling the normal force and its point of application. Virtual holonomic\nconstraints are used to design the trajectory of the center-of-mass of the\ndevil-stick in terms of its orientation angle, and conditions for stable\npropeller motion are derived. Intermittent large-amplitude forces are used to\nasymptotically stabilize a desired propeller motion. Simulations demonstrate\nthe efficacy of the approach in realizing stable propeller motion without loss\nof contact between the actuator and devil-stick.\n","authors":["Aakash Khandelwal","Ranjan Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2501.17789v1.pdf","comment":"6 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2501.17773v1","updated":"2025-01-29T17:09:28Z","published":"2025-01-29T17:09:28Z","title":"SafePR: Unified Approach for Safe Parallel Robots by Contact Detection\n  and Reaction with Redundancy Resolution","summary":"  Fast and safe motion is crucial for the successful deployment of physically\ninteractive robots. Parallel robots (PRs) offer the potential for higher speeds\nwhile maintaining the same energy limits due to their low moving masses.\nHowever, they require methods for contact detection and reaction while avoiding\nsingularities and self-collisions. We address this issue and present SafePR - a\nunified approach for the detection and localization, including the distinction\nbetween collision and clamping to perform a reaction that is safe for humans\nand feasible for PRs. Our approach uses information from the encoders and motor\ncurrents to estimate forces via a generalized-momentum observer. Neural\nnetworks and particle filters classify and localize the contacts. We introduce\nreactions with redundancy resolution to avoid type-II singularities and\nself-collisions. Our approach detected and terminated 72 real-world collision\nand clamping contacts with end-effector speeds of up to 1.5 m/s, each within\n25-275 ms. The forces were below the thresholds from ISO/TS 15066. By using\nbuilt-in sensors, SafePR enables safe interaction with already assembled PRs\nwithout the need for new hardware components.\n","authors":["Aran Mohammad","Tim-Lukas Habich","Thomas Seel","Moritz Schappler"],"pdf_url":"https://arxiv.org/pdf/2501.17773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17754v1","updated":"2025-01-29T16:47:59Z","published":"2025-01-29T16:47:59Z","title":"Analysis of the navigation of magnetic microrobots through cerebral\n  bifurcations","summary":"  Local administration of thrombolytics in ischemic stroke could accelerate\nclot lysis and the ensuing reperfusion while minimizing the side effects of\nsystemic administration. Medical microrobots could be injected into the\nbloodstream and magnetically navigated to the clot for administering the drugs\ndirectly to the target. The magnetic manipulation required to navigate medical\nmicrorobots will depend on various parameters such as the microrobots size, the\nblood velocity, and the imposed magnetic field gradients. Numerical simulation\nwas used to study the motion of magnetically controlled microrobots flowing\nthrough representative cerebral bifurcations, for predicting the magnetic\ngradients required to navigate the microrobots from the injection point until\nthe target location. Upon thorough validation of the model against several\nindependent analytical and experimental results, the model was used to generate\nmaps and a predictive equation providing quantitative information on the\nrequired magnetic gradients, for different scenarios. The developed maps and\npredictive equation are crucial to inform the design, operation and\noptimization of magnetic navigation systems for healthcare applications.\n","authors":["Pedro G. Alves","Maria Pinto","Rosa Moreira","Derick Sivakumaran","Fabian C. Landers","Maria Guix","Bradley J. Nelson","Andreas D. Flouris","Salvador PanÃ©","Josep PuigmartÃ­-Luis","Tiago Sotto Mayor"],"pdf_url":"https://arxiv.org/pdf/2501.17754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17704v1","updated":"2025-01-29T15:20:43Z","published":"2025-01-29T15:20:43Z","title":"Inferring Implicit Goals Across Differing Task Models","summary":"  One of the significant challenges to generating value-aligned behavior is to\nnot only account for the specified user objectives but also any implicit or\nunspecified user requirements. The existence of such implicit requirements\ncould be particularly common in settings where the user's understanding of the\ntask model may differ from the agent's estimate of the model. Under this\nscenario, the user may incorrectly expect some agent behavior to be inevitable\nor guaranteed. This paper addresses such expectation mismatch in the presence\nof differing models by capturing the possibility of unspecified user subgoal in\nthe context of a task captured as a Markov Decision Process (MDP) and querying\nfor it as required. Our method identifies bottleneck states and uses them as\ncandidates for potential implicit subgoals. We then introduce a querying\nstrategy that will generate the minimal number of queries required to identify\na policy guaranteed to achieve the underlying goal. Our empirical evaluations\ndemonstrate the effectiveness of our approach in inferring and achieving\nunstated goals across various tasks.\n","authors":["Silvia Tulli","Stylianos Loukas Vasileiou","Mohamed Chetouani","Sarath Sreedharan"],"pdf_url":"https://arxiv.org/pdf/2501.17704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11742v5","updated":"2025-01-29T15:05:45Z","published":"2024-03-18T12:54:33Z","title":"Accelerating Model Predictive Control for Legged Robots through\n  Distributed Optimization","summary":"  This paper presents a novel approach to enhance Model Predictive Control\n(MPC) for legged robots through Distributed Optimization. Our method focuses on\ndecomposing the robot dynamics into smaller, parallelizable subsystems, and\nutilizing the Alternating Direction Method of Multipliers (ADMM) to ensure\nconsensus among them. Each subsystem is managed by its own Optimal Control\nProblem, with ADMM facilitating consistency between their optimizations. This\napproach not only decreases the computational time but also allows for\neffective scaling with more complex robot configurations, facilitating the\nintegration of additional subsystems such as articulated arms on a quadruped\nrobot. We demonstrate, through numerical evaluations, the convergence of our\napproach on two systems with increasing complexity. In addition, we showcase\nthat our approach converges towards the same solution when compared to a\nstate-of-the-art centralized whole-body MPC implementation. Moreover, we\nquantitatively compare the computational efficiency of our method to the\ncentralized approach, revealing up to a 75% reduction in computational time.\nOverall, our approach offers a promising avenue for accelerating MPC solutions\nfor legged robots, paving the way for more effective utilization of the\ncomputational performance of modern hardware.\n","authors":["Lorenzo Amatucci","Giulio Turrisi","Angelo Bratta","Victor Barasuol","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2403.11742v5.pdf","comment":"Accepted for publication at the 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2403.11383v3","updated":"2025-01-29T15:00:23Z","published":"2024-03-18T00:19:52Z","title":"On the Benefits of GPU Sample-Based Stochastic Predictive Controllers\n  for Legged Locomotion","summary":"  Quadrupedal robots excel in mobility, navigating complex terrains with\nagility. However, their complex control systems present challenges that are\nstill far from being fully addressed. In this paper, we introduce the use of\nSample-Based Stochastic control strategies for quadrupedal robots, as an\nalternative to traditional optimal control laws. We show that Sample-Based\nStochastic methods, supported by GPU acceleration, can be effectively applied\nto real quadruped robots. In particular, in this work, we focus on achieving\ngait frequency adaptation, a notable challenge in quadrupedal locomotion for\ngradient-based methods. To validate the effectiveness of Sample-Based\nStochastic controllers we test two distinct approaches for quadrupedal robots\nand compare them against a conventional gradient-based Model Predictive Control\nsystem. Our findings, validated both in simulation and on a real 21Kg Aliengo\nquadruped, demonstrate that our method is on par with a traditional Model\nPredictive Control strategy when the robot is subject to zero or moderate\ndisturbance, while it surpasses gradient-based methods in handling sustained\nexternal disturbances, thanks to the straightforward gait adaptation strategy\nthat is possible to achieve within their formulation.\n","authors":["Giulio Turrisi","Valerio Modugno","Lorenzo Amatucci","Dimitrios Kanoulas","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2403.11383v3.pdf","comment":"Accepted for publication at the 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2307.12292v2","updated":"2025-01-29T14:46:57Z","published":"2023-07-23T11:07:45Z","title":"Quadrupedal Footstep Planning using Learned Motion Models of a Black-Box\n  Controller","summary":"  Legged robots are increasingly entering new domains and applications,\nincluding search and rescue, inspection, and logistics. However, for such\nsystems to be valuable in real-world scenarios, they must be able to\nautonomously and robustly navigate irregular terrains. In many cases, robots\nthat are sold on the market do not provide such abilities, being able to\nperform only blind locomotion. Furthermore, their controller cannot be easily\nmodified by the end-user, requiring a new and time-consuming control synthesis.\nIn this work, we present a fast local motion planning pipeline that extends the\ncapabilities of a black-box walking controller that is only able to track\nhigh-level reference velocities. More precisely, we learn a set of motion\nmodels for such a controller that maps high-level velocity commands to Center\nof Mass (CoM) and footstep motions. We then integrate these models with a\nvariant of the A star algorithm to plan the CoM trajectory, footstep sequences,\nand corresponding high-level velocity commands based on visual information,\nallowing the quadruped to safely traverse irregular terrains at demand.\n","authors":["Ilyass Taouil","Giulio Turrisi","Daniel Schleich","Victor Barasuol","Claudio Semini","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2307.12292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12664v4","updated":"2025-01-29T14:42:14Z","published":"2023-07-24T10:10:24Z","title":"SafeSteps: Learning Safer Footstep Planning Policies for Legged Robots\n  via Model-Based Priors","summary":"  We present a footstep planning policy for quadrupedal locomotion that is able\nto directly take into consideration a-priori safety information in its\ndecisions. At its core, a learning process analyzes terrain patches,\nclassifying each landing location by its kinematic feasibility, shin collision,\nand terrain roughness. This information is then encoded into a small vector\nrepresentation and passed as an additional state to the footstep planning\npolicy, which furthermore proposes only safe footstep location by applying a\nmasked variant of the Proximal Policy Optimization algorithm. The performance\nof the proposed approach is shown by comparative simulations and experiments on\nan electric quadruped robot walking in different rough terrain scenarios. We\nshow that violations of the above safety conditions are greatly reduced both\nduring training and the successive deployment of the policy, resulting in an\ninherently safer footstep planner. Furthermore, we show how, as a byproduct,\nfewer reward terms are needed to shape the behavior of the policy, which in\nreturn is able to achieve both better final performances and sample efficiency.\n","authors":["Shafeef Omar","Lorenzo Amatucci","Victor Barasuol","Giulio Turrisi","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2307.12664v4.pdf","comment":"Accepted for publication at the 2023 IEEE-RAS International\n  Conference on Humanoid Robots (Humanoids)"},{"id":"http://arxiv.org/abs/2311.03485v2","updated":"2025-01-29T14:28:23Z","published":"2023-11-06T19:48:03Z","title":"CLIP-Motion: Learning Reward Functions for Robotic Actions Using\n  Consecutive Observations","summary":"  This paper presents a novel method for learning reward functions for robotic\nmotions by harnessing the power of a CLIP-based model. Traditional reward\nfunction design often hinges on manual feature engineering, which can struggle\nto generalize across an array of tasks. Our approach circumvents this challenge\nby capitalizing on CLIP's capability to process both state features and image\ninputs effectively. Given a pair of consecutive observations, our model excels\nin identifying the motion executed between them. We showcase results spanning\nvarious robotic activities, such as directing a gripper to a designated target\nand adjusting the position of a cube. Through experimental evaluations, we\nunderline the proficiency of our method in precisely deducing motion and its\npromise to enhance reinforcement learning training in the realm of robotics.\n","authors":["Xuzhe Dang","Stefan Edelkamp"],"pdf_url":"https://arxiv.org/pdf/2311.03485v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17666v1","updated":"2025-01-29T14:05:38Z","published":"2025-01-29T14:05:38Z","title":"An Intelligent System-on-a-Chip for a Real-Time Assessment of Fuel\n  Consumption to Promote Eco-Driving","summary":"  Pollution that originates from automobiles is a concern in the current world,\nnot only because of global warming, but also due to the harmful effects on\npeople's health and lives. Despite regulations on exhaust gas emissions being\napplied, minimizing unsuitable driving habits that cause elevated fuel\nconsumption and emissions would achieve further reductions. For that reason,\nthis work proposes a self-organized map (SOM)-based intelligent system in order\nto provide drivers with eco-driving-intended driving style (DS)\nrecommendations. The development of the DS advisor uses driving data from the\nUyanik instrumented car. The system classifies drivers regarding the underlying\ncauses of non-optimal DSs from the eco-driving viewpoint. When compared with\nother solutions, the main advantage of this approach is the personalization of\nthe recommendations that are provided to motorists, comprising the handling of\nthe pedals and the gearbox, with potential improvements in both fuel\nconsumption and emissions ranging from the 9.5\\% to the 31.5\\%, or even higher\nfor drivers that are strongly engaged with the system. It was successfully\nimplemented using a field-programmable gate array (FPGA) device of the Xilinx\nZynQ programmable system-on-a-chip (PSoC) family. This SOM-based system allows\nfor real-time implementation, state-of-the-art timing performances, and low\npower consumption, which are suitable for developing advanced driving\nassistance systems (ADASs).\n","authors":["Ãscar Mata-Carballeira","Mikel DÃ­az-RodrÃ­guez","InÃ©s del Campo","Victoria MartÃ­nez"],"pdf_url":"https://arxiv.org/pdf/2501.17666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17665v1","updated":"2025-01-29T14:04:54Z","published":"2025-01-29T14:04:54Z","title":"Planning with Vision-Language Models and a Use Case in Robot-Assisted\n  Teaching","summary":"  Automating the generation of Planning Domain Definition Language (PDDL) with\nLarge Language Model (LLM) opens new research topic in AI planning,\nparticularly for complex real-world tasks. This paper introduces Image2PDDL, a\nnovel framework that leverages Vision-Language Models (VLMs) to automatically\nconvert images of initial states and descriptions of goal states into PDDL\nproblems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL\naddresses key challenges in bridging perceptual understanding with symbolic\nplanning, reducing the expertise required to create structured problem\ninstances, and improving scalability across tasks of varying complexity. We\nevaluate the framework on various domains, including standard planning domains\nlike blocksworld and sliding tile puzzles, using datasets with multiple\ndifficulty levels. Performance is assessed on syntax correctness, ensuring\ngrammar and executability, and content correctness, verifying accurate state\nrepresentation in generated PDDL problems. The proposed approach demonstrates\npromising results across diverse task complexities, suggesting its potential\nfor broader applications in AI planning. We will discuss a potential use case\nin robot-assisted teaching of students with Autism Spectrum Disorder.\n","authors":["Xuzhe Dang","Lada KudlÃ¡ÄkovÃ¡","Stefan Edelkamp"],"pdf_url":"https://arxiv.org/pdf/2501.17665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17664v1","updated":"2025-01-29T14:04:16Z","published":"2025-01-29T14:04:16Z","title":"Analysis of the Motion Sickness and the Lack of Comfort in Car\n  Passengers","summary":"  Advanced driving assistance systems (ADAS) are primarily designed to increase\ndriving safety and reduce traffic congestion without paying too much attention\nto passenger comfort or motion sickness. However, in view of autonomous cars,\nand taking into account that the lack of comfort and motion sickness increase\nin passengers, analysis from a comfort perspective is essential in the future\ncar investigation. The aim of this work is to study in detail how passenger's\ncomfort evaluation parameters vary depending on the driving style, car or road.\nThe database used has been developed by compiling the accelerations suffered by\npassengers when three drivers cruise two different vehicles on different types\nof routes. In order to evaluate both comfort and motion sickness, first, the\nnumerical values of the main comfort evaluation variables reported in the\nliterature have been analyzed. Moreover, a complementary statistical analysis\nof probability density and a power spectral analysis are performed. Finally,\nquantitative results are compared with passenger qualitative feedback. The\nresults show the high dependence of comfort evaluation variables' value with\nthe road type. In addition, it has been demonstrated that the driving style and\nvehicle dynamics amplify or attenuate those values. Additionally, it has been\ndemonstrated that contributions from longitudinal and lateral accelerations\nhave a much greater effect in the lack of comfort than vertical ones. Finally,\nbased on the concrete results obtained, a new experimental campaign is\nproposed.\n","authors":["Estibaliz Asua","Jon GutiÃ©rrez-Zaballa","Ãscar Mata-Carballeira","Jon Ander Ruiz","InÃ©s del Campo"],"pdf_url":"https://arxiv.org/pdf/2501.17664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17661v1","updated":"2025-01-29T14:01:41Z","published":"2025-01-29T14:01:41Z","title":"Multi-Agent Path Finding Using Conflict-Based Search and\n  Structural-Semantic Topometric Maps","summary":"  As industries increasingly adopt large robotic fleets, there is a pressing\nneed for computationally efficient, practical, and optimal conflict-free path\nplanning for multiple robots. Conflict-Based Search (CBS) is a popular method\nfor multi-agent path finding (MAPF) due to its completeness and optimality;\nhowever, it is often impractical for real-world applications, as it is\ncomputationally intensive to solve and relies on assumptions about agents and\noperating environments that are difficult to realize. This article proposes a\nsolution to overcome computational challenges and practicality issues of CBS by\nutilizing structural-semantic topometric maps. Instead of running CBS over\nlarge grid-based maps, the proposed solution runs CBS over a sparse topometric\nmap containing structural-semantic cells representing intersections, pathways,\nand dead ends. This approach significantly accelerates the MAPF process and\nreduces the number of conflict resolutions handled by CBS while operating in\ncontinuous time. In the proposed method, robots are assigned time ranges to\nmove between topometric regions, departing from the traditional CBS assumption\nthat a robot can move to any connected cell in a single time step. The approach\nis validated through real-world multi-robot path-finding experiments and\nbenchmarking simulations. The results demonstrate that the proposed MAPF method\ncan be applied to real-world non-holonomic robots and yields significant\nimprovement in computational efficiency compared to traditional CBS methods\nwhile improving conflict detection and resolution in cases of corridor\nsymmetries.\n","authors":["Scott Fredriksson","Yifan Bai","Akshit Saradagi","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2501.17661v1.pdf","comment":"Accepted for the 2025 IEEE International Conference on Robotics and\n  Automation (ICRA), May 19-23, 2025, Atlanta, USA"},{"id":"http://arxiv.org/abs/2501.17658v1","updated":"2025-01-29T13:44:31Z","published":"2025-01-29T13:44:31Z","title":"An eco-driving approach for ride comfort improvement","summary":"  New challenges on transport systems are emerging due to the advances that the\ncurrent paradigm is experiencing. The breakthrough of the autonomous car brings\nconcerns about ride comfort, while the pollution concerns have arisen in recent\nyears. In the model of automated automobiles, drivers are expected to become\npassengers, so, they will be more prone to suffer from ride discomfort or\nmotion sickness. Conversely, the eco-driving implications should not be set\naside because of the influence of pollution on climate and people's health. For\nthat reason, a joint assessment of the aforementioned points would have a\npositive impact. Thus, this work presents a self-organised map-based solution\nto assess ride comfort features of individuals considering their driving style\nfrom the viewpoint of eco-driving. For this purpose, a previously acquired\ndataset from an instrumented car was used to classify drivers regarding the\ncauses of their lack of ride comfort and eco-friendliness. Once drivers are\nclassified regarding their driving style, natural-language-based\nrecommendations are proposed to increase the engagement with the system. Hence,\npotential improvements of up to the 57.7% for ride comfort evaluation\nparameters, as well as up to the 47.1% in greenhouse-gasses emissions are\nexpected to be reached.\n","authors":["Ãscar Mata-Carballeira","InÃ©s del Campo","Estibalitz Asua"],"pdf_url":"https://arxiv.org/pdf/2501.17658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15384v3","updated":"2025-01-29T12:47:26Z","published":"2024-02-23T15:30:57Z","title":"Closed-loop Multi-step Planning","summary":"  Living organisms interact with their surroundings in a closed-loop fashion,\nwhere sensory inputs dictate the initiation and termination of behaviours. Even\nsimple animals are able to develop and execute complex plans, which has not yet\nbeen replicated in robotics using pure closed-loop input control. We propose a\nsolution to this problem by defining a set of discrete and temporary\nclosed-loop controllers, called ``Tasks'', each representing a closed-loop\nbehaviour. We further introduce a supervisory module which has an innate\nunderstanding of physics and causality, through which it can simulate the\nexecution of Task sequences over time and store the results in a model of the\nenvironment. On the basis of this model, plans can be made by chaining\ntemporary closed-loop controllers. Our proposed framework was implemented for a\nreal robot and tested in two scenarios as proof of concept.\n","authors":["Giulia Lafratta","Bernd Porr","Christopher Chandler","Alice Miller"],"pdf_url":"https://arxiv.org/pdf/2402.15384v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17594v1","updated":"2025-01-29T11:53:58Z","published":"2025-01-29T11:53:58Z","title":"Watch Your STEPP: Semantic Traversability Estimation using Pose\n  Projected Features","summary":"  Understanding the traversability of terrain is essential for autonomous robot\nnavigation, particularly in unstructured environments such as natural\nlandscapes. Although traditional methods, such as occupancy mapping, provide a\nbasic framework, they often fail to account for the complex mobility\ncapabilities of some platforms such as legged robots. In this work, we propose\na method for estimating terrain traversability by learning from demonstrations\nof human walking. Our approach leverages dense, pixel-wise feature embeddings\ngenerated using the DINOv2 vision Transformer model, which are processed\nthrough an encoder-decoder MLP architecture to analyze terrain segments. The\naveraged feature vectors, extracted from the masked regions of interest, are\nused to train the model in a reconstruction-based framework. By minimizing\nreconstruction loss, the network distinguishes between familiar terrain with a\nlow reconstruction error and unfamiliar or hazardous terrain with a higher\nreconstruction error. This approach facilitates the detection of anomalies,\nallowing a legged robot to navigate more effectively through challenging\nterrain. We run real-world experiments on the ANYmal legged robot both indoor\nand outdoor to prove our proposed method. The code is open-source, while video\ndemonstrations can be found on our website: https://rpl-cs-ucl.github.io/STEPP\n","authors":["Sebastian Ãgidius","Dennis Hadjivelichkov","Jianhao Jiao","Jonathan Embley-Riches","Dimitrios Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2501.17594v1.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.17437v1","updated":"2025-01-29T06:32:55Z","published":"2025-01-29T06:32:55Z","title":"Bayesian BIM-Guided Construction Robot Navigation with NLP Safety\n  Prompts in Dynamic Environments","summary":"  Construction robotics increasingly relies on natural language processing for\ntask execution, creating a need for robust methods to interpret commands in\ncomplex, dynamic environments. While existing research primarily focuses on\nwhat tasks robots should perform, less attention has been paid to how these\ntasks should be executed safely and efficiently. This paper presents a novel\nprobabilistic framework that uses sentiment analysis from natural language\ncommands to dynamically adjust robot navigation policies in construction\nenvironments. The framework leverages Building Information Modeling (BIM) data\nand natural language prompts to create adaptive navigation strategies that\naccount for varying levels of environmental risk and uncertainty. We introduce\nan object-aware path planning approach that combines exponential potential\nfields with a grid-based representation of the environment, where the potential\nfields are dynamically adjusted based on the semantic analysis of user prompts.\nThe framework employs Bayesian inference to consolidate multiple information\nsources: the static data from BIM, the semantic content of natural language\ncommands, and the implied safety constraints from user prompts. We demonstrate\nour approach through experiments comparing three scenarios: baseline\nshortest-path planning, safety-oriented navigation, and risk-aware routing.\nResults show that our method successfully adapts path planning based on natural\nlanguage sentiment, achieving a 50\\% improvement in minimum distance to\nobstacles when safety is prioritized, while maintaining reasonable path\nlengths. Scenarios with contrasting prompts, such as \"dangerous\" and \"safe\",\ndemonstrate the framework's ability to modify paths. This approach provides a\nflexible foundation for integrating human knowledge and safety considerations\ninto construction robot navigation.\n","authors":["Mani Amani","Reza Akhavian"],"pdf_url":"https://arxiv.org/pdf/2501.17437v1.pdf","comment":"Submitted to International Symposium on Automation and Robotics in\n  Construction (ISARC)"},{"id":"http://arxiv.org/abs/2501.17431v1","updated":"2025-01-29T06:14:27Z","published":"2025-01-29T06:14:27Z","title":"Human-Aligned Skill Discovery: Balancing Behaviour Exploration and\n  Alignment","summary":"  Unsupervised skill discovery in Reinforcement Learning aims to mimic humans'\nability to autonomously discover diverse behaviors. However, existing methods\nare often unconstrained, making it difficult to find useful skills, especially\nin complex environments, where discovered skills are frequently unsafe or\nimpractical. We address this issue by proposing Human-aligned Skill Discovery\n(HaSD), a framework that incorporates human feedback to discover safer, more\naligned skills. HaSD simultaneously optimises skill diversity and alignment\nwith human values. This approach ensures that alignment is maintained\nthroughout the skill discovery process, eliminating the inefficiencies\nassociated with exploring unaligned skills. We demonstrate its effectiveness in\nboth 2D navigation and SafetyGymnasium environments, showing that HaSD\ndiscovers diverse, human-aligned skills that are safe and useful for downstream\ntasks. Finally, we extend HaSD by learning a range of configurable skills with\nvarying degrees of diversity alignment trade-offs that could be useful in\npractical scenarios.\n","authors":["Maxence Hussonnois","Thommen George Karimpanal","Santu Rana"],"pdf_url":"https://arxiv.org/pdf/2501.17431v1.pdf","comment":"Accepted at the 24th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2025)"},{"id":"http://arxiv.org/abs/2408.07806v2","updated":"2025-01-29T06:13:45Z","published":"2024-08-14T20:30:34Z","title":"From Decision to Action in Surgical Autonomy: Multi-Modal Large Language\n  Models for Robot-Assisted Blood Suction","summary":"  The rise of Large Language Models (LLMs) has impacted research in robotics\nand automation. While progress has been made in integrating LLMs into general\nrobotics tasks, a noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in robotic\nsurgery, which entails the ability to reason and adapt to changes in the\nenvironment, remains a significant challenge. In this work, we propose a\nmulti-modal LLM integration in robot-assisted surgery for autonomous blood\nsuction. The reasoning and prioritization are delegated to the higher-level\ntask-planning LLM, and the motion planning and execution are handled by the\nlower-level deep reinforcement learning model, creating a distributed agency\nbetween the two components. As surgical operations are highly dynamic and may\nencounter unforeseen circumstances, blood clots and active bleeding were\nintroduced to influence decision-making. Results showed that using a\nmulti-modal LLM as a higher-level reasoning unit can account for these surgical\ncomplexities to achieve a level of reasoning previously unattainable in\nrobot-assisted surgeries. These findings demonstrate the potential of\nmulti-modal LLMs to significantly enhance contextual understanding and\ndecision-making in robotic-assisted surgeries, marking a step toward autonomous\nsurgical systems.\n","authors":["Sadra Zargarzadeh","Maryam Mirzaei","Yafei Ou","Mahdi Tavakoli"],"pdf_url":"https://arxiv.org/pdf/2408.07806v2.pdf","comment":"Accepted for Publication in IEEE Robotics and Automation Letters,\n  2025"},{"id":"http://arxiv.org/abs/2501.17424v1","updated":"2025-01-29T05:37:47Z","published":"2025-01-29T05:37:47Z","title":"Certificated Actor-Critic: Hierarchical Reinforcement Learning with\n  Control Barrier Functions for Safe Navigation","summary":"  Control Barrier Functions (CBFs) have emerged as a prominent approach to\ndesigning safe navigation systems of robots. Despite their popularity, current\nCBF-based methods exhibit some limitations: optimization-based safe control\ntechniques tend to be either myopic or computationally intensive, and they rely\non simplified system models; conversely, the learning-based methods suffer from\nthe lack of quantitative indication in terms of navigation performance and\nsafety. In this paper, we present a new model-free reinforcement learning\nalgorithm called Certificated Actor-Critic (CAC), which introduces a\nhierarchical reinforcement learning framework and well-defined reward functions\nderived from CBFs. We carry out theoretical analysis and proof of our\nalgorithm, and propose several improvements in algorithm implementation. Our\nanalysis is validated by two simulation experiments, showing the effectiveness\nof our proposed CAC algorithm.\n","authors":["Junjun Xie","Shuhao Zhao","Liang Hu","Huijun Gao"],"pdf_url":"https://arxiv.org/pdf/2501.17424v1.pdf","comment":"Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2501.16411v2","updated":"2025-01-29T03:52:39Z","published":"2025-01-27T18:59:58Z","title":"PhysBench: Benchmarking and Enhancing Vision-Language Models for\n  Physical World Understanding","summary":"  Understanding the physical world is a fundamental challenge in embodied AI,\ncritical for enabling agents to perform complex tasks and operate safely in\nreal-world environments. While Vision-Language Models (VLMs) have shown great\npromise in reasoning and task planning for embodied agents, their ability to\ncomprehend physical phenomena remains extremely limited. To close this gap, we\nintroduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'\nphysical world understanding capability across a diverse set of tasks.\nPhysBench contains 10,002 entries of interleaved video-image-text data,\ncategorized into four major domains: physical object properties, physical\nobject relationships, physical scene understanding, and physics-based dynamics,\nfurther divided into 19 subclasses and 8 distinct capability dimensions. Our\nextensive experiments, conducted on 75 representative VLMs, reveal that while\nthese models excel in common-sense reasoning, they struggle with understanding\nthe physical world -- likely due to the absence of physical knowledge in their\ntraining data and the lack of embedded physical priors. To tackle the\nshortfall, we introduce PhysAgent, a novel framework that combines the\ngeneralization strengths of VLMs with the specialized expertise of vision\nmodels, significantly enhancing VLMs' physical understanding across a variety\nof tasks, including an 18.4\\% improvement on GPT-4o. Furthermore, our results\ndemonstrate that enhancing VLMs' physical world understanding capabilities can\nhelp embodied agents such as MOKA. We believe that PhysBench and PhysAgent\noffer valuable insights and contribute to bridging the gap between VLMs and\nphysical world understanding.\n","authors":["Wei Chow","Jiageng Mao","Boyi Li","Daniel Seita","Vitor Guizilini","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2501.16411v2.pdf","comment":"ICLR 2025. Project page: https://physbench.github.io/ Dataset:\n  https://huggingface.co/datasets/USC-GVL/PhysBench"},{"id":"http://arxiv.org/abs/2501.02189v3","updated":"2025-01-29T00:26:29Z","published":"2025-01-04T04:59:33Z","title":"Benchmark Evaluations, Applications, and Challenges of Large Vision\n  Language Models: A Survey","summary":"  Multimodal Vision Language Models (VLMs) have emerged as a transformative\ntechnology at the intersection of computer vision and natural language\nprocessing, enabling machines to perceive and reason about the world through\nboth visual and textual modalities. For example, models such as CLIP, Claude,\nand GPT-4V demonstrate strong reasoning and understanding abilities on visual\nand textual data and beat classical single modality vision models on zero-shot\nclassification. Despite their rapid advancements in research and growing\npopularity in applications, a comprehensive survey of existing studies on VLMs\nis notably lacking, particularly for researchers aiming to leverage VLMs in\ntheir specific domains. To this end, we provide a systematic overview of VLMs\nin the following aspects: model information of the major VLMs developed over\nthe past five years (2019-2024); the main architectures and training methods of\nthese VLMs; summary and categorization of the popular benchmarks and evaluation\nmetrics of VLMs; the applications of VLMs including embodied agents, robotics,\nand video generation; the challenges and issues faced by current VLMs such as\nhallucination, fairness, and safety. Detailed collections including papers and\nmodel repository links are listed in\nhttps://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.\n","authors":["Zongxia Li","Xiyang Wu","Hongyang Du","Huy Nghiem","Guangyao Shi"],"pdf_url":"https://arxiv.org/pdf/2501.02189v3.pdf","comment":"35 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.17351v1","updated":"2025-01-29T00:06:01Z","published":"2025-01-29T00:06:01Z","title":"Realtime Limb Trajectory Optimization for Humanoid Running Through\n  Centroidal Angular Momentum Dynamics","summary":"  One of the essential aspects of humanoid robot running is determining the\nlimb-swinging trajectories. During the flight phases, where the ground reaction\nforces are not available for regulation, the limb swinging trajectories are\nsignificant for the stability of the next stance phase. Due to the conservation\nof angular momentum, improper leg and arm swinging results in highly tilted and\nunsustainable body configurations at the next stance phase landing. In such\ncases, the robotic system fails to maintain locomotion independent of the\nstability of the center of mass trajectories. This problem is more apparent for\nfast and high flight time trajectories. This paper proposes a real-time\nnonlinear limb trajectory optimization problem for humanoid running. The\noptimization problem is tested on two different humanoid robot models, and the\ngenerated trajectories are verified using a running algorithm for both robots\nin a simulation environment.\n","authors":["Sait Sovukluk","Robert Schuller","Johannes Englsberger","Christian Ott"],"pdf_url":"https://arxiv.org/pdf/2501.17351v1.pdf","comment":"Submitted to IEEE ICRA2025"},{"id":"http://arxiv.org/abs/2501.03859v2","updated":"2025-01-29T23:50:46Z","published":"2025-01-07T15:16:16Z","title":"A Synergistic Framework for Learning Shape Estimation and Shape-Aware\n  Whole-Body Control Policy for Continuum Robots","summary":"  In this paper, we present a novel synergistic framework for learning shape\nestimation and a shape-aware whole-body control policy for tendon-driven\ncontinuum robots. Our approach leverages the interaction between two Augmented\nNeural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and\nControl-NODE -- to achieve continuous shape estimation and shape-aware control.\nThe Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it\nto adapt and account for model mismatches, while the Control-NODE uses this\nshape information to optimize a whole-body control policy, trained in a Model\nPredictive Control (MPC) fashion. This unified framework effectively overcomes\nlimitations of existing data-driven methods, such as poor shape awareness and\nchallenges in capturing complex nonlinear dynamics. Extensive evaluations in\nboth simulation and real-world environments demonstrate the framework's robust\nperformance in shape estimation, trajectory tracking, and obstacle avoidance.\nThe proposed method consistently outperforms state-of-the-art end-to-end,\nNeural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of\ntracking accuracy and generalization capabilities.\n","authors":["Mohammadreza Kasaei","Farshid Alambeigi","Mohsen Khadem"],"pdf_url":"https://arxiv.org/pdf/2501.03859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18016v1","updated":"2025-01-29T22:06:53Z","published":"2025-01-29T22:06:53Z","title":"Digital Twin-Enabled Real-Time Control in Robotic Additive Manufacturing\n  via Soft Actor-Critic Reinforcement Learning","summary":"  Smart manufacturing systems increasingly rely on adaptive control mechanisms\nto optimize complex processes. This research presents a novel approach\nintegrating Soft Actor-Critic (SAC) reinforcement learning with digital twin\ntechnology to enable real-time process control in robotic additive\nmanufacturing. We demonstrate our methodology using a Viper X300s robot arm,\nimplementing two distinct control scenarios: static target acquisition and\ndynamic trajectory following. The system architecture combines Unity's\nsimulation environment with ROS2 for seamless digital twin synchronization,\nwhile leveraging transfer learning to efficiently adapt trained models across\ntasks. Our hierarchical reward structure addresses common reinforcement\nlearning challenges including local minima avoidance, convergence acceleration,\nand training stability. Experimental results show rapid policy convergence and\nrobust task execution in both simulated and physical environments, with\nperformance metrics including cumulative reward, value prediction accuracy,\npolicy loss, and discrete entropy coefficient demonstrating the effectiveness\nof our approach. This work advances the integration of reinforcement learning\nwith digital twins for industrial robotics applications, providing a framework\nfor enhanced adaptive real-time control for smart additive manufacturing\nprocess.\n","authors":["Matsive Ali","Sandesh Giri","Sen Liu","Qin Yang"],"pdf_url":"https://arxiv.org/pdf/2501.18016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06987v2","updated":"2025-01-29T22:02:56Z","published":"2025-01-13T00:29:57Z","title":"Hand-Object Contact Detection using Grasp Quality Metrics","summary":"  We propose a novel hand-object contact detection system based on grasp\nquality metrics extracted from object and hand poses, and evaluated its\nperformance using the DexYCB dataset. Our evaluation demonstrated the system's\nhigh accuracy (approaching 90%). Future work will focus on a real-time\nimplementation using vision-based estimation, and integrating it to a\nrobot-to-human handover system.\n","authors":["Thanh Vinh Nguyen","Akansel Cosgun"],"pdf_url":"https://arxiv.org/pdf/2501.06987v2.pdf","comment":"Submitted to the 2025 IEEE/ACM International Conference on\n  Human-Robot Interaction (HRI'25)"},{"id":"http://arxiv.org/abs/2409.09295v2","updated":"2025-01-29T21:55:32Z","published":"2024-09-14T04:09:26Z","title":"GEVO: Memory-Efficient Monocular Visual Odometry Using Gaussians","summary":"  Constructing a high-fidelity representation of the 3D scene using a monocular\ncamera can enable a wide range of applications on mobile devices, such as\nmicro-robots, smartphones, and AR/VR headsets. On these devices, memory is\noften limited in capacity and its access often dominates the consumption of\ncompute energy. Although Gaussian Splatting (GS) allows for high-fidelity\nreconstruction of 3D scenes, current GS-based SLAM is not memory efficient as a\nlarge number of past images is stored to retrain Gaussians for reducing\ncatastrophic forgetting. These images often require two-orders-of-magnitude\nhigher memory than the map itself and thus dominate the total memory usage. In\nthis work, we present GEVO, a GS-based monocular SLAM framework that achieves\ncomparable fidelity as prior methods by rendering (instead of storing) them\nfrom the existing map. Novel Gaussian initialization and optimization\ntechniques are proposed to remove artifacts from the map and delay the\ndegradation of the rendered images over time. Across a variety of environments,\nGEVO achieves comparable map fidelity while reducing the memory overhead to\naround 58 MBs, which is up to 94x lower than prior works.\n","authors":["Dasong Gao","Peter Zhi Xuan Li","Vivienne Sze","Sertac Karaman"],"pdf_url":"https://arxiv.org/pdf/2409.09295v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2501.17982v1","updated":"2025-01-29T20:37:01Z","published":"2025-01-29T20:37:01Z","title":"Belief Roadmaps with Uncertain Landmark Evanescence","summary":"  We would like a robot to navigate to a goal location while minimizing state\nuncertainty. To aid the robot in this endeavor, maps provide a prior belief\nover the location of objects and regions of interest. To localize itself within\nthe map, a robot identifies mapped landmarks using its sensors. However, as the\ntime between map creation and robot deployment increases, portions of the map\ncan become stale, and landmarks, once believed to be permanent, may disappear.\nWe refer to the propensity of a landmark to disappear as landmark evanescence.\nReasoning about landmark evanescence during path planning, and the associated\nimpact on localization accuracy, requires analyzing the presence or absence of\neach landmark, leading to an exponential number of possible outcomes of a given\nmotion plan. To address this complexity, we develop BRULE, an extension of the\nBelief Roadmap. During planning, we replace the belief over future robot poses\nwith a Gaussian mixture which is able to capture the effects of landmark\nevanescence. Furthermore, we show that belief updates can be made efficient,\nand that maintaining a random subset of mixture components is sufficient to\nfind high quality solutions. We demonstrate performance in simulated and\nreal-world experiments. Software is available at https://bit.ly/BRULE.\n","authors":["Erick Fuentes","Jared Strader","Ethan Fahnestock","Nicholas Roy"],"pdf_url":"https://arxiv.org/pdf/2501.17982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17968v1","updated":"2025-01-29T20:06:15Z","published":"2025-01-29T20:06:15Z","title":"Online Trajectory Replanner for Dynamically Grasping Irregular Objects","summary":"  This paper presents a new trajectory replanner for grasping irregular\nobjects. Unlike conventional grasping tasks where the object's geometry is\nassumed simple, we aim to achieve a \"dynamic grasp\" of the irregular objects,\nwhich requires continuous adjustment during the grasping process. To\neffectively handle irregular objects, we propose a trajectory optimization\nframework that comprises two phases. Firstly, in a specified time limit of 10s,\ninitial offline trajectories are computed for a seamless motion from an initial\nconfiguration of the robot to grasp the object and deliver it to a pre-defined\ntarget location. Secondly, fast online trajectory optimization is implemented\nto update robot trajectories in real-time within 100 ms. This helps to mitigate\npose estimation errors from the vision system. To account for model\ninaccuracies, disturbances, and other non-modeled effects, trajectory tracking\ncontrollers for both the robot and the gripper are implemented to execute the\noptimal trajectories from the proposed framework. The intensive experimental\nresults effectively demonstrate the performance of our trajectory planning\nframework in both simulation and real-world scenarios.\n","authors":["Minh Nhat Vu","Florian Grander","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2501.17968v1.pdf","comment":"7 pages. Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2501.09905v4","updated":"2025-01-29T19:58:23Z","published":"2025-01-17T01:32:18Z","title":"SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon\n  Visuomotor Learning","summary":"  We present a low-cost legged mobile manipulation system that solves\nlong-horizon real-world tasks, trained by reinforcement learning purely in\nsimulation. This system is made possible by 1) a hierarchical design of a\nhigh-level policy for visual-mobile manipulation following task instructions,\nand a low-level quadruped locomotion policy, 2) a teacher and student training\npipeline for the high level, which trains a teacher to tackle long-horizon\ntasks using privileged task decomposition and target object information, and\nfurther trains a student for visual-mobile manipulation via RL guided by the\nteacher's behavior, and 3) a suite of techniques for minimizing the sim-to-real\ngap.\n  In contrast to many previous works that use high-end equipments, our system\ndemonstrates effective performance with more accessible hardware --\nspecifically, a Unitree Go1 quadruped, a WidowX-250S arm, and a single\nwrist-mounted RGB camera -- despite the increased challenges of sim-to-real\ntransfer. Trained fully in simulation, a single policy autonomously solves\nlong-horizon tasks involving search, move to, grasp, transport, and drop into,\nachieving nearly 80% real-world success. This performance is comparable to that\nof expert human teleoperation on the same tasks while the robot is more\nefficient, operating at about 1.5x the speed of the teleoperation. Finally, we\nperform extensive ablations on key techniques for efficient RL training and\neffective sim-to-real transfer, and demonstrate effective deployment across\ndiverse indoor and outdoor scenes under various lighting conditions.\n","authors":["Haichao Zhang","Haonan Yu","Le Zhao","Andrew Choi","Qinxun Bai","Break Yang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2501.09905v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17963v1","updated":"2025-01-29T19:53:14Z","published":"2025-01-29T19:53:14Z","title":"Physics-Grounded Differentiable Simulation for Soft Growing Robots","summary":"  Soft-growing robots (i.e., vine robots) are a promising class of soft robots\nthat allow for navigation and growth in tightly confined environments. However,\nthese robots remain challenging to model and control due to the complex\ninterplay of the inflated structure and inextensible materials, which leads to\nobstacles for autonomous operation and design optimization. Although there\nexist simulators for these systems that have achieved qualitative and\nquantitative success in matching high-level behavior, they still often fail to\ncapture realistic vine robot shapes using simplified parameter models and have\ndifficulties in high-throughput simulation necessary for planning and parameter\noptimization. We propose a differentiable simulator for these systems, enabling\nthe use of the simulator \"in-the-loop\" of gradient-based optimization\napproaches to address the issues listed above. With the more complex parameter\nfitting made possible by this approach, we experimentally validate and\nintegrate a closed-form nonlinear stiffness model for thin-walled inflated\ntubes based on a first-principles approach to local material wrinkling. Our\nsimulator also takes advantage of data-parallel operations by leveraging\nexisting differentiable computation frameworks, allowing multiple simultaneous\nrollouts. We demonstrate the feasibility of using a physics-grounded nonlinear\nstiffness model within our simulator, and how it can be an effective tool in\nsim-to-real transfer. We provide our implementation open source.\n","authors":["Lucas Chen","Yitian Gao","Sicheng Wang","Francesco Fuentes","Laura H. Blumenschein","Zachary Kingston"],"pdf_url":"https://arxiv.org/pdf/2501.17963v1.pdf","comment":"8 pages, 7 figures. IEEE-RAS International Conference on Soft\n  Robotics (RoboSoft) 2025"},{"id":"http://arxiv.org/abs/2501.17962v1","updated":"2025-01-29T19:52:24Z","published":"2025-01-29T19:52:24Z","title":"Agricultural Industry Initiatives on Autonomy: How collaborative\n  initiatives of VDMA and AEF can facilitate complexity in domain crossing\n  harmonization needs","summary":"  The agricultural industry is undergoing a significant transformation with the\nincreasing adoption of autonomous technologies. Addressing complex challenges\nrelated to safety and security, components and validation procedures, and\nliability distribution is essential to facilitate the adoption of autonomous\ntechnologies. This paper explores the collaborative groups and initiatives\nundertaken to address these challenges. These groups investigate inter alia\nthree focal topics: 1) describe the functional architecture of the operational\nrange, 2) define the work context, i.e., the realistic scenarios that emerge in\nvarious agricultural applications, and 3) the static and dynamic detection\ncases that need to be detected by sensor sets. Linked by the Agricultural\nOperational Design Domain (Agri-ODD), use case descriptions, risk analysis, and\nquestions of liability can be handled. By providing an overview of these\ncollaborative initiatives, this paper aims to highlight the joint development\nof autonomous agricultural systems that enhance the overall efficiency of\nfarming operations.\n","authors":["Georg Happich","Alexander Grever","Julius SchÃ¶ning"],"pdf_url":"https://arxiv.org/pdf/2501.17962v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.07508v3","updated":"2025-01-29T19:15:11Z","published":"2024-08-14T12:36:12Z","title":"Non-Gaited Legged Locomotion with Monte-Carlo Tree Search and Supervised\n  Learning","summary":"  Legged robots are able to navigate complex terrains by continuously\ninteracting with the environment through careful selection of contact sequences\nand timings. However, the combinatorial nature behind contact planning hinders\nthe applicability of such optimization problems on hardware. In this work, we\npresent a novel approach that optimizes gait sequences and respective timings\nfor legged robots in the context of optimization-based controllers through the\nuse of sampling-based methods and supervised learning techniques. We propose to\nbootstrap the search by learning an optimal value function in order to speed-up\nthe gait planning procedure making it applicable in real-time. To validate our\nproposed method, we showcase its performance both in simulation and on hardware\nusing a 22 kg electric quadruped robot. The method is assessed on different\nterrains, under external perturbations, and in comparison to a standard control\napproach where the gait sequence is fixed a priori.\n","authors":["Ilyass Taouil","Lorenzo Amatucci","Majid Khadiv","Angela Dai","Victor Barasuol","Giulio Turrisi","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2408.07508v3.pdf","comment":null}]},"2025-01-30T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2501.18592v1","updated":"2025-01-30T18:59:36Z","published":"2025-01-30T18:59:36Z","title":"Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models","summary":"  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n","authors":["Hao Dong","Moru Liu","Kaiyang Zhou","Eleni Chatzi","Juho Kannala","Cyrill Stachniss","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2501.18592v1.pdf","comment":"Project page:\n  https://github.com/donghao51/Awesome-Multimodal-Adaptation"},{"id":"http://arxiv.org/abs/2501.18564v1","updated":"2025-01-30T18:37:16Z","published":"2025-01-30T18:37:16Z","title":"SAM2Act: Integrating Visual Foundation Model with A Memory Architecture\n  for Robotic Manipulation","summary":"  Robotic manipulation systems operating in diverse, dynamic environments must\nexhibit three critical abilities: multitask interaction, generalization to\nunseen scenarios, and spatial memory. While significant progress has been made\nin robotic manipulation, existing approaches often fall short in generalization\nto complex environmental variations and addressing memory-dependent tasks. To\nbridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based\npolicy that leverages multi-resolution upsampling with visual representations\nfrom large-scale foundation model. SAM2Act achieves a state-of-the-art average\nsuccess rate of 86.8% across 18 tasks in the RLBench benchmark, and\ndemonstrates robust generalization on The Colosseum benchmark, with only a 4.3%\nperformance gap under diverse environmental perturbations. Building on this\nfoundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2,\nwhich incorporates a memory bank, an encoder, and an attention mechanism to\nenhance spatial memory. To address the need for evaluating memory-dependent\ntasks, we introduce MemoryBench, a novel benchmark designed to assess spatial\nmemory and action recall in robotic manipulation. SAM2Act+ achieves competitive\nperformance on MemoryBench, significantly outperforming existing approaches and\npushing the boundaries of memory-enabled robotic systems. Project page:\nhttps://sam2act.github.io/\n","authors":["Haoquan Fang","Markus Grotz","Wilbert Pumacay","Yi Ru Wang","Dieter Fox","Ranjay Krishna","Jiafei Duan"],"pdf_url":"https://arxiv.org/pdf/2501.18564v1.pdf","comment":"Including Appendix, Project page: https://sam2act.github.io/"},{"id":"http://arxiv.org/abs/2501.18543v1","updated":"2025-01-30T18:12:11Z","published":"2025-01-30T18:12:11Z","title":"Learning Priors of Human Motion With Vision Transformers","summary":"  A clear understanding of where humans move in a scenario, their usual paths\nand speeds, and where they stop, is very important for different applications,\nsuch as mobility studies in urban areas or robot navigation tasks within\nhuman-populated environments. We propose in this article, a neural architecture\nbased on Vision Transformers (ViTs) to provide this information. This solution\ncan arguably capture spatial correlations more effectively than Convolutional\nNeural Networks (CNNs). In the paper, we describe the methodology and proposed\nneural architecture and show the experiments' results with a standard dataset.\nWe show that the proposed ViT architecture improves the metrics compared to a\nmethod based on a CNN.\n","authors":["Placido Falqueto","Alberto Sanfeliu","Luigi Palopoli","Daniele Fontanelli"],"pdf_url":"https://arxiv.org/pdf/2501.18543v1.pdf","comment":"2024 IEEE 48th Annual Computers, Software, and Applications\n  Conference (COMPSAC). IEEE, 2024"},{"id":"http://arxiv.org/abs/2501.13919v2","updated":"2025-01-30T17:35:08Z","published":"2025-01-23T18:58:03Z","title":"Temporal Preference Optimization for Long-Form Video Understanding","summary":"  Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website.\n","authors":["Rui Li","Xiaohan Wang","Yuhui Zhang","Zeyu Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2501.13919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20095v3","updated":"2025-01-30T17:34:37Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Vision Language Models (VLMs) have recently been leveraged to generate\nrobotic actions, forming Vision-Language-Action (VLA) models. However, directly\nadapting a pretrained VLM for robotic control remains challenging, particularly\nwhen constrained by a limited number of robot demonstrations. In this work, we\nintroduce LLaRA: Large Language and Robotics Assistant, a framework that\nformulates robot action policy as visuo-textual conversations and enables an\nefficient transfer of a pretrained VLM into a powerful VLA, motivated by the\nsuccess of visual instruction tuning in Computer Vision. First, we present an\nautomated pipeline to generate conversation-style instruction tuning data for\nrobots from existing behavior cloning datasets, aligning robotic actions with\nimage pixel coordinates. Further, we enhance this dataset in a self-supervised\nmanner by defining six auxiliary tasks, without requiring any additional action\nannotations. We show that a VLM finetuned with a limited amount of such\ndatasets can produce meaningful action decisions for robotic control. Through\nexperiments across multiple simulated and real-world tasks, we demonstrate that\nLLaRA achieves state-of-the-art performance while preserving the generalization\ncapabilities of large language models. The code, datasets, and pretrained\nmodels are available at https://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.18516v1","updated":"2025-01-30T17:28:11Z","published":"2025-01-30T17:28:11Z","title":"Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models","summary":"  Object rearrangement is a significant task for collaborative robots, where\nthey are directed to manipulate objects into a specified goal state.\nDetermining the placement of objects is a major challenge that influences the\nefficiency of the rearrangement process. Most current methods heavily rely on\npre-collected datasets to train the model for predicting the goal position and\nare restricted to specific instructions, which limits their broader\napplicability and effectiveness.In this paper, we propose a framework of\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Particularly, our approach mimics human reasoning by using past\nsuccessful experiences as a reference to infer the desired goal position. Based\non LLM's strong natural language comprehension and inference ability, our\nmethod can generalise to handle various everyday objects and free-form language\ninstructions in a zero-shot manner. Experimental results demonstrate that our\nmethods can effectively execute the robotic rearrangement tasks, even those\ninvolving long sequential orders.\n","authors":["Guanqun Cao","Ryan Mckenna","John Oyekan"],"pdf_url":"https://arxiv.org/pdf/2501.18516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18505v1","updated":"2025-01-30T17:15:11Z","published":"2025-01-30T17:15:11Z","title":"Path Planning and Optimization for Cuspidal 6R Manipulators","summary":"  A cuspidal robot can move from one inverse kinematics (IK) solution to\nanother without crossing a singularity. Multiple industrial robots are\ncuspidal. They tend to have a beautiful mechanical design, but they pose path\nplanning challenges. A task-space path may have a valid IK solution for each\npoint along the path, but a continuous joint-space path may depend on the\nchoice of the IK solution or even be infeasible. This paper presents new\nanalysis, path planning, and optimization methods to enhance the utility of\ncuspidal robots. We first demonstrate an efficient method to identify cuspidal\nrobots and show, for the first time, that the ABB GoFa and certain robots with\nthree parallel joint axes are cuspidal. We then propose a new path planning\nmethod for cuspidal robots by finding all IK solutions for each point along a\ntask-space path and constructing a graph to connect each vertex corresponding\nto an IK solution. Graph edges are weighted based on the optimization metric,\nsuch as minimizing joint velocity. The optimal feasible path is the shortest\npath in the graph. This method can find non-singular paths as well as smooth\npaths which pass through singularities. Finally, this path planning method is\nincorporated into a path optimization algorithm. Given a fixed workspace\ntoolpath, we optimize the offset of the toolpath in the robot base frame while\nensuring continuous joint motion. Code examples are available in a publicly\naccessible repository.\n","authors":["Alexander J. Elias","John T. Wen"],"pdf_url":"https://arxiv.org/pdf/2501.18505v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.18490v1","updated":"2025-01-30T17:05:32Z","published":"2025-01-30T17:05:32Z","title":"Curriculum-based Sample Efficient Reinforcement Learning for Robust\n  Stabilization of a Quadrotor","summary":"  This article introduces a curriculum learning approach to develop a\nreinforcement learning-based robust stabilizing controller for a Quadrotor that\nmeets predefined performance criteria. The learning objective is to achieve\ndesired positions from random initial conditions while adhering to both\ntransient and steady-state performance specifications. This objective is\nchallenging for conventional one-stage end-to-end reinforcement learning, due\nto the strong coupling between position and orientation dynamics, the\ncomplexity in designing and tuning the reward function, and poor sample\nefficiency, which necessitates substantial computational resources and leads to\nextended convergence times. To address these challenges, this work decomposes\nthe learning objective into a three-stage curriculum that incrementally\nincreases task complexity. The curriculum begins with learning to achieve\nstable hovering from a fixed initial condition, followed by progressively\nintroducing randomization in initial positions, orientations and velocities. A\nnovel additive reward function is proposed, to incorporate transient and\nsteady-state performance specifications. The results demonstrate that the\nProximal Policy Optimization (PPO)-based curriculum learning approach, coupled\nwith the proposed reward structure, achieves superior performance compared to a\nsingle-stage PPO-trained policy with the same reward function, while\nsignificantly reducing computational resource requirements and convergence\ntime. The curriculum-trained policy's performance and robustness are thoroughly\nvalidated under random initial conditions and in the presence of disturbances.\n","authors":["Fausto Mauricio Lagos Suarez","Akshit Saradagi","Vidya Sumathy","Shruti Kotpaliwar","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2501.18490v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.18448v1","updated":"2025-01-30T16:00:26Z","published":"2025-01-30T16:00:26Z","title":"Autonomy and Safety Assurance in the Early Development of Robotics and\n  Autonomous Systems","summary":"  This report provides an overview of the workshop titled Autonomy and Safety\nAssurance in the Early Development of Robotics and Autonomous Systems, hosted\nby the Centre for Robotic Autonomy in Demanding and Long-Lasting Environments\n(CRADLE) on September 2, 2024, at The University of Manchester, UK. The event\nbrought together representatives from six regulatory and assurance bodies\nacross diverse sectors to discuss challenges and evidence for ensuring the\nsafety of autonomous and robotic systems, particularly autonomous inspection\nrobots (AIR). The workshop featured six invited talks by the regulatory and\nassurance bodies. CRADLE aims to make assurance an integral part of engineering\nreliable, transparent, and trustworthy autonomous systems. Key discussions\nrevolved around three research questions: (i) challenges in assuring safety for\nAIR; (ii) evidence for safety assurance; and (iii) how assurance cases need to\ndiffer for autonomous systems. Following the invited talks, the breakout groups\nfurther discussed the research questions using case studies from ground (rail),\nnuclear, underwater, and drone-based AIR. This workshop offered a valuable\nopportunity for representatives from industry, academia, and regulatory bodies\nto discuss challenges related to assured autonomy. Feedback from participants\nindicated a strong willingness to adopt a design-for-assurance process to\nensure that robots are developed and verified to meet regulatory expectations.\n","authors":["Dhaminda B. Abeywickrama","Michael Fisher","Frederic Wheeler","Louise Dennis"],"pdf_url":"https://arxiv.org/pdf/2501.18448v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2501.18351v1","updated":"2025-01-30T13:55:08Z","published":"2025-01-30T13:55:08Z","title":"Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic\n  Navigation in Unstructured Outdoor Environments","summary":"  Path planning with strong environmental adaptability plays a crucial role in\nrobotic navigation in unstructured outdoor environments, especially in the case\nof low-quality location and map information. The path planning ability of a\nrobot depends on the identification of the traversability of global and local\nground areas. In real-world scenarios, the complexity of outdoor open\nenvironments makes it difficult for robots to identify the traversability of\nground areas that lack a clearly defined structure. Moreover, most existing\nmethods have rarely analyzed the integration of local and global traversability\nidentifications in unstructured outdoor scenarios. To address this problem, we\npropose a novel method, Dual-BEV Nav, first introducing Bird's Eye View (BEV)\nrepresentations into local planning to generate high-quality traversable paths.\nThen, these paths are projected onto the global traversability map generated by\nthe global BEV planning model to obtain the optimal waypoints. By integrating\nthe traversability from both local and global BEV, we establish a dual-layer\nBEV heuristic planning paradigm, enabling long-distance navigation in\nunstructured outdoor environments. We test our approach through both public\ndataset evaluations and real-world robot deployments, yielding promising\nresults. Compared to baselines, the Dual-BEV Nav improved temporal distance\nprediction accuracy by up to $18.7\\%$. In the real-world deployment, under\nconditions significantly different from the training set and with notable\nocclusions in the global BEV, the Dual-BEV Nav successfully achieved a\n65-meter-long outdoor navigation. Further analysis demonstrates that the local\nBEV representation significantly enhances the rationality of the planning,\nwhile the global BEV probability map ensures the robustness of the overall\nplanning.\n","authors":["Jianfeng Zhang","Hanlin Dong","Jian Yang","Jiahui Liu","Shibo Huang","Ke Li","Xuan Tang","Xian Wei","Xiong You"],"pdf_url":"https://arxiv.org/pdf/2501.18351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15123v2","updated":"2025-01-30T13:00:49Z","published":"2024-10-19T14:38:29Z","title":"MeshDMP: Motion Planning on Discrete Manifolds using Dynamic Movement\n  Primitives","summary":"  An open problem in industrial automation is to reliably perform tasks\nrequiring in-contact movements with complex workpieces, as current solutions\nlack the ability to seamlessly adapt to the workpiece geometry. In this paper,\nwe propose a Learning from Demonstration approach that allows a robot\nmanipulator to learn and generalise motions across complex surfaces by\nleveraging differential mathematical operators on discrete manifolds to embed\ninformation on the geometry of the workpiece extracted from triangular meshes,\nand extend the Dynamic Movement Primitives (DMPs) framework to generate motions\non the mesh surfaces. We also propose an effective strategy to adapt the motion\nto different surfaces, by introducing an isometric transformation of the\nlearned forcing term. The resulting approach, namely MeshDMP, is evaluated both\nin simulation and real experiments, showing promising results in typical\nindustrial automation tasks like car surface polishing.\n","authors":["Matteo Dalle Vedove","Fares J. Abu-Dakka","Luigi Palopoli","Daniele Fontanelli","Matteo Saveriano"],"pdf_url":"https://arxiv.org/pdf/2410.15123v2.pdf","comment":"Accepted at the 2025 IEEE International Conference on Robotics and\n  Automation"},{"id":"http://arxiv.org/abs/2501.18315v1","updated":"2025-01-30T12:49:17Z","published":"2025-01-30T12:49:17Z","title":"Surface Defect Identification using Bayesian Filtering on a 3D Mesh","summary":"  This paper presents a CAD-based approach for automated surface defect\ndetection. We leverage the a-priori knowledge embedded in a CAD model and\nintegrate it with point cloud data acquired from commercially available stereo\nand depth cameras. The proposed method first transforms the CAD model into a\nhigh-density polygonal mesh, where each vertex represents a state variable in\n3D space. Subsequently, a weighted least squares algorithm is employed to\niteratively estimate the state of the scanned workpiece based on the captured\npoint cloud measurements. This framework offers the potential to incorporate\ninformation from diverse sensors into the CAD domain, facilitating a more\ncomprehensive analysis. Preliminary results demonstrate promising performance,\nwith the algorithm achieving convergence to a sub-millimeter standard deviation\nin the region of interest using only approximately 50 point cloud samples. This\nhighlights the potential of utilising commercially available stereo cameras for\nhigh-precision quality control applications.\n","authors":["Matteo Dalle Vedove","Matteo Bonetto","Edoardo Lamon","Luigi Palopoli","Matteo Saveriano","Daniele Fontanelli"],"pdf_url":"https://arxiv.org/pdf/2501.18315v1.pdf","comment":"Presented at IMEKO2024 World Congress, Hamburg, Germany, 26-29\n  October 2024"},{"id":"http://arxiv.org/abs/2501.18309v1","updated":"2025-01-30T12:36:36Z","published":"2025-01-30T12:36:36Z","title":"Knowledge in multi-robot systems: an interplay of dynamics, computation\n  and communication","summary":"  We show that the hybrid systems perspective of distributed multi-robot\nsystems is compatible with logical models of knowledge already used in\ndistributed computing, and demonstrate its usefulness by deriving sufficient\nepistemic conditions for exploration and gathering robot tasks to be solvable.\nWe provide a separation of the physical and computational aspects of a robotic\nsystem, allowing us to decouple the problems related to each and directly use\nmethods from control theory and distributed computing, fields that are\ntraditionally distant in the literature. Finally, we demonstrate a novel\napproach for reasoning about the knowledge in multi-robot systems through a\nprincipled method of converting a switched hybrid dynamical system into a\ntemporal-epistemic logic model, passing through an abstract state machine\nrepresentation. This creates space for methods and results to be exchanged\nacross the fields of control theory, distributed computing and\ntemporal-epistemic logic, while reasoning about multi-robot systems.\n","authors":["Giorgio Cignarale","Stephan Felber","Eric Goubault","Bernardo Hummes Flores","Hugo Rincon Galeana"],"pdf_url":"https://arxiv.org/pdf/2501.18309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17351v2","updated":"2025-01-30T12:32:12Z","published":"2025-01-29T00:06:01Z","title":"Realtime Limb Trajectory Optimization for Humanoid Running Through\n  Centroidal Angular Momentum Dynamics","summary":"  One of the essential aspects of humanoid robot running is determining the\nlimb-swinging trajectories. During the flight phases, where the ground reaction\nforces are not available for regulation, the limb swinging trajectories are\nsignificant for the stability of the next stance phase. Due to the conservation\nof angular momentum, improper leg and arm swinging results in highly tilted and\nunsustainable body configurations at the next stance phase landing. In such\ncases, the robotic system fails to maintain locomotion independent of the\nstability of the center of mass trajectories. This problem is more apparent for\nfast and high flight time trajectories. This paper proposes a real-time\nnonlinear limb trajectory optimization problem for humanoid running. The\noptimization problem is tested on two different humanoid robot models, and the\ngenerated trajectories are verified using a running algorithm for both robots\nin a simulation environment.\n","authors":["Sait Sovukluk","Robert Schuller","Johannes Englsberger","Christian Ott"],"pdf_url":"https://arxiv.org/pdf/2501.17351v2.pdf","comment":"This paper has been accepted for publication at the IEEE\n  International Conference on Robotics and Automation (ICRA), Atlanta 2025. v2:\n  - A Github link to the proposed optimization tool is added. - There are no\n  changes in the method and results"},{"id":"http://arxiv.org/abs/2409.15345v2","updated":"2025-01-30T12:20:12Z","published":"2024-09-10T10:59:32Z","title":"Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual\n  perception beyond human capabilities","summary":"  Optical flow, inspired by the mechanisms of biological visual systems,\ncalculates spatial motion vectors within visual scenes that are necessary for\nenabling robotics to excel in complex and dynamic working environments.\nHowever, current optical flow algorithms, despite human-competitive task\nperformance on benchmark datasets, remain constrained by unacceptable time\ndelays (~0.6 seconds per inference, 4X human processing speed) in practical\ndeployment. Here, we introduce a neuromorphic optical flow approach that\naddresses delay bottlenecks by encoding temporal information directly in a\nsynaptic transistor array to assist spatial motion analysis. Compared to\nconventional spatial-only optical flow methods, our spatiotemporal neuromorphic\noptical flow offers the spatial-temporal consistency of motion information,\nrapidly identifying regions of interest in as little as 1-2 ms using the\ntemporal motion cues derived from the embedded temporal information in the\ntwo-dimensional floating gate synaptic transistors. Thus, the visual input can\nbe selectively filtered to achieve faster velocity calculations and various\ntask execution. At the hardware level, due to the atomically sharp interfaces\nbetween distinct functional layers in two-dimensional van der Waals\nheterostructures, the synaptic transistor offers high-frequency response (~100\n{\\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000\ncycles), enabling robust visual processing. In software benchmarks, our system\noutperforms state-of-the-art algorithms with a 400% speedup, frequently\nsurpassing human-level performance while maintaining or enhancing accuracy by\nutilizing the temporal priors provided by the embedded temporal information.\n","authors":["Shengbo Wang","Jingwen Zhao","Tongming Pu","Liangbing Zhao","Xiaoyu Guo","Yue Cheng","Cong Li","Weihao Ma","Chenyu Tang","Zhenyu Xu","Ningli Wang","Luigi Occhipinti","Arokia Nathan","Ravinder Dahiya","Huaqiang Wu","Li Tao","Shuo Gao"],"pdf_url":"https://arxiv.org/pdf/2409.15345v2.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.17349v2","updated":"2025-01-30T12:13:01Z","published":"2025-01-28T23:51:44Z","title":"An Efficient Numerical Function Optimization Framework for Constrained\n  Nonlinear Robotic Problems","summary":"  This paper presents a numerical function optimization framework designed for\nconstrained optimization problems in robotics. The tool is designed with\nreal-time considerations and is suitable for online trajectory and control\ninput optimization problems. The proposed framework does not require any\nanalytical representation of the problem and works with constrained block-box\noptimization functions. The method combines first-order gradient-based line\nsearch algorithms with constraint prioritization through nullspace projections\nonto constraint Jacobian space. The tool is implemented in C++ and provided\nonline for community use, along with some numerical and robotic example\nimplementations presented in this paper.\n","authors":["Sait Sovukluk","Christian Ott"],"pdf_url":"https://arxiv.org/pdf/2501.17349v2.pdf","comment":"This work has been submitted to IFAC for possible publication. v2: -\n  A link to the GitHub repository is added for the proposed optimization tool.\n  - A reference is included for the Humanoid Robot Posture Optimization\n  discussion. - There are no changes in the results or method - Implementation:\n  https://github.com/ssovukluk/ENFORCpp"},{"id":"http://arxiv.org/abs/2412.18516v2","updated":"2025-01-30T11:19:53Z","published":"2024-12-24T15:53:01Z","title":"Generating Explanations for Autonomous Robots: a Systematic Review","summary":"  Building trust between humans and robots has long interested the robotics\ncommunity. Various studies have aimed to clarify the factors that influence the\ndevelopment of user trust. In Human-Robot Interaction (HRI) environments, a\ncritical aspect of trust development is the robot's ability to make its\nbehavior understandable. The concept of an eXplainable Autonomous Robot (XAR)\naddresses this requirement. However, giving a robot self-explanatory abilities\nis a complex task. Robot behavior includes multiple skills and diverse\nsubsystems. This complexity led to research into a wide range of methods for\ngenerating explanations about robot behavior. This paper presents a systematic\nliterature review that analyzes existing strategies for generating explanations\nin robots and studies the current XAR trends. Results indicate promising\nadvancements in explainability systems. However, these systems are still unable\nto fully cover the complex behavior of autonomous robots. Furthermore, we also\nidentify a lack of consensus on the theoretical concept of explainability, and\nthe need for a robust methodology to assess explainability methods and tools\nhas been identified.\n","authors":["David SobrÃ­n-Hidalgo","Ãngel Manuel Guerrero-Higueras","Vicente MatellÃ¡n-Olivera"],"pdf_url":"https://arxiv.org/pdf/2412.18516v2.pdf","comment":"14 pages, 12 figures, 10 tables. This paper is a preprint of an\n  article submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2501.18229v1","updated":"2025-01-30T09:35:17Z","published":"2025-01-30T09:35:17Z","title":"GPD: Guided Polynomial Diffusion for Motion Planning","summary":"  Diffusion-based motion planners are becoming popular due to their\nwell-established performance improvements, stemming from sample diversity and\nthe ease of incorporating new constraints directly during inference. However, a\nprimary limitation of the diffusion process is the requirement for a\nsubstantial number of denoising steps, especially when the denoising process is\ncoupled with gradient-based guidance. In this paper, we introduce, diffusion in\nthe parametric space of trajectories, where the parameters are represented as\nBernstein coefficients. We show that this representation greatly improves the\neffectiveness of the cost function guidance and the inference speed. We also\nintroduce a novel stitching algorithm that leverages the diversity in\ndiffusion-generated trajectories to produce collision-free trajectories with\njust a single cost function-guided model. We demonstrate that our approaches\noutperform current SOTA diffusion-based motion planners for manipulators and\nprovide an ablation study on key components.\n","authors":["Ajit Srikanth","Parth Mahanjan","Kallol Saha","Vishal Mandadi","Pranjal Paul","Pawan Wadhwani","Brojeshwar Bhowmick","Arun Singh","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2501.18229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18220v1","updated":"2025-01-30T09:22:56Z","published":"2025-01-30T09:22:56Z","title":"On-Line Learning for Planning and Control of Underactuated Robots with\n  Uncertain Dynamics","summary":"  We present an iterative approach for planning and controlling motions of\nunderactuated robots with uncertain dynamics. At its core, there is a learning\nprocess which estimates the perturbations induced by the model uncertainty on\nthe active and passive degrees of freedom. The generic iteration of the\nalgorithm makes use of the learned data in both the planning phase, which is\nbased on optimization, and the control phase, where partial feedback\nlinearization of the active dofs is performed on the model updated on-line. The\nperformance of the proposed approach is shown by comparative simulations and\nexperiments on a Pendubot executing various types of swing-up maneuvers. Very\nfew iterations are typically needed to generate dynamically feasible\ntrajectories and the tracking control that guarantees their accurate execution,\neven in the presence of large model uncertainties.\n","authors":["Giulio Turrisi","Marco Capotondi","Claudio Gaz","Valerio Modugno","Giuseppe Oriolo","Alessandro De Luca"],"pdf_url":"https://arxiv.org/pdf/2501.18220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10027v3","updated":"2025-01-30T06:46:59Z","published":"2024-09-16T06:35:18Z","title":"E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models","summary":"  Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.\n","authors":["Chan Kim","Keonwoo Kim","Mintaek Oh","Hanbi Baek","Jiyang Lee","Donghwi Jung","Soojin Woo","Younkyung Woo","John Tucker","Roya Firoozi","Seung-Woo Seo","Mac Schwager","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2409.10027v3.pdf","comment":"19 pages, 28 figures. Project page: https://e2map.github.io. Accepted\n  to ICRA 2025"},{"id":"http://arxiv.org/abs/2501.18162v1","updated":"2025-01-30T06:10:23Z","published":"2025-01-30T06:10:23Z","title":"IROAM: Improving Roadside Monocular 3D Object Detection Learning from\n  Autonomous Vehicle Data Domain","summary":"  In autonomous driving, The perception capabilities of the ego-vehicle can be\nimproved with roadside sensors, which can provide a holistic view of the\nenvironment. However, existing monocular detection methods designed for vehicle\ncameras are not suitable for roadside cameras due to viewpoint domain gaps. To\nbridge this gap and Improve ROAdside Monocular 3D object detection, we propose\nIROAM, a semantic-geometry decoupled contrastive learning framework, which\ntakes vehicle-side and roadside data as input simultaneously. IROAM has two\nsignificant modules. In-Domain Query Interaction module utilizes a transformer\nto learn content and depth information for each domain and outputs object\nqueries. Cross-Domain Query Enhancement To learn better feature representations\nfrom two domains, Cross-Domain Query Enhancement decouples queries into\nsemantic and geometry parts and only the former is used for contrastive\nlearning. Experiments demonstrate the effectiveness of IROAM in improving\nroadside detector's performance. The results validate that IROAM has the\ncapabilities to learn cross-domain information.\n","authors":["Zhe Wang","Xiaoliang Huo","Siqi Fan","Jingjing Liu","Ya-Qin Zhang","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2501.18162v1.pdf","comment":"7 pages, 5 figures, ICRA2025"},{"id":"http://arxiv.org/abs/2407.16288v2","updated":"2025-01-30T05:31:09Z","published":"2024-07-23T08:43:28Z","title":"On the Use of Immersive Digital Technologies for Designing and Operating\n  UAVs","summary":"  Unmanned Aerial Vehicles (UAVs) offer agile, secure and efficient solutions\nfor communication relay networks. However, their modeling and control are\nchallenging, and the mismatch between simulations and actual conditions limits\nreal-world deployment. Moreover, improving situational awareness is essential.\nSeveral studies proposed integrating the operation of UAVs with immersive\ndigital technologies such as Digital Twin (DT) and Extended Reality (XR) to\novercome these challenges. This paper provides a comprehensive overview of the\nlatest research and developments involving immersive digital technologies for\nUAVs. We explore the use of Machine Learning (ML) techniques, particularly Deep\nReinforcement Learning (DRL), to improve the capabilities of DT for UAV\nsystems. We provide discussion, identify key research gaps, and propose\ncountermeasures based on Generative AI (GAI), emphasizing the significant role\nof AI in advancing DT technology for UAVs. Furthermore, we review the\nliterature, provide discussion, and examine how the XR technology can transform\nUAV operations with the support of GAI, and explore its practical challenges.\nFinally, we propose future research directions to further develop the\napplication of immersive digital technologies for UAV operation.\n","authors":["Yousef Emami","Kai Li","Luis Almeida","Sai Zou","Wei Ni"],"pdf_url":"https://arxiv.org/pdf/2407.16288v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2406.13301v2","updated":"2025-01-30T03:40:15Z","published":"2024-06-19T07:42:02Z","title":"ARDuP: Active Region Video Diffusion for Universal Policies","summary":"  Sequential decision-making can be formulated as a text-conditioned video\ngeneration problem, where a video planner, guided by a text-defined goal,\ngenerates future frames visualizing planned actions, from which control actions\nare subsequently derived. In this work, we introduce Active Region Video\nDiffusion for Universal Policies (ARDuP), a novel framework for video-based\npolicy learning that emphasizes the generation of active regions, i.e.\npotential interaction areas, enhancing the conditional policy's focus on\ninteractive areas critical for task execution. This innovative framework\nintegrates active region conditioning with latent diffusion models for video\nplanning and employs latent representations for direct action decoding during\ninverse dynamic modeling. By utilizing motion cues in videos for automatic\nactive region discovery, our method eliminates the need for manual annotations\nof active regions. We validate ARDuP's efficacy via extensive experiments on\nsimulator CLIPort and the real-world dataset BridgeData v2, achieving notable\nimprovements in success rates and generating convincingly realistic video\nplans.\n","authors":["Shuaiyi Huang","Mara Levy","Zhenyu Jiang","Anima Anandkumar","Yuke Zhu","Linxi Fan","De-An Huang","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2406.13301v2.pdf","comment":"Accepted by IROS 2024 (Oral)"},{"id":"http://arxiv.org/abs/2501.18110v1","updated":"2025-01-30T03:29:42Z","published":"2025-01-30T03:29:42Z","title":"Lifelong 3D Mapping Framework for Hand-held & Robot-mounted LiDAR\n  Mapping Systems","summary":"  We propose a lifelong 3D mapping framework that is modular, cloud-native by\ndesign and more importantly, works for both hand-held and robot-mounted 3D\nLiDAR mapping systems. Our proposed framework comprises of dynamic point\nremoval, multi-session map alignment, map change detection and map version\ncontrol. First, our sensor-setup agnostic dynamic point removal algorithm works\nseamlessly with both hand-held and robot-mounted setups to produce clean static\n3D maps. Second, the multi-session map alignment aligns these clean static maps\nautomatically, without manual parameter fine-tuning, into a single reference\nframe, using a two stage approach based on feature descriptor matching and fine\nregistration. Third, our novel map change detection identifies positive and\nnegative changes between two aligned maps. Finally, the map version control\nmaintains a single base map that represents the current state of the\nenvironment, and stores the detected positive and negative changes, and\nboundary information. Our unique map version control system can reconstruct any\nof the previous clean session maps and allows users to query changes between\nany two random mapping sessions, all without storing any input raw session\nmaps, making it very unique. Extensive experiments are performed using\nhand-held commercial LiDAR mapping devices and open-source robot-mounted LiDAR\nSLAM algorithms to evaluate each module and the whole 3D lifelong mapping\nframework.\n","authors":["Liudi Yang","Sai Manoj Prakhya","Senhua Zhu","Ziyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2501.18110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16227v2","updated":"2025-01-30T02:47:02Z","published":"2024-02-26T01:03:27Z","title":"Scaling Robust Optimization for Multi-Agent Robotic Systems: A\n  Distributed Perspective","summary":"  This paper presents a novel distributed robust optimization scheme for\nsteering distributions of multi-agent systems under stochastic and\ndeterministic uncertainty. Robust optimization is a subfield of optimization\nwhich aims to discover an optimal solution that remains robustly feasible for\nall possible realizations of the problem parameters within a given uncertainty\nset. Such approaches would naturally constitute an ideal candidate for\nmulti-robot control, where in addition to stochastic noise, there might be\nexogenous deterministic disturbances. Nevertheless, as these methods are\nusually associated with significantly high computational demands, their\napplication to multi-agent robotics has remained limited. The scope of this\nwork is to propose a scalable robust optimization framework that effectively\naddresses both types of uncertainties, while retaining computational efficiency\nand scalability. In this direction, we provide tractable approximations for\nrobust constraints that are relevant in multi-robot settings. Subsequently, we\ndemonstrate how computations can be distributed through an Alternating\nDirection Method of Multipliers (ADMM) approach towards achieving scalability\nand communication efficiency. All improvements are also theoretically justified\nby establishing and comparing the resulting computational complexities.\nSimulation results highlight the performance of the proposed algorithm in\neffectively handling both stochastic and deterministic uncertainty in\nmulti-robot systems. The scalability of the method is also emphasized by\nshowcasing tasks with up to hundreds of agents. The results of this work\nindicate the promise of blending robust optimization, distribution steering and\ndistributed optimization towards achieving scalable, safe and robust\nmulti-robot control.\n","authors":["Arshiya Taj Abdul","Augustinos D. Saravanos","Evangelos A. Theodorou"],"pdf_url":"https://arxiv.org/pdf/2402.16227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18093v1","updated":"2025-01-30T02:09:35Z","published":"2025-01-30T02:09:35Z","title":"Reward Prediction Error Prioritisation in Experience Replay: The RPE-PER\n  Method","summary":"  Reinforcement Learning algorithms aim to learn optimal control strategies\nthrough iterative interactions with an environment. A critical element in this\nprocess is the experience replay buffer, which stores past experiences,\nallowing the algorithm to learn from a diverse range of interactions rather\nthan just the most recent ones. This buffer is especially essential in dynamic\nenvironments with limited experiences. However, efficiently selecting\nhigh-value experiences to accelerate training remains a challenge. Drawing\ninspiration from the role of reward prediction errors (RPEs) in biological\nsystems, where they are essential for adaptive behaviour and learning, we\nintroduce Reward Predictive Error Prioritised Experience Replay (RPE-PER). This\nnovel approach prioritises experiences in the buffer based on RPEs. Our method\nemploys a critic network, EMCN, that predicts rewards in addition to the\nQ-values produced by standard critic networks. The discrepancy between these\npredicted and actual rewards is computed as RPE and utilised as a signal for\nexperience prioritisation. Experimental evaluations across various continuous\ncontrol tasks demonstrate RPE-PER's effectiveness in enhancing the learning\nspeed and performance of off-policy actor-critic algorithms compared to\nbaseline approaches.\n","authors":["Hoda Yamani","Yuning Xing","Lee Violet C. Ong","Bruce A. MacDonald","Henry Williams"],"pdf_url":"https://arxiv.org/pdf/2501.18093v1.pdf","comment":"This paper was accepted for presentation at the 2024 Australasian\n  Conference on Robotics and Automation (ACRA 2024). It consists of 10 pages,\n  including four figures and two tables"},{"id":"http://arxiv.org/abs/2501.18086v1","updated":"2025-01-30T01:56:07Z","published":"2025-01-30T01:56:07Z","title":"DIAL: Distribution-Informed Adaptive Learning of Multi-Task Constraints\n  for Safety-Critical Systems","summary":"  Safe reinforcement learning has traditionally relied on predefined constraint\nfunctions to ensure safety in complex real-world tasks, such as autonomous\ndriving. However, defining these functions accurately for varied tasks is a\npersistent challenge. Recent research highlights the potential of leveraging\npre-acquired task-agnostic knowledge to enhance both safety and sample\nefficiency in related tasks. Building on this insight, we propose a novel\nmethod to learn shared constraint distributions across multiple tasks. Our\napproach identifies the shared constraints through imitation learning and then\nadapts to new tasks by adjusting risk levels within these learned\ndistributions. This adaptability addresses variations in risk sensitivity\nstemming from expert-specific biases, ensuring consistent adherence to general\nsafety principles even with imperfect demonstrations. Our method can be applied\nto control and navigation domains, including multi-task and meta-task\nscenarios, accommodating constraints such as maintaining safe distances or\nadhering to speed limits. Experimental results validate the efficacy of our\napproach, demonstrating superior safety performance and success rates compared\nto baselines, all without requiring task-specific constraint definitions. These\nfindings underscore the versatility and practicality of our method across a\nwide range of real-world tasks.\n","authors":["Se-Wook Yoo","Seung-Woo Seo"],"pdf_url":"https://arxiv.org/pdf/2501.18086v1.pdf","comment":"16 pages, 14 figures, 6 tables, submission to T-RO in 2024"},{"id":"http://arxiv.org/abs/2501.18075v1","updated":"2025-01-30T00:58:31Z","published":"2025-01-30T00:58:31Z","title":"Synthesizing Grasps and Regrasps for Complex Manipulation Tasks","summary":"  In complex manipulation tasks, e.g., manipulation by pivoting, the motion of\nthe object being manipulated has to satisfy path constraints that can change\nduring the motion. Therefore, a single grasp may not be sufficient for the\nentire path, and the object may need to be regrasped. Additionally, geometric\ndata for objects from a sensor are usually available in the form of point\nclouds. The problem of computing grasps and regrasps from point-cloud\nrepresentation of objects for complex manipulation tasks is a key problem in\nendowing robots with manipulation capabilities beyond pick-and-place. In this\npaper, we formalize the problem of grasping/regrasping for complex manipulation\ntasks with objects represented by (partial) point clouds and present an\nalgorithm to solve it. We represent a complex manipulation task as a sequence\nof constant screw motions. Using a manipulation plan skeleton as a sequence of\nconstant screw motions, we use a grasp metric to find graspable regions on the\nobject for every constant screw segment. The overlap of the graspable regions\nfor contiguous screws are then used to determine when and how many times the\nobject needs to be regrasped. We present experimental results on point cloud\ndata collected from RGB-D sensors to illustrate our approach.\n","authors":["Aditya Patankar","Dasharadhan Mahalingam","Nilanjan Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2501.18075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18802v1","updated":"2025-01-30T23:38:52Z","published":"2025-01-30T23:38:52Z","title":"Agile and Cooperative Aerial Manipulation of a Cable-Suspended Load","summary":"  Quadrotors can carry slung loads to hard-to-reach locations at high speed.\nSince a single quadrotor has limited payload capacities, using a team of\nquadrotors to collaboratively manipulate a heavy object is a scalable and\npromising solution. However, existing control algorithms for multi-lifting\nsystems only enable low-speed and low-acceleration operations due to the\ncomplex dynamic coupling between quadrotors and the load, limiting their use in\ntime-critical missions such as search and rescue. In this work, we present a\nsolution to significantly enhance the agility of cable-suspended multi-lifting\nsystems. Unlike traditional cascaded solutions, we introduce a trajectory-based\nframework that solves the whole-body kinodynamic motion planning problem\nonline, accounting for the dynamic coupling effects and constraints between the\nquadrotors and the load. The planned trajectory is provided to the quadrotors\nas a reference in a receding-horizon fashion and is tracked by an onboard\ncontroller that observes and compensates for the cable tension. Real-world\nexperiments demonstrate that our framework can achieve at least eight times\ngreater acceleration than state-of-the-art methods to follow agile\ntrajectories. Our method can even perform complex maneuvers such as flying\nthrough narrow passages at high speed. Additionally, it exhibits high\nrobustness against load uncertainties and does not require adding any sensors\nto the load, demonstrating strong practicality.\n","authors":["Sihao Sun","Xuerui Wang","Dario Sanalitro","Antonio Franchi","Marco Tognon","Javier Alonso-Mora"],"pdf_url":"https://arxiv.org/pdf/2501.18802v1.pdf","comment":"38 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.18796v1","updated":"2025-01-30T23:15:03Z","published":"2025-01-30T23:15:03Z","title":"Designing Kresling Origami for Personalised Wrist Orthosis","summary":"  The wrist plays a pivotal role in facilitating motion dexterity and hand\nfunctions. Wrist orthoses, from passive braces to active exoskeletons, provide\nan effective solution for the assistance and rehabilitation of motor abilities.\nHowever, the type of motions facilitated by currently available orthoses is\nlimited, with little emphasis on personalised design. To address these gaps,\nthis paper proposes a novel wrist orthosis design inspired by the Kresling\norigami. The design can be adapted to accommodate various individual shape\nparameters, which benefits from the topological variations and intrinsic\ncompliance of origami. Heat-sealable fabrics are used to replicate the\nnon-rigid nature of the Kresling origami. The orthosis is capable of six\ndistinct motion modes with a detachable tendon-based actuation system.\nExperimental characterisation of the workspace has been conducted by activating\ntendons individually. The maximum bending angle in each direction ranges from\n18.81{\\deg} to 32.63{\\deg}. When tendons are pulled in combination, the maximum\nbending angles in the dorsal, palmar, radial, and ulnar directions are\n31.66{\\deg}, 30.38{\\deg}, 27.14{\\deg}, and 14.92{\\deg}, respectively. The\ncapability to generate complex motions such as the dart-throwing motion and\ncircumduction has also been experimentally validated. The work presents a\npromising foundation for the development of personalised wrist orthoses for\ntraining and rehabilitation.\n","authors":["Chenying Liu","Shuai Mao","Yixing Lei","Liang He"],"pdf_url":"https://arxiv.org/pdf/2501.18796v1.pdf","comment":"Accepted for publication in the 2025 IEEE/RAS International\n  Conference on Soft Robotics"},{"id":"http://arxiv.org/abs/2501.18769v1","updated":"2025-01-30T21:45:32Z","published":"2025-01-30T21:45:32Z","title":"One Stack, Diverse Vehicles: Checking Safe Portability of Automated\n  Driving Software","summary":"  Integrating an automated driving software stack into vehicles with variable\nconfiguration is challenging, especially due to different hardware\ncharacteristics. Further, to provide software updates to a vehicle fleet in the\nfield, the functional safety of every affected configuration has to be ensured.\nThese additional demands for dependability and the increasing hardware\ndiversity in automated driving make rigorous automatic analysis essential. This\npaper addresses this challenge by using formal portability checking of adaptive\ncruise controller code for different vehicle configurations. Given a formal\nspecification of the safe behavior, models of target configurations are\nderived, which capture relevant effects of sensors, actuators and computing\nplatforms. A corresponding safe set is obtained and used to check if the\ndesired behavior is achievable on all targets. In a case study, portability\nchecking of a traditional and a neural network controller are performed\nautomatically within minutes for each vehicle hardware configuration. The check\nprovides feedback for necessary adaptations of the controllers, thus, allowing\nrapid integration and testing of software or parameter changes.\n","authors":["Vladislav Nenchev"],"pdf_url":"https://arxiv.org/pdf/2501.18769v1.pdf","comment":"Preprint to appear in 2025 IEEE/SICE International Symposium on\n  System Integration (SII)"},{"id":"http://arxiv.org/abs/2412.16401v2","updated":"2025-01-30T20:35:26Z","published":"2024-12-20T23:29:53Z","title":"Clarke Transform and Encoder-Decoder Architecture for Arbitrary Joints\n  Locations in Displacement-Actuated Continuum Robots","summary":"  In this paper, we consider an arbitrary number of joints and their arbitrary\njoint locations along the center line of a displacement-actuated continuum\nrobot. To achieve this, we revisit the derivation of the Clarke transform\nleading to a formulation capable of considering arbitrary joint locations. The\nproposed modified Clarke transform opens new opportunities in mechanical design\nand algorithmic approaches beyond the current limiting dependency on symmetric\narranged joint locations. By presenting an encoder-decoder architecture based\non the Clarke transform, joint values between different robot designs can be\ntransformed enabling the use of an analogous robot design and direct knowledge\ntransfer. To demonstrate its versatility, applications of control and\ntrajectory generation in simulation are presented, which can be easily\nintegrated into an existing framework designed, for instance, for three\nsymmetric arranged joints.\n","authors":["Reinhard M. Grassmann","Jessica Burgner-Kahrs"],"pdf_url":"https://arxiv.org/pdf/2412.16401v2.pdf","comment":"Accepted for publication in IEEE International Conference on Soft\n  Robotics (RoboSoft 2025), 8 pages, 11 figures, and 2 tables"},{"id":"http://arxiv.org/abs/2501.18733v1","updated":"2025-01-30T20:19:01Z","published":"2025-01-30T20:19:01Z","title":"Integrating LMM Planners and 3D Skill Policies for Generalizable\n  Manipulation","summary":"  The recent advancements in visual reasoning capabilities of large multimodal\nmodels (LMMs) and the semantic enrichment of 3D feature fields have expanded\nthe horizons of robotic capabilities. These developments hold significant\npotential for bridging the gap between high-level reasoning from LMMs and\nlow-level control policies utilizing 3D feature fields. In this work, we\nintroduce LMM-3DP, a framework that can integrate LMM planners and 3D skill\nPolicies. Our approach consists of three key perspectives: high-level planning,\nlow-level control, and effective integration. For high-level planning, LMM-3DP\nsupports dynamic scene understanding for environment disturbances, a critic\nagent with self-feedback, history policy memorization, and reattempts after\nfailures. For low-level control, LMM-3DP utilizes a semantic-aware 3D feature\nfield for accurate manipulation. In aligning high-level and low-level control\nfor robot actions, language embeddings representing the high-level policy are\njointly attended with the 3D feature field in the 3D transformer for seamless\nintegration. We extensively evaluate our approach across multiple skills and\nlong-horizon tasks in a real-world kitchen environment. Our results show a\nsignificant 1.45x success rate increase in low-level control and an approximate\n1.5x improvement in high-level planning accuracy compared to LLM-based\nbaselines. Demo videos and an overview of LMM-3DP are available at\nhttps://lmm-3dp-release.github.io.\n","authors":["Yuelei Li","Ge Yan","Annabella Macaluso","Mazeyu Ji","Xueyan Zou","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.18733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18723v1","updated":"2025-01-30T19:56:04Z","published":"2025-01-30T19:56:04Z","title":"Scaling Policy Gradient Quality-Diversity with Massive Parallelization\n  via Behavioral Variations","summary":"  Quality-Diversity optimization comprises a family of evolutionary algorithms\naimed at generating a collection of diverse and high-performing solutions.\nMAP-Elites (ME), a notable example, is used effectively in fields like\nevolutionary robotics. However, the reliance of ME on random mutations from\nGenetic Algorithms limits its ability to evolve high-dimensional solutions.\nMethods proposed to overcome this include using gradient-based operators like\npolicy gradients or natural evolution strategies. While successful at scaling\nME for neuroevolution, these methods often suffer from slow training speeds, or\ndifficulties in scaling with massive parallelization due to high computational\ndemands or reliance on centralized actor-critic training. In this work, we\nintroduce a fast, sample-efficient ME based algorithm capable of scaling up\nwith massive parallelization, significantly reducing runtimes without\ncompromising performance. Our method, ASCII-ME, unlike existing policy gradient\nquality-diversity methods, does not rely on centralized actor-critic training.\nIt performs behavioral variations based on time step performance metrics and\nmaps these variations to solutions using policy gradients. Our experiments show\nthat ASCII-ME can generate a diverse collection of high-performing deep neural\nnetwork policies in less than 250 seconds on a single GPU. Additionally, it\noperates on average, five times faster than state-of-the-art algorithms while\nstill maintaining competitive sample efficiency.\n","authors":["Konstantinos Mitsides","Maxence Faldor","Antoine Cully"],"pdf_url":"https://arxiv.org/pdf/2501.18723v1.pdf","comment":null}]},"2025-01-31T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2501.19400v1","updated":"2025-01-31T18:57:08Z","published":"2025-01-31T18:57:08Z","title":"Vintix: Action Model via In-Context Reinforcement Learning","summary":"  In-Context Reinforcement Learning (ICRL) represents a promising paradigm for\ndeveloping generalist agents that learn at inference time through\ntrial-and-error interactions, analogous to how large language models adapt\ncontextually, but with a focus on reward maximization. However, the scalability\nof ICRL beyond toy tasks and single-domain settings remains an open challenge.\nIn this work, we present the first steps toward scaling ICRL by introducing a\nfixed, cross-domain model capable of learning behaviors through in-context\nreinforcement learning. Our results demonstrate that Algorithm Distillation, a\nframework designed to facilitate ICRL, offers a compelling and competitive\nalternative to expert distillation to construct versatile action models. These\nfindings highlight the potential of ICRL as a scalable approach for generalist\ndecision-making systems. Code to be released at\nhttps://github.com/dunnolab/vintix\n","authors":["Andrey Polubarov","Nikita Lyubaykin","Alexander Derevyagin","Ilya Zisman","Denis Tarasov","Alexander Nikulin","Vladislav Kurenkov"],"pdf_url":"https://arxiv.org/pdf/2501.19400v1.pdf","comment":"Preprint. In review"},{"id":"http://arxiv.org/abs/2501.19395v1","updated":"2025-01-31T18:50:04Z","published":"2025-01-31T18:50:04Z","title":"Precision Harvesting in Cluttered Environments: Integrating End Effector\n  Design with Dual Camera Perception","summary":"  Due to labor shortages in specialty crop industries, a need for robotic\nautomation to increase agricultural efficiency and productivity has arisen.\nPrevious manipulation systems perform well in harvesting in uncluttered and\nstructured environments. High tunnel environments are more compact and\ncluttered in nature, requiring a rethinking of the large form factor systems\nand grippers. We propose a novel codesigned framework incorporating a global\ndetection camera and a local eye-in-hand camera that demonstrates precise\nlocalization of small fruits via closed-loop visual feedback and reliable error\nhandling. Field experiments in high tunnels show our system can reach an\naverage of 85.0\\% of cherry tomato fruit in 10.98s on average.\n","authors":["Kendall Koe","Poojan Kalpeshbhai Shah","Benjamin Walt","Jordan Westphal","Samhita Marri","Shivani Kamtikar","James Seungbum Nam","Naveen Kumar Uppalapati","Girish Krishnan","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2501.19395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19391v1","updated":"2025-01-31T18:47:21Z","published":"2025-01-31T18:47:21Z","title":"Perceptive Mixed-Integer Footstep Control for Underactuated Bipedal\n  Walking on Rough Terrain","summary":"  Traversing rough terrain requires dynamic bipeds to stabilize themselves\nthrough foot placement without stepping in unsafe areas. Planning these\nfootsteps online is challenging given non-convexity of the safe terrain, and\nimperfect perception and state estimation. This paper addresses these\nchallenges with a full-stack perception and control system for achieving\nunderactuated walking on discontinuous terrain. First, we develop\nmodel-predictive footstep control (MPFC), a single mixed-integer quadratic\nprogram which assumes a convex polygon terrain decomposition to optimize over\ndiscrete foothold choice, footstep position, ankle torque, template dynamics,\nand footstep timing at over 100 Hz. We then propose a novel approach for\ngenerating convex polygon terrain decompositions online. Our perception stack\ndecouples safe-terrain classification from fitting planar polygons, generating\na temporally consistent terrain segmentation in real time using a single CPU\nthread. We demonstrate the performance of our perception and control stack\nthrough outdoor experiments with the underactuated biped Cassie, achieving\nstate of the art perceptive bipedal walking on discontinuous terrain.\nSupplemental Video: https://youtu.be/eCOD1bMi638\n","authors":["Brian Acosta","Michael Posa"],"pdf_url":"https://arxiv.org/pdf/2501.19391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19382v1","updated":"2025-01-31T18:36:04Z","published":"2025-01-31T18:36:04Z","title":"LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention\n  Networks","summary":"  In this paper, we propose a novel loop closure detection algorithm that uses\ngraph attention neural networks to encode semantic graphs to perform place\nrecognition and then use semantic registration to estimate the 6 DoF relative\npose constraint. Our place recognition algorithm has two key modules, namely, a\nsemantic graph encoder module and a graph comparison module. The semantic graph\nencoder employs graph attention networks to efficiently encode spatial,\nsemantic and geometric information from the semantic graph of the input point\ncloud. We then use self-attention mechanism in both node-embedding and\ngraph-embedding steps to create distinctive graph vectors. The graph vectors of\nthe current scan and a keyframe scan are then compared in the graph comparison\nmodule to identify a possible loop closure. Specifically, employing the\ndifference of the two graph vectors showed a significant improvement in\nperformance, as shown in ablation studies. Lastly, we implemented a semantic\nregistration algorithm that takes in loop closure candidate scans and estimates\nthe relative 6 DoF pose constraint for the LiDAR SLAM system. Extensive\nevaluation on public datasets shows that our model is more accurate and robust,\nachieving 13% improvement in maximum F1 score on the SemanticKITTI dataset,\nwhen compared to the baseline semantic graph algorithm. For the benefit of the\ncommunity, we open-source the complete implementation of our proposed algorithm\nand custom implementation of semantic registration at\nhttps://github.com/crepuscularlight/SemanticLoopClosure\n","authors":["Liudi Yang","Ruben Mascaro","Ignacio Alzugaray","Sai Manoj Prakhya","Marco Karrer","Ziyuan Liu","Margarita Chli"],"pdf_url":"https://arxiv.org/pdf/2501.19382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10382v2","updated":"2025-01-31T18:22:56Z","published":"2024-07-15T01:25:39Z","title":"Communication- and Computation-Efficient Distributed Submodular\n  Optimization in Robot Mesh Networks","summary":"  We provide a communication- and computation-efficient method for distributed\nsubmodular optimization in robot mesh networks. Submodularity is a property of\ndiminishing returns that arises in active information gathering such as\nmapping, surveillance, and target tracking. Our method, Resource-Aware\ndistributed Greedy (RAG), introduces a new distributed optimization paradigm\nthat enables scalable and near-optimal action coordination. To this end, RAG\nrequires each robot to make decisions based only on information received from\nand about their neighbors. In contrast, the current paradigms allow the relay\nof information about all robots across the network. As a result, RAG's\ndecision-time scales linearly with the network size, while state-of-the-art\nnear-optimal submodular optimization algorithms scale cubically. We also\ncharacterize how the designed mesh-network topology affects RAG's approximation\nperformance. Our analysis implies that sparser networks favor scalability\nwithout proportionally compromising approximation performance: while RAG's\ndecision time scales linearly with network size, the gain in approximation\nperformance scales sublinearly. We demonstrate RAG's performance in simulated\nscenarios of area detection with up to 45 robots, simulating realistic\nrobot-to-robot (r2r) communication speeds such as the 0.25 Mbps speed of the\nDigi XBee 3 Zigbee 3.0. In the simulations, RAG enables real-time planning, up\nto three orders of magnitude faster than competitive near-optimal algorithms,\nwhile also achieving superior mean coverage performance. To enable the\nsimulations, we extend the high-fidelity and photo-realistic simulator AirSim\nby integrating a scalable collaborative autonomy pipeline to tens of robots and\nsimulating r2r communication delays. Our code is available at\nhttps://github.com/UM-iRaL/Resource-Aware-Coordination-AirSim.\n","authors":["Zirui Xu","Sandilya Sai Garimella","Vasileios Tzoumas"],"pdf_url":"https://arxiv.org/pdf/2407.10382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08893v2","updated":"2025-01-31T17:27:39Z","published":"2024-10-11T15:10:40Z","title":"Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and\n  Parameter Efficient","summary":"  Model-based reinforcement learning (RL) offers a solution to the data\ninefficiency that plagues most model-free RL algorithms. However, learning a\nrobust world model often demands complex and deep architectures, which are\nexpensive to compute and train. Within the world model, dynamics models are\nparticularly crucial for accurate predictions, and various dynamics-model\narchitectures have been explored, each with its own set of challenges.\nCurrently, recurrent neural network (RNN) based world models face issues such\nas vanishing gradients and difficulty in capturing long-term dependencies\neffectively. In contrast, use of transformers suffers from the well-known\nissues of self-attention mechanisms, where both memory and computational\ncomplexity scale as $O(n^2)$, with $n$ representing the sequence length.\n  To address these challenges we propose a state space model (SSM) based world\nmodel, specifically based on Mamba, that achieves $O(n)$ memory and\ncomputational complexity while effectively capturing long-term dependencies and\nfacilitating the use of longer training sequences efficiently. We also\nintroduce a novel sampling method to mitigate the suboptimality caused by an\nincorrect world model in the early stages of training, combining it with the\naforementioned technique to achieve a normalised score comparable to other\nstate-of-the-art model-based RL algorithms using only a 7 million trainable\nparameter world model. This model is accessible and can be trained on an\noff-the-shelf laptop. Our code is available at\nhttps://github.com/realwenlongwang/Drama.git\n","authors":["Wenlong Wang","Ivana Dusparic","Yucheng Shi","Ke Zhang","Vinny Cahill"],"pdf_url":"https://arxiv.org/pdf/2410.08893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19319v1","updated":"2025-01-31T17:15:34Z","published":"2025-01-31T17:15:34Z","title":"Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven\n  Surface Normal-aware Tracking and Mapping","summary":"  Simultaneous Localization and Mapping (SLAM) is essential for precise\nsurgical interventions and robotic tasks in minimally invasive procedures.\nWhile recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM\nwith high-quality novel view synthesis and fast rendering, these systems\nstruggle with accurate depth and surface reconstruction due to multi-view\ninconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between\nthe reconstructed frames. In this work, we present Endo-2DTAM, a real-time\nendoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these\nchallenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which\nconsists of tracking, mapping, and bundle adjustment modules for geometrically\naccurate reconstruction. Our robust tracking module combines point-to-point and\npoint-to-plane distance metrics, while the mapping module utilizes normal\nconsistency and depth distortion to enhance surface reconstruction quality. We\nalso introduce a pose-consistent strategy for efficient and geometrically\ncoherent keyframe sampling. Extensive experiments on public endoscopic datasets\ndemonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth\nreconstruction of surgical scenes while maintaining computationally efficient\ntracking, high-quality visual appearance, and real-time rendering. Our code\nwill be released at github.com/lastbasket/Endo-2DTAM.\n","authors":["Yiming Huang","Beilei Cui","Long Bai","Zhen Chen","Jinlin Wu","Zhen Li","Hongbin Liu","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2501.19319v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2501.19274v1","updated":"2025-01-31T16:37:09Z","published":"2025-01-31T16:37:09Z","title":"GO: The Great Outdoors Multimodal Dataset","summary":"  The Great Outdoors (GO) dataset is a multi-modal annotated data resource\naimed at advancing ground robotics research in unstructured environments. This\ndataset provides the most comprehensive set of data modalities and annotations\ncompared to existing off-road datasets. In total, the GO dataset includes six\nunique sensor types with high-quality semantic annotations and GPS traces to\nsupport tasks such as semantic segmentation, object detection, and SLAM. The\ndiverse environmental conditions represented in the dataset present significant\nreal-world challenges that provide opportunities to develop more robust\nsolutions to support the continued advancement of field robotics, autonomous\nexploration, and perception systems in natural environments. The dataset can be\ndownloaded at: https://www.unmannedlab.org/the-great-outdoors-dataset/\n","authors":["Peng Jiang","Kasi Viswanath","Akhil Nagariya","George Chustz","Maggie Wigness","Philip Osteen","Timothy Overbye","Christian Ellis","Long Quang","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2501.19274v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.19259v1","updated":"2025-01-31T16:17:03Z","published":"2025-01-31T16:17:03Z","title":"Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for\n  Autonomous Drone FlighT at the Edge","summary":"  The integration of human-intuitive interactions into autonomous systems has\nbeen limited. Traditional Natural Language Processing (NLP) systems struggle\nwith context and intent understanding, severely restricting human-robot\ninteraction. Recent advancements in Large Language Models (LLMs) have\ntransformed this dynamic, allowing for intuitive and high-level communication\nthrough speech and text, and bridging the gap between human commands and\nrobotic actions. Additionally, autonomous navigation has emerged as a central\nfocus in robotics research, with artificial intelligence (AI) increasingly\nbeing leveraged to enhance these systems. However, existing AI-based navigation\nalgorithms face significant challenges in latency-critical tasks where rapid\ndecision-making is critical. Traditional frame-based vision systems, while\neffective for high-level decision-making, suffer from high energy consumption\nand latency, limiting their applicability in real-time scenarios. Neuromorphic\nvision systems, combining event-based cameras and spiking neural networks\n(SNNs), offer a promising alternative by enabling energy-efficient, low-latency\nnavigation. Despite their potential, real-world implementations of these\nsystems, particularly on physical platforms such as drones, remain scarce. In\nthis work, we present Neuro-LIFT, a real-time neuromorphic navigation framework\nimplemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural\nlanguage processing, Neuro-LIFT translates human speech into high-level\nplanning commands which are then autonomously executed using event-based\nneuromorphic vision and physics-driven planning. Our framework demonstrates its\ncapabilities in navigating in a dynamic environment, avoiding obstacles, and\nadapting to human instructions in real-time.\n","authors":["Amogh Joshi","Sourav Sanyal","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2501.19259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19256v1","updated":"2025-01-31T16:12:23Z","published":"2025-01-31T16:12:23Z","title":"Objective Metrics for Human-Subjects Evaluation in Explainable\n  Reinforcement Learning","summary":"  Explanation is a fundamentally human process. Understanding the goal and\naudience of the explanation is vital, yet existing work on explainable\nreinforcement learning (XRL) routinely does not consult humans in their\nevaluations. Even when they do, they routinely resort to subjective metrics,\nsuch as confidence or understanding, that can only inform researchers of users'\nopinions, not their practical effectiveness for a given problem. This paper\ncalls on researchers to use objective human metrics for explanation evaluations\nbased on observable and actionable behaviour to build more reproducible,\ncomparable, and epistemically grounded research. To this end, we curate,\ndescribe, and compare several objective evaluation methodologies for applying\nexplanations to debugging agent behaviour and supporting human-agent teaming,\nillustrating our proposed methods using a novel grid-based environment. We\ndiscuss how subjective and objective metrics complement each other to provide\nholistic validation and how future work needs to utilise standardised\nbenchmarks for testing to enable greater comparisons between research.\n","authors":["Balint Gyevnar","Mark Towers"],"pdf_url":"https://arxiv.org/pdf/2501.19256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09536v2","updated":"2025-01-31T16:09:12Z","published":"2024-10-12T13:55:26Z","title":"TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning","summary":"  This work introduces Transformer-based Off-Policy Episodic Reinforcement\nLearning (TOP-ERL), a novel algorithm that enables off-policy updates in the\nERL framework. In ERL, policies predict entire action trajectories over\nmultiple time steps instead of single actions at every time step. These\ntrajectories are typically parameterized by trajectory generators such as\nMovement Primitives (MP), allowing for smooth and efficient exploration over\nlong horizons while capturing high-level temporal correlations. However, ERL\nmethods are often constrained to on-policy frameworks due to the difficulty of\nevaluating state-action values for entire action sequences, limiting their\nsample efficiency and preventing the use of more efficient off-policy\narchitectures. TOP-ERL addresses this shortcoming by segmenting long action\nsequences and estimating the state-action values for each segment using a\ntransformer-based critic architecture alongside an n-step return estimation.\nThese contributions result in efficient and stable training that is reflected\nin the empirical results conducted on sophisticated robot learning\nenvironments. TOP-ERL significantly outperforms state-of-the-art RL methods.\nThorough ablation studies additionally show the impact of key design choices on\nthe model performance.\n","authors":["Ge Li","Dong Tian","Hongyi Zhou","Xinkai Jiang","Rudolf Lioutikov","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2410.09536v2.pdf","comment":"Codebase: https://github.com/BruceGeLi/TOP_ERL_ICLR25. arXiv admin\n  note: text overlap with arXiv:2401.11437"},{"id":"http://arxiv.org/abs/2405.09359v2","updated":"2025-01-31T14:47:22Z","published":"2024-05-15T14:12:38Z","title":"Visual Attention Based Cognitive Human-Robot Collaboration for Pedicle\n  Screw Placement in Robot-Assisted Orthopedic Surgery","summary":"  Current orthopedic robotic systems largely focus on navigation, aiding\nsurgeons in positioning a guiding tube but still requiring manual drilling and\nscrew placement. The automation of this task not only demands high precision\nand safety due to the intricate physical interactions between the surgical tool\nand bone but also poses significant risks when executed without adequate human\noversight. As it involves continuous physical interaction, the robot should\ncollaborate with the surgeon, understand the human intent, and always include\nthe surgeon in the loop. To achieve this, this paper proposes a new cognitive\nhuman-robot collaboration framework, including the intuitive AR-haptic\nhuman-robot interface, the visual-attention-based surgeon model, and the shared\ninteraction control scheme for the robot. User studies on a robotic platform\nfor orthopedic surgery are presented to illustrate the performance of the\nproposed method. The results demonstrate that the proposed human-robot\ncollaboration framework outperforms full robot and full human control in terms\nof safety and ergonomics.\n","authors":["Chen Chen","Qikai Zou","Yuhang Song","Mingrui Yu","Senqiang Zhu","Shiji Song","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2405.09359v2.pdf","comment":"7 pages, 8 figures, in IROS 2024"},{"id":"http://arxiv.org/abs/2412.15398v2","updated":"2025-01-31T14:28:46Z","published":"2024-12-19T21:03:56Z","title":"Tabletop Object Rearrangement: Structure, Complexity, and Efficient\n  Combinatorial Search-Based Solutions","summary":"  This thesis provides an in-depth structural analysis and efficient\nalgorithmic solutions for tabletop object rearrangement with overhand grasps\n(TORO), a foundational task in advancing intelligent robotic manipulation.\nRearranging multiple objects in a confined workspace presents two primary\nchallenges: sequencing actions to minimize pick-and-place operations - an\nNP-hard problem in TORO - and determining temporary object placements (\"buffer\nposes\") within a cluttered environment, which is essential yet highly complex.\nFor TORO with available external free space, this work investigates the minimum\nbuffer space, or \"running buffer size,\" required for temporary relocations,\npresenting both theoretical insights and exact algorithms. For TORO without\nexternal free space, the concept of lazy buffer verification is introduced,\nwith its efficiency evaluated across various manipulator configurations,\nincluding single-arm, dual-arm, and mobile manipulators.\n","authors":["Kai Gao"],"pdf_url":"https://arxiv.org/pdf/2412.15398v2.pdf","comment":"PhD Thesis of Kai Gao, written under the direction of Prof. Jingjin\n  Yu"},{"id":"http://arxiv.org/abs/2405.17794v3","updated":"2025-01-31T12:54:03Z","published":"2024-05-28T03:45:32Z","title":"LNS2+RL: Combining Multi-Agent Reinforcement Learning with Large\n  Neighborhood Search in Multi-Agent Path Finding","summary":"  Multi-Agent Path Finding (MAPF) is a critical component of logistics and\nwarehouse management, which focuses on planning collision-free paths for a team\nof robots in a known environment. Recent work introduced a novel MAPF approach,\nLNS2, which proposed to repair a quickly obtained set of infeasible paths via\niterative replanning, by relying on a fast, yet lower-quality, prioritized\nplanning (PP) algorithm. At the same time, there has been a recent push for\nMulti-Agent Reinforcement Learning (MARL) based MAPF algorithms, which exhibit\nimproved cooperation over such PP algorithms, although inevitably remaining\nslower. In this paper, we introduce a new MAPF algorithm, LNS2+RL, which\ncombines the distinct yet complementary characteristics of LNS2 and MARL to\neffectively balance their individual limitations and get the best from both\nworlds. During early iterations, LNS2+RL relies on MARL for low-level\nreplanning, which we show eliminates collisions much more than a PP algorithm.\nThere, our MARL-based planner allows agents to reason about past and future\ninformation to gradually learn cooperative decision-making through a finely\ndesigned curriculum learning. At later stages of planning, LNS2+RL adaptively\nswitches to PP algorithm to quickly resolve the remaining collisions, naturally\ntrading off solution quality (number of collisions in the solution) and\ncomputational efficiency. Our comprehensive experiments on high-agent-density\ntasks across various team sizes, world sizes, and map structures consistently\ndemonstrate the superior performance of LNS2+RL compared to many MAPF\nalgorithms, including LNS2, LaCAM, EECBS, and SCRIMP. In maps with complex\nstructures, the advantages of LNS2+RL are particularly pronounced, with LNS2+RL\nachieving a success rate of over 50% in nearly half of the tested tasks, while\nthat of LaCAM, EECBS and SCRIMP falls to 0%.\n","authors":["Yutong Wang","Tanishq Duhan","Jiaoyang Li","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2405.17794v3.pdf","comment":"Accepted for presentation at AAAI 2025"},{"id":"http://arxiv.org/abs/2407.03340v2","updated":"2025-01-31T12:15:57Z","published":"2024-05-20T13:09:32Z","title":"A Multi-Modal Explainability Approach for Human-Aware Robots in\n  Multi-Party Conversation","summary":"  The addressee estimation (understanding to whom somebody is talking) is a\nfundamental task for human activity recognition in multi-party conversation\nscenarios. Specifically, in the field of human-robot interaction, it becomes\neven more crucial to enable social robots to participate in such interactive\ncontexts. However, it is usually implemented as a binary classification task,\nrestricting the robot's capability to estimate whether it was addressed\n\\review{or not, which} limits its interactive skills. For a social robot to\ngain the trust of humans, it is also important to manifest a certain level of\ntransparency and explainability. Explainable artificial intelligence thus plays\na significant role in the current machine learning applications and models, to\nprovide explanations for their decisions besides excellent performance. In our\nwork, we a) present an addressee estimation model with improved performance in\ncomparison with the previous state-of-the-art; b) further modify this model to\ninclude inherently explainable attention-based segments; c) implement the\nexplainable addressee estimation as part of a modular cognitive architecture\nfor multi-party conversation in an iCub robot; d) validate the real-time\nperformance of the explainable model in multi-party human-robot interaction; e)\npropose several ways to incorporate explainability and transparency in the\naforementioned architecture; and f) perform an online user study to analyze the\neffect of various explanations on how human participants perceive the robot.\n","authors":["Iveta BeÄkovÃ¡","Å tefan PÃ³coÅ¡","Giulia Belgiovine","Marco Matarese","Omar Eldardeer","Alessandra Sciutti","Carlo Mazzola"],"pdf_url":"https://arxiv.org/pdf/2407.03340v2.pdf","comment":"32pp (+6pp sup.mat.) Accepted in Computer Vision and Image\n  Understanding Journal on January 23, 2025. This research received funding\n  Horizon-Europe TERAIS project (G.A. 101079338) and Slovak Research and\n  Development Agency, project no. APVV-21-0105"},{"id":"http://arxiv.org/abs/2501.19072v1","updated":"2025-01-31T12:00:54Z","published":"2025-01-31T12:00:54Z","title":"SpikingSoft: A Spiking Neuron Controller for Bio-inspired Locomotion\n  with Soft Snake Robots","summary":"  Inspired by the dynamic coupling of moto-neurons and physical elasticity in\nanimals, this work explores the possibility of generating locomotion gaits by\nutilizing physical oscillations in a soft snake by means of a low-level spiking\nneural mechanism. To achieve this goal, we introduce the Double Threshold\nSpiking neuron model with adjustable thresholds to generate varied output\npatterns. This neuron model can excite the natural dynamics of soft robotic\nsnakes, and it enables distinct movements, such as turning or moving forward,\nby simply altering the neural thresholds. Finally, we demonstrate that our\napproach, termed SpikingSoft, naturally pairs and integrates with reinforcement\nlearning. The high-level agent only needs to adjust the two thresholds to\ngenerate complex movement patterns, thus strongly simplifying the learning of\nreactive locomotion. Simulation results demonstrate that the proposed\narchitecture significantly enhances the performance of the soft snake robot,\nenabling it to achieve target objectives with a 21.6% increase in success rate,\na 29% reduction in time to reach the target, and smoother movements compared to\nthe vanilla reinforcement learning controllers or Central Pattern Generator\ncontroller acting in torque space.\n","authors":["Chuhan Zhang","Cong Wang","Wei Pan","Cosimo Della Santina"],"pdf_url":"https://arxiv.org/pdf/2501.19072v1.pdf","comment":"8th IEEE-RAS International Conference on Soft Robotics"},{"id":"http://arxiv.org/abs/2501.19058v1","updated":"2025-01-31T11:41:32Z","published":"2025-01-31T11:41:32Z","title":"Gravity Compensation of the dVRK-Si Patient Side Manipulator based on\n  Dynamic Model Identification","summary":"  The da Vinci Research Kit (dVRK, also known as dVRK Classic) is an\nopen-source teleoperated surgical robotic system whose hardware is obtained\nfrom the first generation da Vinci Surgical System (Intuitive, Sunnyvale, CA,\nUSA). The dVRK has greatly facilitated research in robot-assisted surgery over\nthe past decade and helped researchers address multiple major challenges in\nthis domain. Recently, the dVRK-Si system, a new version of the dVRK which uses\nmechanical components from the da Vinci Si Surgical System, became available to\nthe community. The major difference between the first generation da Vinci and\nthe da Vinci Si is in the structural upgrade of the Patient Side Manipulator\n(PSM). Because of this upgrade, the gravity of the dVRK-Si PSM can no longer be\nignored as in the dVRK Classic. The high gravity offset may lead to relatively\nlow control accuracy and longer response time. In addition, although\nsubstantial progress has been made in addressing the dynamic model\nidentification problem for the dVRK Classic, further research is required on\nmodel-based control for the dVRK-Si, due to differences in mechanical\ncomponents and the demand for enhanced control performance. To address these\nproblems, in this work, we present (1) a novel full kinematic model of the\ndVRK-Si PSM, and (2) a gravity compensation approach based on the dynamic model\nidentification.\n","authors":["Haoying Zhou","Hao Yang","Anton Deguet","Loris Fichera","Jie Ying Wu","Peter Kazanzides"],"pdf_url":"https://arxiv.org/pdf/2501.19058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19045v1","updated":"2025-01-31T11:17:19Z","published":"2025-01-31T11:17:19Z","title":"Trajectory Optimization Under Stochastic Dynamics Leveraging Maximum\n  Mean Discrepancy","summary":"  This paper addresses sampling-based trajectory optimization for risk-aware\nnavigation under stochastic dynamics. Typically such approaches operate by\ncomputing $\\tilde{N}$ perturbed rollouts around the nominal dynamics to\nestimate the collision risk associated with a sequence of control commands. We\nconsider a setting where it is expensive to estimate risk using perturbed\nrollouts, for example, due to expensive collision-checks. We put forward two\nkey contributions. First, we develop an algorithm that distills the statistical\ninformation from a larger set of rollouts to a reduced-set with sample size\n$N<<\\tilde{N}$. Consequently, we estimate collision risk using just $N$\nrollouts instead of $\\tilde{N}$. Second, we formulate a novel surrogate for the\ncollision risk that can leverage the distilled statistical information\ncontained in the reduced-set. We formalize both algorithmic contributions using\ndistribution embedding in Reproducing Kernel Hilbert Space (RKHS) and Maximum\nMean Discrepancy (MMD). We perform extensive benchmarking to demonstrate that\nour MMD-based approach leads to safer trajectories at low sample regime than\nexisting baselines using Conditional Value-at Risk (CVaR) based collision risk\nestimate.\n","authors":["Basant Sharma","Arun Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2501.19045v1.pdf","comment":"https://github.com/Basant1861/MPC-MMD"},{"id":"http://arxiv.org/abs/2501.19042v1","updated":"2025-01-31T11:13:09Z","published":"2025-01-31T11:13:09Z","title":"Swarm-Gen: Fast Generation of Diverse Feasible Swarm Behaviors","summary":"  Coordination behavior in robot swarms is inherently multi-modal in nature.\nThat is, there are numerous ways in which a swarm of robots can avoid\ninter-agent collisions and reach their respective goals. However, the problem\nof generating diverse and feasible swarm behaviors in a scalable manner remains\nlargely unaddressed. In this paper, we fill this gap by combining generative\nmodels with a safety-filter (SF). Specifically, we sample diverse trajectories\nfrom a learned generative model which is subsequently projected onto the\nfeasible set using the SF. We experiment with two choices for generative\nmodels, namely: Conditional Variational Autoencoder (CVAE) and Vector-Quantized\nVariational Autoencoder (VQ-VAE). We highlight the trade-offs these two models\nprovide in terms of computation time and trajectory diversity. We develop a\ncustom solver for our SF and equip it with a neural network that predicts\ncontext-specific initialization. Thecinitialization network is trained in a\nself-supervised manner, taking advantage of the differentiability of the SF\nsolver. We provide two sets of empirical results. First, we demonstrate that we\ncan generate a large set of multi-modal, feasible trajectories, simulating\ndiverse swarm behaviors, within a few tens of milliseconds. Second, we show\nthat our initialization network provides faster convergence of our SF solver\nvis-a-vis other alternative heuristics.\n","authors":["Simon Idoko","B. Bhanu Teja","K. Madhava Krishna","Arun Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2501.19042v1.pdf","comment":"Submitted to RAL"},{"id":"http://arxiv.org/abs/2501.18956v1","updated":"2025-01-31T08:32:06Z","published":"2025-01-31T08:32:06Z","title":"Differentiable Simulation of Soft Robots with Frictional Contacts","summary":"  In recent years, soft robotics simulators have evolved to offer various\nfunctionalities, including the simulation of different material types (e.g.,\nelastic, hyper-elastic) and actuation methods (e.g., pneumatic, cable-driven,\nservomotor). These simulators also provide tools for various tasks, such as\ncalibration, design, and control. However, efficiently and accurately computing\nderivatives within these simulators remains a challenge, particularly in the\npresence of physical contact interactions. Incorporating these derivatives can,\nfor instance, significantly improve the convergence speed of control methods\nlike reinforcement learning and trajectory optimization, enable gradient-based\ntechniques for design, or facilitate end-to-end machine-learning approaches for\nmodel reduction. This paper addresses these challenges by introducing a unified\nmethod for computing the derivatives of mechanical equations within the finite\nelement method framework, including contact interactions modeled as a nonlinear\ncomplementarity problem. The proposed approach handles both collision and\nfriction phases, accounts for their nonsmooth dynamics, and leverages the\nsparsity introduced by mesh-based models. Its effectiveness is demonstrated\nthrough several examples of controlling and calibrating soft systems.\n","authors":["Etienne MÃ©nager","Louis Montaut","Quentin Le Lidec","Justin Carpentier"],"pdf_url":"https://arxiv.org/pdf/2501.18956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18955v1","updated":"2025-01-31T08:27:32Z","published":"2025-01-31T08:27:32Z","title":"Deep Learning based Quasi-consciousness Training for Robot Intelligent\n  Model","summary":"  This paper explores a deep learning based robot intelligent model that\nrenders robots learn and reason for complex tasks. First, by constructing a\nnetwork of environmental factor matrix to stimulate the learning process of the\nrobot intelligent model, the model parameters must be subjected to coarse &\nfine tuning to optimize the loss function for minimizing the loss score,\nmeanwhile robot intelligent model can fuse all previously known concepts\ntogether to represent things never experienced before, which need robot\nintelligent model can be generalized extensively. Secondly, in order to\nprogressively develop a robot intelligent model with primary consciousness,\nevery robot must be subjected to at least 1~3 years of special school for\ntraining anthropomorphic behaviour patterns to understand and process complex\nenvironmental information and make rational decisions. This work explores and\ndelivers the potential application of deep learning-based quasi-consciousness\ntraining in the field of robot intelligent model.\n","authors":["Yuchun Li","Fang Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18943v1","updated":"2025-01-31T08:07:17Z","published":"2025-01-31T08:07:17Z","title":"HeLiOS: Heterogeneous LiDAR Place Recognition via Overlap-based Learning\n  and Local Spherical Transformer","summary":"  LiDAR place recognition is a crucial module in localization that matches the\ncurrent location with previously observed environments. Most existing\napproaches in LiDAR place recognition dominantly focus on the spinning type\nLiDAR to exploit its large FOV for matching. However, with the recent emergence\nof various LiDAR types, the importance of matching data across different LiDAR\ntypes has grown significantly-a challenge that has been largely overlooked for\nmany years. To address these challenges, we introduce HeLiOS, a deep network\ntailored for heterogeneous LiDAR place recognition, which utilizes small local\nwindows with spherical transformers and optimal transport-based cluster\nassignment for robust global descriptors. Our overlap-based data mining and\nguided-triplet loss overcome the limitations of traditional distance-based\nmining and discrete class constraints. HeLiOS is validated on public datasets,\ndemonstrating performance in heterogeneous LiDAR place recognition while\nincluding an evaluation for long-term recognition, showcasing its ability to\nhandle unseen LiDAR types. We release the HeLiOS code as an open source for the\nrobotics community at https://github.com/minwoo0611/HeLiOS.\n","authors":["Minwoo Jung","Sangwoo Jung","Hyeonjae Gil","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2501.18943v1.pdf","comment":"8 pages, 7 figures, 5 table"},{"id":"http://arxiv.org/abs/2501.18942v1","updated":"2025-01-31T08:05:28Z","published":"2025-01-31T08:05:28Z","title":"Open-Source Autonomous Driving Software Platforms: Comparison of\n  Autoware and Apollo","summary":"  Full-stack autonomous driving system spans diverse technological\ndomains-including perception, planning, and control-that each require in-depth\nresearch. Moreover, validating such technologies of the system necessitates\nextensive supporting infrastructure, from simulators and sensors to\nhigh-definition maps. These complexities with barrier to entry pose substantial\nlimitations for individual developers and research groups. Recently,\nopen-source autonomous driving software platforms have emerged to address this\nchallenge by providing autonomous driving technologies and practical supporting\ninfrastructure for implementing and evaluating autonomous driving\nfunctionalities. Among the prominent open-source platforms, Autoware and Apollo\nare frequently adopted in both academia and industry. While previous studies\nhave assessed each platform independently, few have offered a quantitative and\ndetailed head-to-head comparison of their capabilities. In this paper, we\nsystematically examine the core modules of Autoware and Apollo and evaluate\ntheir middleware performance to highlight key differences. These insights serve\nas a practical reference for researchers and engineers, guiding them in\nselecting the most suitable platform for their specific development\nenvironments and advancing the field of full-stack autonomous driving system.\n","authors":["Hee-Yang Jung","Dong-Hee Paek","Seung-Hyun Kong"],"pdf_url":"https://arxiv.org/pdf/2501.18942v1.pdf","comment":"arxiv preprint"},{"id":"http://arxiv.org/abs/2501.10499v2","updated":"2025-01-31T05:51:29Z","published":"2025-01-17T15:16:43Z","title":"Learning More With Less: Sample Efficient Dynamics Learning and\n  Model-Based RL for Loco-Manipulation","summary":"  Combining the agility of legged locomotion with the capabilities of\nmanipulation, loco-manipulation platforms have the potential to perform complex\ntasks in real-world applications. To this end, state-of-the-art quadrupeds with\nattached manipulators, such as the Boston Dynamics Spot, have emerged to\nprovide a capable and robust platform. However, both the complexity of\nloco-manipulation control, as well as the black-box nature of commercial\nplatforms pose challenges for developing accurate dynamics models and control\npolicies. We address these challenges by developing a hand-crafted kinematic\nmodel for a quadruped-with-arm platform and, together with recent advances in\nBayesian Neural Network (BNN)-based dynamics learning using physical priors,\nefficiently learn an accurate dynamics model from data. We then derive control\npolicies for loco-manipulation via model-based reinforcement learning (RL). We\ndemonstrate the effectiveness of this approach on hardware using the Boston\nDynamics Spot with a manipulator, accurately performing dynamic end-effector\ntrajectory tracking even in low data regimes.\n","authors":["Benjamin Hoffman","Jin Cheng","Chenhao Li","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2501.10499v2.pdf","comment":"Master Thesis at ETH Zurich"},{"id":"http://arxiv.org/abs/2501.18899v1","updated":"2025-01-31T05:40:19Z","published":"2025-01-31T05:40:19Z","title":"Minimum Time Strategies for a Differential Drive Robot Escaping from a\n  Circular Detection Region","summary":"  A Differential Drive Robot (DDR) located inside a circular detection region\nin the plane wants to escape from it in minimum time. Various robotics\napplications can be modeled like the previous problem, such as a DDR escaping\nas soon as possible from a forbidden/dangerous region in the plane or running\nout from the sensor footprint of an unmanned vehicle flying at a constant\naltitude. In this paper, we find the motion strategies to accomplish its goal\nunder two scenarios. In one, the detection region moves slower than the DDR and\nseeks to prevent escape; in another, its position is fixed. We formulate the\nproblem as a zero-sum pursuit-evasion game, and using differential games\ntheory, we compute the players' time-optimal motion strategies. Given the DDR's\nspeed advantage, it can always escape by translating away from the center of\nthe detection region at maximum speed. In this work, we show that the previous\nstrategy could be optimal in some cases; however, other motion strategies\nemerge based on the player's speed ratio and the players' initial\nconfigurations.\n","authors":["Ubaldo Ruiz"],"pdf_url":"https://arxiv.org/pdf/2501.18899v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.15830v3","updated":"2025-01-31T03:45:56Z","published":"2025-01-27T07:34:33Z","title":"SpatialVLA: Exploring Spatial Representations for Visual-Language-Action\n  Model","summary":"  In this paper, we claim that spatial understanding is the keypoint in robot\nmanipulation, and propose SpatialVLA to explore effective spatial\nrepresentations for the robot foundation model. Specifically, we introduce\nEgo3D Position Encoding to inject 3D information into the input observations of\nthe visual-language-action model, and propose Adaptive Action Grids to\nrepresent spatial robot movement actions with adaptive discretized action\ngrids, facilitating learning generalizable and transferrable spatial action\nknowledge for cross-robot control. SpatialVLA is first pre-trained on top of a\nvision-language model with 1.1 Million real-world robot episodes, to learn a\ngeneralist manipulation policy across multiple robot environments and tasks.\nAfter pre-training, SpatialVLA is directly applied to perform numerous tasks in\na zero-shot manner. The superior results in both simulation and real-world\nrobots demonstrate its advantage of inferring complex robot motion trajectories\nand its strong in-domain multi-task generalization ability. We further show the\nproposed Adaptive Action Grids offer a new and effective way to fine-tune the\npre-trained SpatialVLA model for new simulation and real-world setups, where\nthe pre-learned action grids are re-discretized to capture robot-specific\nspatial action movements of new setups. The superior results from extensive\nevaluations demonstrate the exceptional in-distribution generalization and\nout-of-distribution adaptation capability, highlighting the crucial benefit of\nthe proposed spatial-aware representations for generalist robot policy\nlearning. All the details and codes will be open-sourced.\n","authors":["Delin Qu","Haoming Song","Qizhi Chen","Yuanqi Yao","Xinyi Ye","Yan Ding","Zhigang Wang","JiaYuan Gu","Bin Zhao","Dong Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2501.15830v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19579v2","updated":"2025-01-31T02:43:32Z","published":"2024-09-29T06:42:12Z","title":"Leveraging Surgical Activity Grammar for Primary Intention Prediction in\n  Laparoscopy Procedures","summary":"  Surgical procedures are inherently complex and dynamic, with intricate\ndependencies and various execution paths. Accurate identification of the\nintentions behind critical actions, referred to as Primary Intentions (PIs), is\ncrucial to understanding and planning the procedure. This paper presents a\nnovel framework that advances PI recognition in instructional videos by\ncombining top-down grammatical structure with bottom-up visual cues. The\ngrammatical structure is based on a rich corpus of surgical procedures,\noffering a hierarchical perspective on surgical activities. A grammar parser,\nutilizing the surgical activity grammar, processes visual data obtained from\nlaparoscopic images through surgical action detectors, ensuring a more precise\ninterpretation of the visual information. Experimental results on the benchmark\ndataset demonstrate that our method outperforms existing surgical activity\ndetectors that rely solely on visual features. Our research provides a\npromising foundation for developing advanced robotic surgical systems with\nenhanced planning and automation capabilities.\n","authors":["Jie Zhang","Song Zhou","Yiwei Wang","Chidan Wan","Huan Zhao","Xiong Cai","Han Ding"],"pdf_url":"https://arxiv.org/pdf/2409.19579v2.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2501.18848v1","updated":"2025-01-31T02:02:40Z","published":"2025-01-31T02:02:40Z","title":"Reinforcement Learning of Flexible Policies for Symbolic Instructions\n  with Adjustable Mapping Specifications","summary":"  Symbolic task representation is a powerful tool for encoding human\ninstructions and domain knowledge. Such instructions guide robots to accomplish\ndiverse objectives and meet constraints through reinforcement learning (RL).\nMost existing methods are based on fixed mappings from environmental states to\nsymbols. However, in inspection tasks, where equipment conditions must be\nevaluated from multiple perspectives to avoid errors of oversight, robots must\nfulfill the same symbol from different states. To help robots respond to\nflexible symbol mapping, we propose representing symbols and their mapping\nspecifications separately within an RL policy. This approach imposes on RL\npolicy to learn combinations of symbolic instructions and mapping\nspecifications, requiring an efficient learning framework. To cope with this\nissue, we introduce an approach for learning flexible policies called Symbolic\nInstructions with Adjustable Mapping Specifications (SIAMS). This paper\nrepresents symbolic instructions using linear temporal logic (LTL), a formal\nlanguage that can be easily integrated into RL. Our method addresses the\ndiversified completion patterns of instructions by (1) a specification-aware\nstate modulation, which embeds differences in mapping specifications in state\nfeatures, and (2) a symbol-number-based task curriculum, which gradually\nprovides tasks according to the learning's progress. Evaluations in 3D\nsimulations with discrete and continuous action spaces demonstrate that our\nmethod outperforms context-aware multitask RL comparisons.\n","authors":["Wataru Hatanaka","Ryota Yamashina","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2501.18848v1.pdf","comment":"8 pages, Accepted by IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2401.01881v3","updated":"2025-01-31T00:20:39Z","published":"2024-01-03T18:42:22Z","title":"Robust Control Barrier Functions using Uncertainty Estimation with\n  Application to Mobile Robots","summary":"  This paper proposes a safety-critical control design approach for nonlinear\ncontrol affine systems in the presence of matched and unmatched uncertainties.\nOur constructive framework couples control barrier function (CBF) theory with a\nnew uncertainty estimator to ensure robust safety. We use the estimated\nuncertainty, along with a derived upper bound on the estimation error, for\nsynthesizing CBFs and safety-critical controllers via a quadratic program-based\nfeedback control law that rigorously ensures robust safety while improving\ndisturbance rejection performance. We extend the method to higher-order CBFs\n(HOCBFs) to achieve safety under unmatched uncertainty, which may cause\nrelative degree differences with respect to control input and disturbances. We\nassume the relative degree difference is at most one, resulting in a\nsecond-order cone constraint. We demonstrate the proposed robust HOCBF method\nthrough a simulation of an uncertain elastic actuator control problem and\nexperimentally validate the efficacy of our robust CBF framework on a tracked\nrobot with slope-induced matched and unmatched perturbations.\n","authors":["Ersin Das","Joel W. Burdick"],"pdf_url":"https://arxiv.org/pdf/2401.01881v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18808v1","updated":"2025-01-31T00:03:21Z","published":"2025-01-31T00:03:21Z","title":"Learning Hamiltonian Dynamics with Bayesian Data Assimilation","summary":"  In this paper, we develop a neural network-based approach for time-series\nprediction in unknown Hamiltonian dynamical systems. Our approach leverages a\nsurrogate model and learns the system dynamics using generalized coordinates\n(positions) and their conjugate momenta while preserving a constant\nHamiltonian. To further enhance long-term prediction accuracy, we introduce an\nAutoregressive Hamiltonian Neural Network, which incorporates autoregressive\nprediction errors into the training objective. Additionally, we employ Bayesian\ndata assimilation to refine predictions in real-time using online measurement\ndata. Numerical experiments on a spring-mass system and highly elliptic orbits\nunder gravitational perturbations demonstrate the effectiveness of the proposed\nmethod, highlighting its potential for accurate and robust long-term\npredictions.\n","authors":["Taehyeun Kim","Tae-Geun Kim","Anouck Girard","Ilya Kolmanovsky"],"pdf_url":"https://arxiv.org/pdf/2501.18808v1.pdf","comment":"8 pages, 12 figures"}]},"2025-02-03T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2412.05313v5","updated":"2025-02-03T18:54:17Z","published":"2024-11-28T19:31:50Z","title":"Î»: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile\n  Manipulation Robotics","summary":"  Efficiently learning and executing long-horizon mobile manipulation (MoMa)\ntasks is crucial for advancing robotics in household and workplace settings.\nHowever, current MoMa models are data-inefficient, underscoring the need for\nimproved models that require realistic-sized benchmarks to evaluate their\nefficiency, which do not exist. To address this, we introduce the LAMBDA\n({\\lambda}) benchmark (Long-horizon Actions for Mobile-manipulation\nBenchmarking of Directed Activities), which evaluates the data efficiency of\nmodels on language-conditioned, long-horizon, multi-room, multi-floor,\npick-and-place tasks using a dataset of manageable size, more feasible for\ncollection. The benchmark includes 571 human-collected demonstrations that\nprovide realism and diversity in simulated and real-world settings. Unlike\nplanner-generated data, these trajectories offer natural variability and\nreplay-verifiability, ensuring robust learning and evaluation. We benchmark\nseveral models, including learning-based models and a neuro-symbolic modular\napproach combining foundation models with task and motion planning.\nLearning-based models show suboptimal success rates, even when leveraging\npretrained weights, underscoring significant data inefficiencies. However, the\nneuro-symbolic approach performs significantly better while being more data\nefficient. Findings highlight the need for more data-efficient learning-based\nMoMa approaches. {\\lambda} addresses this gap by serving as a key benchmark for\nevaluating the data efficiency of those future models in handling household\nrobotics tasks.\n","authors":["Ahmed Jaafar","Shreyas Sundara Raman","Yichen Wei","Sudarshan Harithas","Sofia Juliani","Anneke Wernerfelt","Benedict Quartey","Ifrah Idrees","Jason Xinyu Liu","Stefanie Tellex"],"pdf_url":"https://arxiv.org/pdf/2412.05313v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05717v8","updated":"2025-02-03T17:01:36Z","published":"2024-07-08T08:21:22Z","title":"A New Framework for Nonlinear Kalman Filters","summary":"  The Kalman filter (KF) is a state estimation algorithm that optimally\ncombines system knowledge and measurements to minimize the mean squared error\nof the estimated states. While KF was initially designed for linear systems,\nnumerous extensions of it, such as extended Kalman filter (EKF), unscented\nKalman filter (UKF), cubature Kalman filter (CKF), etc., have been proposed for\nnonlinear systems. Although different types of nonlinear KFs have different\npros and cons, they all use the same framework of linear KF. Yet, according to\nwhat we found in this paper, the framework tends to give overconfident and less\naccurate state estimations when the measurement functions are nonlinear.\nTherefore, in this study, we designed a new framework that can be combined with\nany existing type of nonlinear KFs and showed theoretically and empirically\nthat the new framework estimates the states and covariance more accurately than\nthe old one. The new framework was tested on four different nonlinear KFs and\nfive different tasks, showcasing its ability to reduce estimation errors by\nseveral orders of magnitude in low-measurement-noise conditions.\n","authors":["Shida Jiang","Junzhe Shi","Scott Moura"],"pdf_url":"https://arxiv.org/pdf/2407.05717v8.pdf","comment":"In the MATLAB code for UKF and CKF, we replaced \"sqrt(Variance)\" with\n  \"chol(Variance).'\", making the algorithm faster"},{"id":"http://arxiv.org/abs/2501.18592v2","updated":"2025-02-03T16:01:11Z","published":"2025-01-30T18:59:36Z","title":"Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models","summary":"  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n","authors":["Hao Dong","Moru Liu","Kaiyang Zhou","Eleni Chatzi","Juho Kannala","Cyrill Stachniss","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2501.18592v2.pdf","comment":"Project page:\n  https://github.com/donghao51/Awesome-Multimodal-Adaptation"},{"id":"http://arxiv.org/abs/2408.07795v2","updated":"2025-02-03T15:49:04Z","published":"2024-08-14T20:14:03Z","title":"Exoskeleton-Assisted Balance and Task Evaluation During Quiet Stance and\n  Kneeling in Construction","summary":"  Construction workers exert intense physical effort and experience serious\nsafety and health risks in hazardous working environments. Quiet stance and\nkneeling are among the most common postures performed by construction workers\nduring their daily work. This paper analyzes lower-limb joint influence on\nneural balance control strategies using the frequency behavior of the\nintersection point of ground reaction forces. To evaluate the impact of\nelevation and wearable knee exoskeletons on postural balance and welding task\nperformance, we design and integrate virtual- and mixed-reality (VR/MR) to\nsimulate elevated environments and welding tasks. A linear quadratic\nregulator-controlled triple- and double-link inverted pendulum model is used\nfor balance strategy quantification in quiet stance and kneeling, respectively.\nExtensive multi-subject experiments are conducted to evaluate the usability of\noccupational exoskeletons in destabilizing construction environments. The\nquantified balance strategies capture the significance of knee joint during\nbalance control of quiet stance and kneeling gaits. Results show that center of\npressure sway area reduced up to 62% in quiet stance and 39% in kneeling for\nsubjects tested in high-elevation VR/MR worksites when provided knee\nexoskeleton assistance. The comprehensive balance and multitask evaluation\nmethodology developed aims to reveal exoskeleton design considerations to\nmitigate the fall risk in construction.\n","authors":["Gayatri Sreenivasan","Chunchu Zhu","Jingang Yi"],"pdf_url":"https://arxiv.org/pdf/2408.07795v2.pdf","comment":"14 pages, 15 figures, accepted by IEEE Transactions on Automation\n  Science and Engineering"},{"id":"http://arxiv.org/abs/2410.02980v2","updated":"2025-02-03T15:04:47Z","published":"2024-10-03T20:43:59Z","title":"DecTrain: Deciding When to Train a Monocular Depth DNN Online","summary":"  Deep neural networks (DNNs) can deteriorate in accuracy when deployment data\ndiffers from training data. While performing online training at all timesteps\ncan improve accuracy, it is computationally expensive. We propose DecTrain, a\nnew algorithm that decides when to train a monocular depth DNN online using\nself-supervision with low overhead. To make the decision at each timestep,\nDecTrain compares the cost of training with the predicted accuracy gain. We\nevaluate DecTrain on out-of-distribution data, and find DecTrain maintains\naccuracy compared to online training at all timesteps, while training only 44%\nof the time on average. We also compare the recovery of a low inference cost\nDNN using DecTrain and a more generalizable high inference cost DNN on various\nsequences. DecTrain recovers the majority (97%) of the accuracy gain of online\ntraining at all timesteps while reducing computation compared to the high\ninference cost DNN which recovers only 66%. With an even smaller DNN, we\nachieve 89% recovery while reducing computation by 56%. DecTrain enables\nlow-cost online training for a smaller DNN to have competitive accuracy with a\nlarger, more generalizable DNN at a lower overall computational cost.\n","authors":["Zih-Sing Fu","Soumya Sudhakar","Sertac Karaman","Vivienne Sze"],"pdf_url":"https://arxiv.org/pdf/2410.02980v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.02995v3","updated":"2025-02-03T12:08:50Z","published":"2024-10-03T21:11:42Z","title":"Task-free Lifelong Robot Learning with Retrieval-based Weighted Local\n  Adaptation","summary":"  A fundamental objective in intelligent robotics is to move towards lifelong\nlearning robot that can learn and adapt to unseen scenarios over time. However,\ncontinually learning new tasks would introduce catastrophic forgetting problems\ndue to data distribution shifts. To mitigate this, we store a subset of data\nfrom previous tasks and utilize it in two manners: leveraging experience replay\nto retain learned skills and applying a novel Retrieval-based Local Adaptation\ntechnique to restore relevant knowledge. Since a lifelong learning robot must\noperate in task-free scenarios, where task IDs and even boundaries are not\navailable, our method performs effectively without relying on such information.\nWe also incorporate a selective weighting mechanism to focus on the most\n\"forgotten\" skill segment, ensuring effective knowledge restoration.\nExperimental results across diverse manipulation tasks demonstrate that our\nframework provides a scalable paradigm for lifelong learning, enhancing robot\nperformance in open-ended, task-free scenarios.\n","authors":["Pengzhi Yang","Xinyu Wang","Ruipeng Zhang","Cong Wang","Frans A. Oliehoek","Jens Kober"],"pdf_url":"https://arxiv.org/pdf/2410.02995v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15068v2","updated":"2025-02-03T10:39:08Z","published":"2025-01-25T04:19:33Z","title":"An Atomic Skill Library Construction Method for Data-Efficient Embodied\n  Manipulation","summary":"  Embodied manipulation is a fundamental ability in the realm of embodied\nartificial intelligence. Although current embodied manipulation models show\ncertain generalizations in specific settings, they struggle in new environments\nand tasks due to the complexity and diversity of real-world scenarios. The\ntraditional end-to-end data collection and training manner leads to significant\ndata demands. Decomposing end-to-end tasks into reusable atomic skills helps\nreduce data requirements and improve task execution success rate. However,\nexisting methods are limited by predefined skill sets that cannot be\ndynamically updated. To address the issue, we introduce a three-wheeled\ndata-driven method to build an atomic skill library, which contains general\nskills enabling the execution of complex tasks. We divide tasks into subtasks\nusing the Vision-Language Planning (VLP). Then, atomic skill definitions are\nformed by abstracting the subtasks. Finally, an atomic skill library is\nconstructed via data collection and Vision-Language-Action (VLA) fine-tuning.\nAs the atomic skill library expands dynamically with the three-wheel update\nstrategy, the range of tasks it can cover grows naturally. In this way, our\nmethod shifts focus from end-to-end tasks to atomic skills, significantly\nreducing data costs while maintaining high performance and enabling efficient\nadaptation to new tasks. Extensive experiments in real-world settings\ndemonstrate the effectiveness and efficiency of our approach.\n","authors":["Dongjiang Li","Bo Peng","Chang Li","Ning Qiao","Qi Zheng","Lei Sun","Yusen Qin","Bangguo Li","Yifeng Luan","Yibing Zhan","Mingang Sun","Tong Xu","Lusong Li","Hui Shen","Xiaodong He"],"pdf_url":"https://arxiv.org/pdf/2501.15068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10784v2","updated":"2025-02-03T08:31:18Z","published":"2024-10-14T17:52:04Z","title":"Probabilistic Degeneracy Detection for Point-to-Plane Error Minimization","summary":"  Degeneracies arising from uninformative geometry are known to deteriorate\nLiDAR-based localization and mapping. This work introduces a new probabilistic\nmethod to detect and mitigate the effect of degeneracies in point-to-plane\nerror minimization. The noise on the Hessian of the point-to-plane optimization\nproblem is characterized by the noise on points and surface normals used in its\nconstruction. We exploit this characterization to quantify the probability of a\ndirection being degenerate. The degeneracy-detection procedure is used in a new\nreal-time degeneracy-aware iterative closest point algorithm for LiDAR\nregistration, in which we smoothly attenuate updates in degenerate directions.\nThe method's parameters are selected based on the noise characteristics\nprovided in the LiDAR's datasheet. We validate the approach in four real-world\nexperiments, demonstrating that it outperforms state-of-the-art methods at\ndetecting and mitigating the adverse effects of degeneracies. For the benefit\nof the community, we release the code for the method at:\ngithub.com/ntnu-arl/drpm.\n","authors":["Johan Hatleskog","Kostas Alexis"],"pdf_url":"https://arxiv.org/pdf/2410.10784v2.pdf","comment":"8 pages, 5 figures, accepted by IEEE Robotics and Automation Letters\n  (IEEE RAL). Supplementary video: https://www.youtube.com/watch?v=bKnHs_wwnXs.\n  Code: https://github.com/ntnu-arl/drpm"},{"id":"http://arxiv.org/abs/2411.02158v2","updated":"2025-02-03T08:21:47Z","published":"2024-11-04T15:17:19Z","title":"Learning Multiple Initial Solutions to Optimization Problems","summary":"  Sequentially solving similar optimization problems under strict runtime\nconstraints is essential for many applications, such as robot control,\nautonomous driving, and portfolio management. The performance of local\noptimization methods in these settings is sensitive to the initial solution:\npoor initialization can lead to slow convergence or suboptimal solutions. To\naddress this challenge, we propose learning to predict \\emph{multiple} diverse\ninitial solutions given parameters that define the problem instance. We\nintroduce two strategies for utilizing multiple initial solutions: (i) a\nsingle-optimizer approach, where the most promising initial solution is chosen\nusing a selection function, and (ii) a multiple-optimizers approach, where\nseveral optimizers, potentially run in parallel, are each initialized with a\ndifferent solution, with the best solution chosen afterward. Notably, by\nincluding a default initialization among predicted ones, the cost of the final\noutput is guaranteed to be equal or lower than with the default initialization.\nWe validate our method on three optimal control benchmark tasks: cart-pole,\nreacher, and autonomous driving, using different optimizers: DDP, MPPI, and\niLQR. We find significant and consistent improvement with our method across all\nevaluation settings and demonstrate that it efficiently scales with the number\nof initial solutions required. The code is available at MISO\n(https://github.com/EladSharony/miso).\n","authors":["Elad Sharony","Heng Yang","Tong Che","Marco Pavone","Shie Mannor","Peter Karkus"],"pdf_url":"https://arxiv.org/pdf/2411.02158v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2409.11709v2","updated":"2025-02-03T05:22:43Z","published":"2024-09-18T05:30:44Z","title":"Multi-robot connective collaboration toward collective obstacle field\n  traversal","summary":"  Environments with large terrain height variations present great challenges\nfor legged robot locomotion. Drawing inspiration from fire ants' collective\nassembly behavior, we study strategies that can enable two ``connectable''\nrobots to collectively navigate over bumpy terrains with height variations\nlarger than robot leg length. Each robot was designed to be extremely simple,\nwith a cubical body and one rotary motor actuating four vertical peg legs that\nmove in pairs. Two or more robots could physically connect to one another to\nenhance collective mobility. We performed locomotion experiments with a\ntwo-robot group, across an obstacle field filled with uniformly-distributed\nsemi-spherical ``boulders''. Experimentally-measured robot speed suggested that\nthe connection length between the robots has a significant effect on collective\nmobility: connection length C in [0.86, 0.9] robot unit body length (UBL) were\nable to produce sustainable movements across the obstacle field, whereas\nconnection length C in [0.63, 0.84] and [0.92, 1.1] UBL resulted in low\ntraversability. An energy landscape based model revealed the underlying\nmechanism of how connection length modulated collective mobility through the\nsystem's potential energy landscape, and informed adaptation strategies for the\ntwo-robot system to adapt their connection length for traversing obstacle\nfields with varying spatial frequencies. Our results demonstrated that by\nvarying the connection configuration between the robots, the two-robot system\ncould leverage mechanical intelligence to better utilize obstacle interaction\nforces and produce improved locomotion. Going forward, we envision that\ngeneralized principles of robot-environment coupling can inform design and\ncontrol strategies for a large group of small robots to achieve ant-like\ncollective environment negotiation.\n","authors":["Haodi Hu","Xingjue Liao","Wuhao Du","Feifei Qian"],"pdf_url":"https://arxiv.org/pdf/2409.11709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15178v3","updated":"2025-02-03T04:57:52Z","published":"2024-10-19T18:46:17Z","title":"GUIDEd Agents: Enhancing Navigation Policies through Task-Specific\n  Uncertainty Abstraction in Localization-Limited Environments","summary":"  Autonomous vehicles performing navigation tasks in complex environments face\nsignificant challenges due to uncertainty in state estimation. In many\nscenarios, such as stealth operations or resource-constrained settings,\naccessing high-precision localization comes at a significant cost, forcing\nrobots to rely primarily on less precise state estimates. Our key observation\nis that different tasks require varying levels of precision in different\nregions: a robot navigating a crowded space might need precise localization\nnear obstacles but can operate effectively with less precision elsewhere. In\nthis paper, we present a planning method for integrating task-specific\nuncertainty requirements directly into navigation policies. We introduce\nTask-Specific Uncertainty Maps (TSUMs), which abstract the acceptable levels of\nstate estimation uncertainty across different regions. TSUMs align task\nrequirements and environmental features using a shared representation space,\ngenerated via a domain-adapted encoder. Using TSUMs, we propose Generalized\nUncertainty Integration for Decision-Making and Execution (GUIDE), a policy\nconditioning framework that incorporates these uncertainty requirements into\nrobot decision-making. We find that TSUMs provide an effective way to abstract\ntask-specific uncertainty requirements, and conditioning policies on TSUMs\nenables the robot to reason about the context-dependent value of certainty and\nadapt its behavior accordingly. We show how integrating GUIDE into\nreinforcement learning frameworks allows the agent to learn navigation policies\nthat effectively balance task completion and uncertainty management without\nexplicit reward engineering. We evaluate GUIDE on various real-world robotic\nnavigation tasks and find that it demonstrates significant improvement in task\ncompletion rates compared to baseline methods that do not explicitly consider\ntask-specific uncertainty.\n","authors":["Gokul Puthumanaillam","Paulo Padrao","Jose Fuentes","Leonardo Bobadilla","Melkior Ornik"],"pdf_url":"https://arxiv.org/pdf/2410.15178v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05273v3","updated":"2025-02-03T04:07:37Z","published":"2024-09-12T09:18:09Z","title":"HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers","summary":"  Large Vision-Language-Action (VLA) models, leveraging powerful pre trained\nVision-Language Models (VLMs) backends, have shown promise in robotic control\ndue to their impressive generalization ability. However, the success comes at a\ncost. Their reliance on VLM backends with billions of parameters leads to high\ncomputational costs and inference latency, limiting the testing scenarios to\nmainly quasi-static tasks and hindering performance in dynamic tasks requiring\nrapid interactions. To address these limitations, this paper proposes HiRT, a\nHierarchical Robot Transformer framework that enables flexible frequency and\nperformance trade-off. HiRT keeps VLMs running at low frequencies to capture\ntemporarily invariant features while enabling real-time interaction through a\nhigh-frequency vision-based policy guided by the slowly updated features.\nExperiment results in both simulation and real-world settings demonstrate\nsignificant improvements over baseline methods. Empirically, in static tasks,\nwe double the control frequency and achieve comparable success rates.\nAdditionally, on novel real-world dynamic ma nipulation tasks which are\nchallenging for previous VLA models, HiRT improves the success rate from 48% to\n75%.\n","authors":["Jianke Zhang","Yanjiang Guo","Xiaoyu Chen","Yen-Jen Wang","Yucheng Hu","Chengming Shi","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05273v3.pdf","comment":"Accepted to CORL 2024"},{"id":"http://arxiv.org/abs/2501.13416v2","updated":"2025-02-03T03:14:14Z","published":"2025-01-23T06:42:28Z","title":"M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction\n  with Person-aware Blockwise Attention","summary":"  Understanding social signals in multi-party conversations is important for\nhuman-robot interaction and artificial social intelligence. Social signals\ninclude body pose, head pose, speech, and context-specific activities like\nacquiring and taking bites of food when dining. Past work in multi-party\ninteraction tends to build task-specific models for predicting social signals.\nIn this work, we address the challenge of predicting multimodal social signals\nin multi-party settings in a single model. We introduce M3PT, a causal\ntransformer architecture with modality and temporal blockwise attention masking\nto simultaneously process multiple social cues across multiple participants and\ntheir temporal interactions. We train and evaluate M3PT on the Human-Human\nCommensality Dataset (HHCD), and demonstrate that using multiple modalities\nimproves bite timing and speaking status prediction. Source code:\nhttps://github.com/AbrarAnwar/masked-social-signals/.\n","authors":["Yiming Tang","Abrar Anwar","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2501.13416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10027v4","updated":"2025-02-03T01:26:49Z","published":"2024-09-16T06:35:18Z","title":"E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models","summary":"  Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.\n","authors":["Chan Kim","Keonwoo Kim","Mintaek Oh","Hanbi Baek","Jiyang Lee","Donghwi Jung","Soojin Woo","Younkyung Woo","John Tucker","Roya Firoozi","Seung-Woo Seo","Mac Schwager","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2409.10027v4.pdf","comment":"19 pages, 28 figures. Project page: https://e2map.github.io. Accepted\n  to ICRA 2025"}]},"2025-02-02T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2410.11008v2","updated":"2025-02-02T20:47:48Z","published":"2024-10-14T18:49:41Z","title":"V2I-Calib++: A Multi-terminal Spatial Calibration Approach in Urban\n  Intersections for Collaborative Perception","summary":"  Urban intersections, dense with pedestrian and vehicular traffic and\ncompounded by GPS signal obstructions from high-rise buildings, are among the\nmost challenging areas in urban traffic systems. Traditional single-vehicle\nintelligence systems often perform poorly in such environments due to a lack of\nglobal traffic flow information and the ability to respond to unexpected\nevents. Vehicle-to-Everything (V2X) technology, through real-time communication\nbetween vehicles (V2V) and vehicles to infrastructure (V2I), offers a robust\nsolution. However, practical applications still face numerous challenges.\nCalibration among heterogeneous vehicle and infrastructure endpoints in\nmulti-end LiDAR systems is crucial for ensuring the accuracy and consistency of\nperception system data. Most existing multi-end calibration methods rely on\ninitial calibration values provided by positioning systems, but the instability\nof GPS signals due to high buildings in urban canyons poses severe challenges\nto these methods. To address this issue, this paper proposes a novel multi-end\nLiDAR system calibration method that does not require positioning priors to\ndetermine initial external parameters and meets real-time requirements. Our\nmethod introduces an innovative multi-end perception object association\ntechnique, utilizing a new Overall Distance metric (oDist) to measure the\nspatial association between perception objects, and effectively combines global\nconsistency search algorithms with optimal transport theory. By this means, we\ncan extract co-observed targets from object association results for further\nexternal parameter computation and optimization. Extensive comparative and\nablation experiments conducted on the simulated dataset V2X-Sim and the real\ndataset DAIR-V2X confirm the effectiveness and efficiency of our method. The\ncode for this method can be accessed at:\nhttps://github.com/MassimoQu/v2i-calib.\n","authors":["Qianxin Qu","Yijin Xiong","Xinyu Zhang","Chen Xia","Qian Peng","Ziqiang Song","Kang Liu","Xin Wu","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2410.11008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09150v3","updated":"2025-02-02T07:59:57Z","published":"2024-04-14T05:58:52Z","title":"Learning Cross-hand Policies for High-DOF Reaching and Grasping","summary":"  Reaching-and-grasping is a fundamental skill for robotic manipulation, but\nexisting methods usually train models on a specific gripper and cannot be\nreused on another gripper. In this paper, we propose a novel method that can\nlearn a unified policy model that can be easily transferred to different\ndexterous grippers. Our method consists of two stages: a gripper-agnostic\npolicy model that predicts the displacements of pre-defined key points on the\ngripper, and a gripper-specific adaptation model that translates these\ndisplacements into adjustments for controlling the grippers' joints. The\ngripper state and interactions with objects are captured at the finger level\nusing robust geometric representations, integrated with a transformer-based\nnetwork to address variations in gripper morphology and geometry. In the\nexperiments, we evaluate our method on several dexterous grippers and diverse\nobjects, and the result shows that our method significantly outperforms the\nbaseline methods. Pioneering the transfer of grasp policies across dexterous\ngrippers, our method effectively demonstrates its potential for learning\ngeneralizable and transferable manipulation skills for various robotic hands.\n","authors":["Qijin She","Shishun Zhang","Yunfan Ye","Ruizhen Hu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2404.09150v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2311.02787v3","updated":"2025-02-02T04:40:51Z","published":"2023-11-05T22:43:29Z","title":"Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable\n  Manipulation with Tools","summary":"  Deformable object manipulation stands as one of the most captivating yet\nformidable challenges in robotics. While previous techniques have predominantly\nrelied on learning latent dynamics through demonstrations, typically\nrepresented as either particles or images, there exists a pertinent limitation:\nacquiring suitable demonstrations, especially for long-horizon tasks, can be\nelusive. Moreover, basing learning entirely on demonstrations can hamper the\nmodel's ability to generalize beyond the demonstrated tasks. In this work, we\nintroduce a demonstration-free hierarchical planning approach capable of\ntackling intricate long-horizon tasks without necessitating any training. We\nemploy large language models (LLMs) to articulate a high-level, stage-by-stage\nplan corresponding to a specified task. For every individual stage, the LLM\nprovides both the tool's name and the Python code to craft intermediate subgoal\npoint clouds. With the tool and subgoal for a particular stage at our disposal,\nwe present a granular closed-loop model predictive control strategy. This\nleverages Differentiable Physics with Point-to-Point correspondence\n(DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied\niteratively. Experimental findings affirm that our technique surpasses multiple\nbenchmarks in dough manipulation, spanning both short and long horizons.\nRemarkably, our model demonstrates robust generalization capabilities to novel\nand previously unencountered complex tasks without any preliminary\ndemonstrations. We further substantiate our approach with experimental trials\non real-world robotic platforms. Our project page:\nhttps://qq456cvb.github.io/projects/donut.\n","authors":["Yang You","Bokui Shen","Congyue Deng","Haoran Geng","Songlin Wei","He Wang","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2311.02787v3.pdf","comment":"8 pages. IEEE Robotics and Automation Letters (RA-L). Preprint\n  Version. Accepted January, 2025"},{"id":"http://arxiv.org/abs/2407.08907v4","updated":"2025-02-02T01:55:25Z","published":"2024-07-12T00:57:36Z","title":"Tightly-Coupled LiDAR-IMU-Wheel Odometry with an Online Neural Kinematic\n  Model Learning via Factor Graph Optimization","summary":"  Environments lacking geometric features (e.g., tunnels and long straight\ncorridors) are challenging for LiDAR-based odometry algorithms because LiDAR\npoint clouds degenerate in such environments. For wheeled robots, a wheel\nkinematic model (i.e., wheel odometry) can improve the reliability of the\nodometry estimation. However, the kinematic model suffers from complex motions\n(e.g., wheel slippage, lateral movement) in the case of skid-steering robots\nparticularly because this robot model rotates by skidding its wheels.\nFurthermore, these errors change nonlinearly when the wheel slippage is large\n(e.g., drifting) and are subject to terrain-dependent parameters. To\nsimultaneously tackle point cloud degeneration and the kinematic model errors,\nwe developed a LiDAR-IMU-wheel odometry algorithm incorporating online training\nof a neural network that learns the kinematic model of wheeled robots with\nnonlinearity. We propose to train the neural network online on a factor graph\nalong with robot states, allowing the learning-based kinematic model to adapt\nto the current terrain condition. The proposed method jointly solves online\ntraining of the neural network and LiDAR-IMU-wheel odometry on a unified factor\ngraph to retain the consistency of all those constraints. Through experiments,\nwe first verified that the proposed network adapted to a changing environment,\nresulting in an accurate odometry estimation across different environments. We\nthen confirmed that the proposed odometry estimation algorithm was robust\nagainst point cloud degeneration and nonlinearity (e.g., large wheel slippage\nby drifting) of the kinematic model. The summary video is available here:\nhttps://www.youtube.com/watch?v=CvRVhdda7Cw\n","authors":["Taku Okawara","Kenji Koide","Shuji Oishi","Masashi Yokozuka","Atsuhiko Banno","Kentaro Uno","Kazuya Yoshida"],"pdf_url":"https://arxiv.org/pdf/2407.08907v4.pdf","comment":"Accepted by the journal, Robotics and Autonomous Systems"}]},"2025-02-01T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2501.14238v2","updated":"2025-02-01T18:04:12Z","published":"2025-01-24T04:50:16Z","title":"Point-LN: A Lightweight Framework for Efficient Point Cloud\n  Classification Using Non-Parametric Positional Encoding","summary":"  We introduce Point-LN, a novel lightweight framework engineered for efficient\n3D point cloud classification. Point-LN integrates essential non-parametric\ncomponents-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),\nand non-learnable positional encoding-with a streamlined learnable classifier\nthat significantly enhances classification accuracy while maintaining a minimal\nparameter footprint. This hybrid architecture ensures low computational costs\nand rapid inference speeds, making Point-LN ideal for real-time and\nresource-constrained applications. Comprehensive evaluations on benchmark\ndatasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN\nachieves competitive performance compared to state-of-the-art methods, all\nwhile offering exceptional efficiency. These results establish Point-LN as a\nrobust and scalable solution for diverse point cloud classification tasks,\nhighlighting its potential for widespread adoption in various computer vision\napplications.\n","authors":["Marzieh Mohammadi","Amir Salarpour","Pedram MohajerAnsari"],"pdf_url":"https://arxiv.org/pdf/2501.14238v2.pdf","comment":"This paper has been accepted for presentation at the 29th\n  International Computer Conference, Computer Society of Iran (CSICC) 2025"},{"id":"http://arxiv.org/abs/2501.03907v2","updated":"2025-02-01T14:44:56Z","published":"2025-01-07T16:22:12Z","title":"Implicit Coordination using Active Epistemic Inference for Multi-Robot\n  Systems","summary":"  A Multi-robot system (MRS) provides significant advantages for intricate\ntasks such as environmental monitoring, underwater inspections, and space\nmissions. However, addressing potential communication failures or the lack of\ncommunication infrastructure in these fields remains a challenge. A significant\nportion of MRS research presumes that the system can maintain communication\nwith proximity constraints, but this approach does not solve situations where\ncommunication is either non-existent, unreliable, or poses a security risk.\nSome approaches tackle this issue using predictions about other robots while\nnot communicating, but these methods generally only permit agents to utilize\nfirst-order reasoning, which involves reasoning based purely on their own\nobservations. In contrast, to deal with this problem, our proposed framework\nutilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a\nrobot's perspective to reason about a belief of others observations. Our\napproach has two main phases: i) an efficient runtime plan adaptation using\nactive inference to signal intentions and reason about a robot's own belief and\nthe beliefs of others in the system, and ii) a hierarchical epistemic planning\nframework to iteratively reason about the current MRS mission state. The\nproposed framework outperforms greedy and first-order reasoning approaches and\nis validated using simulations and experiments with heterogeneous robotic\nsystems.\n","authors":["Lauren Bramblett","Jonathan Reasoner","Nicola Bezzo"],"pdf_url":"https://arxiv.org/pdf/2501.03907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07344v2","updated":"2025-02-01T13:38:33Z","published":"2024-04-10T20:59:59Z","title":"Interactive Learning of Physical Object Properties Through Robot\n  Manipulation and Database of Object Measurements","summary":"  This work presents a framework for automatically extracting physical object\nproperties, such as material composition, mass, volume, and stiffness, through\nrobot manipulation and a database of object measurements. The framework\ninvolves exploratory action selection to maximize learning about objects on a\ntable. A Bayesian network models conditional dependencies between object\nproperties, incorporating prior probability distributions and uncertainty\nassociated with measurement actions. The algorithm selects optimal exploratory\nactions based on expected information gain and updates object properties\nthrough Bayesian inference. Experimental evaluation demonstrates effective\naction selection compared to a baseline and correct termination of the\nexperiments if there is nothing more to be learned. The algorithm proved to\nbehave intelligently when presented with trick objects with material properties\nin conflict with their appearance. The robot pipeline integrates with a logging\nmodule and an online database of objects, containing over 24,000 measurements\nof 63 objects with different grippers. All code and data are publicly\navailable, facilitating automatic digitization of objects and their physical\nproperties through exploratory manipulations.\n","authors":["Andrej Kruzliak","Jiri Hartvich","Shubhan P. Patni","Lukas Rustler","Jan Kristof Behrens","Fares J. Abu-Dakka","Krystian Mikolajczyk","Ville Kyrki","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2404.07344v2.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.05258v2","updated":"2025-02-01T12:50:28Z","published":"2024-05-08T17:59:53Z","title":"Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving","summary":"  Efficient data utilization is crucial for advancing 3D scene understanding in\nautonomous driving, where reliance on heavily human-annotated LiDAR point\nclouds challenges fully supervised methods. Addressing this, our study extends\ninto semi-supervised learning for LiDAR semantic segmentation, leveraging the\nintrinsic spatial priors of driving scenes and multi-sensor complements to\naugment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved\nframework that integrates laser beam manipulations from disparate LiDAR scans\nand incorporates LiDAR-camera correspondences to further assist data-efficient\nlearning. Our framework is tailored to enhance 3D scene consistency\nregularization by incorporating multi-modality, including 1) multi-modal\nLaserMix operation for fine-grained cross-sensor interactions; 2)\ncamera-to-LiDAR feature distillation that enhances LiDAR feature learning; and\n3) language-driven knowledge guidance generating auxiliary supervisions using\nopen-vocabulary models. The versatility of LaserMix++ enables applications\nacross LiDAR representations, establishing it as a universally applicable\nsolution. Our framework is rigorously validated through theoretical analysis\nand extensive experiments on popular driving perception datasets. Results\ndemonstrate that LaserMix++ markedly outperforms fully supervised alternatives,\nachieving comparable accuracy with five times fewer annotations and\nsignificantly improving the supervised-only baselines. This substantial\nadvancement underscores the potential of semi-supervised approaches in reducing\nthe reliance on extensive labeled data in LiDAR-based 3D scene understanding\nsystems.\n","authors":["Lingdong Kong","Xiang Xu","Jiawei Ren","Wenwei Zhang","Liang Pan","Kai Chen","Wei Tsang Ooi","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2405.05258v2.pdf","comment":"TPAMI 2025; 18 pages, 6 figures, 9 tables; Code at\n  https://github.com/ldkong1205/LaserMix"},{"id":"http://arxiv.org/abs/2405.17426v2","updated":"2025-02-01T12:49:41Z","published":"2024-05-27T17:59:39Z","title":"Benchmarking and Improving Bird's Eye View Perception Robustness in\n  Autonomous Driving","summary":"  Recent advancements in bird's eye view (BEV) representations have shown\nremarkable promise for in-vehicle 3D perception. However, while these methods\nhave achieved impressive results on standard benchmarks, their robustness in\nvaried conditions remains insufficiently assessed. In this study, we present\nRoboBEV, an extensive benchmark suite designed to evaluate the resilience of\nBEV algorithms. This suite incorporates a diverse set of camera corruption\ntypes, each examined over three severity levels. Our benchmarks also consider\nthe impact of complete sensor failures that occur when using multi-modal\nmodels. Through RoboBEV, we assess 33 state-of-the-art BEV-based perception\nmodels spanning tasks like detection, map segmentation, depth estimation, and\noccupancy prediction. Our analyses reveal a noticeable correlation between the\nmodel's performance on in-distribution datasets and its resilience to\nout-of-distribution challenges. Our experimental results also underline the\nefficacy of strategies like pre-training and depth-free BEV transformations in\nenhancing robustness against out-of-distribution data. Furthermore, we observe\nthat leveraging extensive temporal information significantly improves the\nmodel's robustness. Based on our observations, we design an effective\nrobustness enhancement strategy based on the CLIP model. The insights from this\nstudy pave the way for the development of future BEV models that seamlessly\ncombine accuracy with real-world robustness.\n","authors":["Shaoyuan Xie","Lingdong Kong","Wenwei Zhang","Jiawei Ren","Liang Pan","Kai Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2405.17426v2.pdf","comment":"TPAMI 2025; 17 pages, 13 figures, 11 tables; Code at this https URL:\n  https://github.com/Daniel-xsy/RoboBEV"},{"id":"http://arxiv.org/abs/2409.01083v4","updated":"2025-02-01T11:58:47Z","published":"2024-09-02T09:11:28Z","title":"Affordance-based Robot Manipulation with Flow Matching","summary":"  We present a framework for assistive robot manipulation, which focuses on two\nfundamental challenges: first, efficiently adapting large-scale models to\ndownstream scene affordance understanding tasks, especially in daily living\nscenarios where gathering multi-task data involving humans requires strenuous\neffort; second, effectively learning robot action trajectories by grounding the\nvisual affordance model. We tackle the first challenge by employing a\nparameter-efficient prompt tuning method that prepends learnable text prompts\nto the frozen vision model to predict manipulation affordances in multi-task\nscenarios. Then we propose to learn robot action trajectories guided by\naffordances in a supervised flow matching method. Flow matching represents a\nrobot visuomotor policy as a conditional process of flowing random waypoints to\ndesired robot action trajectories. Finally, we introduce a real-world dataset\nwith 10 tasks across Activities of Daily Living to test our framework. Our\nextensive evaluation highlights that the proposed prompt tuning method for\nlearning manipulation affordance achieves competitive performance and even\noutperforms some other finetuning protocols across data scales, while\nsatisfying parameter efficiency. Learning multi-task robot action trajectories\nwith flow matching leads to consistently favorable results in several robot\nmanipulation benchmarks than some alternative behavior cloning methods. This\nincludes more stable training and evaluation, and noticeably faster inference,\nwhile maintaining comparable generalization performance to diffusion policy,\nwhere flow matching performs marginally better in most cases. Our framework\nseamlessly unifies affordance learning and action generation with flow matching\nfor robot manipulation.\n","authors":["Fan Zhang","Michael Gienger"],"pdf_url":"https://arxiv.org/pdf/2409.01083v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19622v2","updated":"2025-02-01T11:17:14Z","published":"2024-03-28T17:42:54Z","title":"RH20T-P: A Primitive-Level Robotic Dataset Towards Composable\n  Generalization Agents","summary":"  Achieving generalizability in solving out-of-distribution tasks is one of the\nultimate goals of learning robotic manipulation. Recent progress of\nVision-Language Models (VLMs) has shown that VLM-based task planners can\nalleviate the difficulty of solving novel tasks, by decomposing the compounded\ntasks as a plan of sequentially executing primitive-level skills that have been\nalready mastered. It is also promising for robotic manipulation to adapt such\ncomposable generalization ability, in the form of composable generalization\nagents (CGAs). However, the community lacks of reliable design of primitive\nskills and a sufficient amount of primitive-level data annotations. Therefore,\nwe propose RH20T-P, a primitive-level robotic manipulation dataset, which\ncontains about 38k video clips covering 67 diverse manipulation tasks in\nreal-world scenarios. Each clip is manually annotated according to a set of\nmeticulously designed primitive skills that are common in robotic manipulation.\nFurthermore, we standardize a plan-execute CGA paradigm and implement an\nexemplar baseline called RA-P on our RH20T-P, whose positive performance on\nsolving unseen tasks validates that the proposed dataset can offer composable\ngeneralization ability to robotic manipulation agents.\n","authors":["Zeren Chen","Zhelun Shi","Xiaoya Lu","Lehan He","Sucheng Qian","Zhenfei Yin","Wanli Ouyang","Jing Shao","Yu Qiao","Cewu Lu","Lu Sheng"],"pdf_url":"https://arxiv.org/pdf/2403.19622v2.pdf","comment":"18 pages, 11 figures, 7 tables. Accepted by NeurIPS 2024 Workshop"},{"id":"http://arxiv.org/abs/2410.01085v2","updated":"2025-02-01T11:15:23Z","published":"2024-10-01T21:28:30Z","title":"RoTip: A Finger-Shaped Tactile Sensor with Active Rotation Capability","summary":"  In recent years, advancements in optical tactile sensor technology have\nprimarily centred on enhancing sensing precision and expanding the range of\nsensing modalities. To meet the requirements for more skilful manipulation,\nthere should be a movement towards making tactile sensors more dynamic. In this\npaper, we introduce RoTip, a novel vision-based tactile sensor that is uniquely\ndesigned with an independently controlled joint and the capability to sense\ncontact over its entire surface. The rotational capability of the sensor is\nparticularly crucial for manipulating everyday objects, especially thin and\nflexible ones, as it enables the sensor to mobilize while in contact with the\nobject's surface. The manipulation experiments demonstrate the ability of our\nproposed RoTip to manipulate rigid and flexible objects, and the full-finger\ntactile feedback and active rotation capabilities have the potential to explore\nmore complex and precise manipulation tasks.\n","authors":["Xuyang Zhang","Jiaqi Jiang","Shan Luo"],"pdf_url":"https://arxiv.org/pdf/2410.01085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11111v2","updated":"2025-02-01T10:17:17Z","published":"2025-01-19T16:55:50Z","title":"OpenLiDARMap: Zero-Drift Point Cloud Mapping using Map Priors","summary":"  Accurate localization is a critical component of mobile autonomous systems,\nespecially in Global Navigation Satellite Systems (GNSS)-denied environments\nwhere traditional methods fail. In such scenarios, environmental sensing is\nessential for reliable operation. However, approaches such as LiDAR odometry\nand Simultaneous Localization and Mapping (SLAM) suffer from drift over long\ndistances, especially in the absence of loop closures. Map-based localization\noffers a robust alternative, but the challenge lies in creating and\ngeoreferencing maps without GNSS support. To address this issue, we propose a\nmethod for creating georeferenced maps without GNSS by using publicly available\ndata, such as building footprints and surface models derived from sparse aerial\nscans. Our approach integrates these data with onboard LiDAR scans to produce\ndense, accurate, georeferenced 3D point cloud maps. By combining an Iterative\nClosest Point (ICP) scan-to-scan and scan-to-map matching strategy, we achieve\nhigh local consistency without suffering from long-term drift. Thus, we\neliminate the reliance on GNSS for the creation of georeferenced maps. The\nresults demonstrate that LiDAR-only mapping can produce accurate georeferenced\npoint cloud maps when augmented with existing map priors.\n","authors":["Dominik Kulmer","Maximilian Leitenstern","Marcel Weinmann","Markus Lienkamp"],"pdf_url":"https://arxiv.org/pdf/2501.11111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08687v3","updated":"2025-02-01T09:49:25Z","published":"2024-09-13T10:07:28Z","title":"xTED: Cross-Domain Adaptation via Diffusion-Based Trajectory Editing","summary":"  Reusing pre-collected data from different domains is an appealing solution\nfor decision-making tasks, especially when data in the target domain are\nlimited. Existing cross-domain policy transfer methods mostly aim at learning\ndomain correspondences or corrections to facilitate policy learning, such as\nlearning task/domain-specific discriminators, representations, or policies.\nThis design philosophy often results in heavy model architectures or\ntask/domain-specific modeling, lacking flexibility. This reality makes us\nwonder: can we directly bridge the domain gaps universally at the data level,\ninstead of relying on complex downstream cross-domain policy transfer\nprocedures? In this study, we propose the Cross-Domain Trajectory EDiting\n(xTED) framework that employs a specially designed diffusion model for\ncross-domain trajectory adaptation. Our proposed model architecture effectively\ncaptures the intricate dependencies among states, actions, and rewards, as well\nas the dynamics patterns within target data. Edited by adding noises and\ndenoising with the pre-trained diffusion model, source domain trajectories can\nbe transformed to align with target domain properties while preserving original\nsemantic information. This process effectively corrects underlying domain gaps,\nenhancing state realism and dynamics reliability in source data, and allowing\nflexible integration with various single-domain and cross-domain downstream\npolicy learning methods. Despite its simplicity, xTED demonstrates superior\nperformance in extensive simulation and real-robot experiments.\n","authors":["Haoyi Niu","Qimao Chen","Tenglong Liu","Jianxiong Li","Guyue Zhou","Yi Zhang","Jianming Hu","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2409.08687v3.pdf","comment":"xTED offers a novel, generic, flexible, simple and effective paradigm\n  that casts cross-domain policy adaptation as a data pre-processing problem"},{"id":"http://arxiv.org/abs/2412.01791v2","updated":"2025-02-01T07:58:23Z","published":"2024-11-27T23:15:06Z","title":"DextrAH-RGB: Visuomotor Policies to Grasp Anything with Dexterous Hands","summary":"  One of the most important, yet challenging, skills for a dexterous robot is\ngrasping a diverse range of objects. Much of the prior work has been limited by\nspeed, generality, or reliance on depth maps and object poses. In this paper,\nwe introduce DextrAH-RGB, a system that can perform dexterous arm-hand grasping\nend-to-end from RGB image input. We train a privileged fabric-guided policy\n(FGP) in simulation through reinforcement learning that acts on a geometric\nfabric controller to dexterously grasp a wide variety of objects. We then\ndistill this privileged FGP into a RGB-based FGP strictly in simulation using\nphotorealistic tiled rendering. To our knowledge, this is the first work that\nis able to demonstrate robust sim2real transfer of an end2end RGB-based policy\nfor complex, dynamic, contact-rich tasks such as dexterous grasping.\nDextrAH-RGB is competitive with depth-based dexterous grasping policies, and\ngeneralizes to novel objects with unseen geometry, texture, and lighting\nconditions in the real world. Videos of our system grasping a diverse range of\nunseen objects are available at \\url{https://dextrah-rgb.github.io/}.\n","authors":["Ritvik Singh","Arthur Allshire","Ankur Handa","Nathan Ratliff","Karl Van Wyk"],"pdf_url":"https://arxiv.org/pdf/2412.01791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09740v2","updated":"2025-02-01T07:24:29Z","published":"2024-10-13T06:18:53Z","title":"Gaussian Splatting Visual MPC for Granular Media Manipulation","summary":"  Recent advancements in learned 3D representations have enabled significant\nprogress in solving complex robotic manipulation tasks, particularly for\nrigid-body objects. However, manipulating granular materials such as beans,\nnuts, and rice, remains challenging due to the intricate physics of particle\ninteractions, high-dimensional and partially observable state, inability to\nvisually track individual particles in a pile, and the computational demands of\naccurate dynamics prediction. Current deep latent dynamics models often\nstruggle to generalize in granular material manipulation due to a lack of\ninductive biases. In this work, we propose a novel approach that learns a\nvisual dynamics model over Gaussian splatting representations of scenes and\nleverages this model for manipulating granular media via Model-Predictive\nControl. Our method enables efficient optimization for complex manipulation\ntasks on piles of granular media. We evaluate our approach in both simulated\nand real-world settings, demonstrating its ability to solve unseen planning\ntasks and generalize to new environments in a zero-shot transfer. We also show\nsignificant prediction and manipulation performance improvements compared to\nexisting granular media manipulation methods.\n","authors":["Wei-Cheng Tseng","Ellina Zhang","Krishna Murthy Jatavallabhula","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2410.09740v2.pdf","comment":"project website https://weichengtseng.github.io/gs-granular-mani/"},{"id":"http://arxiv.org/abs/2501.15071v2","updated":"2025-02-01T06:49:44Z","published":"2025-01-25T04:33:43Z","title":"Gaze-based Task Decomposition for Robot Manipulation in Imitation\n  Learning","summary":"  In imitation learning for robotic manipulation, decomposing object\nmanipulation tasks into multiple sub-tasks is essential. This decomposition\nenables the reuse of learned skills in varying contexts and the combination of\nacquired skills to perform novel tasks, rather than merely replicating\ndemonstrated motions. Gaze plays a critical role in human object manipulation,\nwhere it is strongly correlated with hand movements. We hypothesize that an\nimitating agent's gaze control, fixating on specific landmarks and\ntransitioning between them, simultaneously segments demonstrated manipulations\ninto sub-tasks. In this study, we propose a simple yet robust task\ndecomposition method based on gaze transitions. The method leverages\nteleoperation, a common modality in robotic manipulation for collecting\ndemonstrations, in which a human operator's gaze is measured and used for task\ndecomposition as a substitute for an imitating agent's gaze. Notably, our\nmethod achieves consistent task decomposition across all demonstrations for\neach task, which is desirable in contexts such as machine learning. We applied\nthis method to demonstrations of various tasks and evaluated the\ncharacteristics and consistency of the resulting sub-tasks. Furthermore,\nthrough extensive testing across a wide range of hyperparameter variations, we\ndemonstrated that the proposed method possesses the robustness necessary for\napplication to different robotic systems.\n","authors":["Ryo Takizawa","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2501.15071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19499v2","updated":"2025-02-01T06:32:46Z","published":"2024-09-29T00:55:20Z","title":"FastUMI: A Scalable and Hardware-Independent Universal Manipulation\n  Interface with Dataset","summary":"  Real-world manipulation data involving robotic arms is crucial for developing\ngeneralist action policies, yet such data remains scarce since existing data\ncollection methods are hindered by high costs, hardware dependencies, and\ncomplex setup requirements. In this work, we introduce FastUMI, a substantial\nredesign of the Universal Manipulation Interface (UMI) system that addresses\nthese challenges by enabling rapid deployment, simplifying hardware-software\nintegration, and delivering robust performance in real-world data acquisition.\nCompared with UMI, FastUMI has several advantages: 1) It adopts a decoupled\nhardware design and incorporates extensive mechanical modifications, removing\ndependencies on specialized robotic components while preserving consistent\nobservation perspectives. 2) It also refines the algorithmic pipeline by\nreplacing complex Visual-Inertial Odometry (VIO) implementations with an\noff-the-shelf tracking module, significantly reducing deployment complexity\nwhile maintaining accuracy. 3) FastUMI includes an ecosystem for data\ncollection, verification, and integration with both established and newly\ndeveloped imitation learning algorithms, accelerating policy learning\nadvancement. Additionally, we have open-sourced a high-quality dataset of over\n10,000 real-world demonstration trajectories spanning 22 everyday tasks,\nforming one of the most diverse UMI-like datasets to date. Experimental results\nconfirm that FastUMI facilitates rapid deployment, reduces operational costs\nand labor demands, and maintains robust performance across diverse manipulation\nscenarios, thereby advancing scalable data-driven robotic learning.\n","authors":[" Zhaxizhuoma","Kehui Liu","Chuyue Guan","Zhongjie Jia","Ziniu Wu","Xin Liu","Tianyu Wang","Shuai Liang","Pengan Chen","Pingrui Zhang","Haoming Song","Delin Qu","Dong Wang","Zhigang Wang","Nieqing Cao","Yan Ding","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2409.19499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18411v2","updated":"2025-02-01T04:59:36Z","published":"2024-04-29T04:00:19Z","title":"SeePerSea: Multi-modal Perception Dataset of In-water Objects for\n  Autonomous Surface Vehicles","summary":"  This paper introduces the first publicly accessible labeled multi-modal\nperception dataset for autonomous maritime navigation, focusing on in-water\nobstacles within the aquatic environment to enhance situational awareness for\nAutonomous Surface Vehicles (ASVs). This dataset, collected over 4 years and\nconsisting of diverse objects encountered under varying environmental\nconditions, aims to bridge the research gap in autonomous surface vehicles by\nproviding a multi-modal, annotated, and ego-centric perception dataset, for\nobject detection and classification. We also show the applicability of the\nproposed dataset by training deep learning-based open-source perception\nalgorithms that have shown success. We expect that our dataset will contribute\nto development of the marine autonomy pipelines and marine (field) robotics.\nThis dataset is opensource and can be found at https://seepersea.github.io/.\n","authors":["Mingi Jeong","Arihant Chadda","Ziang Ren","Luyang Zhao","Haowen Liu","Monika Roznere","Aiwei Zhang","Yitao Jiang","Sabriel Achong","Samuel Lensgraf","Alberto Quattrini Li"],"pdf_url":"https://arxiv.org/pdf/2404.18411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12155v3","updated":"2025-02-01T04:09:07Z","published":"2024-11-19T01:23:52Z","title":"Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot\n  Learning","summary":"  In reinforcement learning (RL), we train a value function to understand the\nlong-term consequence of executing a single action. However, the value of\ntaking each action can be ambiguous in robotics as robot movements are\ntypically the aggregate result of executing multiple small actions. Moreover,\nrobotic training data often consists of noisy trajectories, in which each\naction is noisy but executing a series of actions results in a meaningful robot\nmovement. This further makes it difficult for the value function to understand\nthe effect of individual actions. To address this, we introduce Coarse-to-fine\nQ-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that\nlearns a critic network that outputs Q-values over a sequence of actions, i.e.,\nexplicitly training the value function to learn the consequence of executing\naction sequences. We study our algorithm on 53 robotic tasks with sparse and\ndense rewards, as well as with and without demonstrations, from BiGym,\nHumanoidBench, and RLBench. We find that CQN-AS outperforms various baselines,\nin particular on humanoid control tasks.\n","authors":["Younggyo Seo","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2411.12155v3.pdf","comment":"15 Pages. Website: https://younggyo.me/cqn-as/"},{"id":"http://arxiv.org/abs/2411.04983v2","updated":"2025-02-01T02:40:49Z","published":"2024-11-07T18:54:37Z","title":"DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot\n  Planning","summary":"  The ability to predict future outcomes given control actions is fundamental\nfor physical reasoning. However, such predictive models, often called world\nmodels, remains challenging to learn and are typically developed for\ntask-specific solutions with online policy learning. To unlock world models'\ntrue potential, we argue that they should 1) be trainable on offline,\npre-collected trajectories, 2) support test-time behavior optimization, and 3)\nfacilitate task-agnostic reasoning. To this end, we present DINO World Model\n(DINO-WM), a new method to model visual dynamics without reconstructing the\nvisual world. DINO-WM leverages spatial patch features pre-trained with DINOv2,\nenabling it to learn from offline behavioral trajectories by predicting future\npatch features. This allows DINO-WM to achieve observational goals through\naction sequence optimization, facilitating task-agnostic planning by treating\ngoal features as prediction targets. We demonstrate that DINO-WM achieves\nzero-shot behavioral solutions at test time on six environments without expert\ndemonstrations, reward modeling, or pre-learned inverse models, outperforming\nprior state-of-the-art work across diverse task families such as arbitrarily\nconfigured mazes, push manipulation with varied object shapes, and\nmulti-particle scenarios.\n","authors":["Gaoyue Zhou","Hengkai Pan","Yann LeCun","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2411.04983v2.pdf","comment":null}]}}